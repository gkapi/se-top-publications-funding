ABSTRACT,AUTHORKEYWORDS,AUTHORS,BOOKTITLE,DOI,FUNDINGDETAILS,FUNDINGDETAILSCOUNTRY,FUNDINGTEXT,KEYWORDS,NOTE,TITLE,VENUE,YEAR,SOURCE,TYPE
"AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL's effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project ""as-is""and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. The goal of this work is to provide a comprehensive understanding of the internal mechanisms of AFL by performing experiments and by comparing different metrics used to evaluate fuzzers. This can help to show the effectiveness of some techniques and to clarify which aspects are instead outdated. To perform our study, we performed nine unique experiments that we carried out on the popular Fuzzbench platform. Each test focuses on a different aspect of AFL, ranging from its mutation approach to the feedback encoding scheme and its scheduling methodologies. Our findings show that each design choice affects different factors of AFL. Some of these are positively correlated with the number of detected bugs or the coverage of the target application, whereas other features are related to usability and reliability. Most important, we believe that the outcome of our experiments indicates which parts of AFL we should preserve in the design of modern fuzzers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AFL;  FuzzBench;  Fuzzing,"Fioraldi, A. and Mantovani, A. and Maier, D. and Balzarotti, D.",,10.1145/3580596,Advanced Research Projects Agency,United States,,"AFL;  Feedback encoding schemes;  Fuzzbench;  Fuzzing;  Internal design;  Performance;  Security practitioners;  Security testing;  Target application, Program debugging",cited By 1,Dissecting American Fuzzy Lop: A FuzzBench Evaluation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques. © 2023 Association for Computing Machinery.",adaptive partition testing;  Additional Key Words and PhrasesMetamorphic testing;  feedback control;  metamorphic relation;  random testing;  test execution,"Sun, C.-A. and Dai, H. and Liu, H. and Chen, T.Y.",,10.1145/3533314,Aeronautical Science Foundation of China,China,,"Application programs;  Feedback control;  Information use;  Open source software;  Software testing;  Verification;  Well testing, Adaptive partition testing;  Adaptive partitions;  Additional key word and phrasesmetamorphic testing;  Key words;  Metamorphic relations;  Metamorphic testing;  Partition testing;  Random testing;  Test case;  Test execution, Cost effectiveness",cited By 1,Feedback-Directed Metamorphic Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, ""a given action is always followed by another action before a given deadline"". This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant. © 2023 Association for Computing Machinery.",Monitoring;  parametric timed automata;  real-time systems,"Waga, M. and André, É. and Hasuo, I.",,10.1145/3517194,Agence Nationale de la Recherche,France,,"Interactive computer systems;  Model checking;  Pattern matching;  Specifications;  Uncertainty analysis, Optimisations;  Parametric timed automata;  Pattern-matching;  Property;  Real - Time system;  Timed model checking;  Timing parameters, Real time systems",cited By 3,Parametric Timed Pattern Matching,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware testing;  deep neural networks;  generative models;  test coverage,"Dola, S. and Dwyer, M.B. and Soffa, M.L.",,10.1145/3576040,Air Force Office of Scientific Research,United States,"This material is based in part upon work supported by National Science Foundation awards 2019239 and 2129824, by The Air Force Office of Scientific Research under award number FA9550-21-0164, and by Lockheed Martin Advanced Technology Laboratories.","Black-box testing;  Cost effectiveness, Additional key word and phrasessoftware testing;  Black box test;  Feature interactions;  Generative model;  Input distributions;  Key words;  Neural-networks;  Test adequacies;  Test inputs;  Test-coverage, Deep neural networks",cited By 2,Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes. © 2023 Association for Computing Machinery.",industry;  machine learning;  Regression testing;  taxonomy;  test case prioritisation,"Ramírez, A. and Feldt, R. and Romero, J.R.",,10.1145/3511805,Andalusian Regional Government,Spain,,"Cost benefit analysis;  Software testing;  Transmission control protocol, Adverse effect;  Classifieds;  Industry needs;  Machine-learning;  Re-runs;  Regression testing;  Software company;  Systems under tests;  Test case;  Test case prioritization, Machine learning",cited By 2,"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,Anhui Provincial Department of Science and Technology,China,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation;  data structure analysis;  value-flow analysis,"Wang, C. and Wang, W. and Yao, P. and Shi, Q. and Zhou, J. and Xiao, X. and Zhang, C.",,10.1145/3565800,Ant Group,China,,"Data structures;  Flow graphs;  Graphic methods;  Value engineering, Additional key word and phrasesabstract interpretation;  Data structure analyse;  False positive;  Flow-graphs;  Key words;  Memory layout;  Structure analysis;  Ubiquitous data;  Value flow;  Value flow analysis, Containers",cited By 0,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"This article proves a structured program theorem for flowchart quantum programs. The theorem states that any flowchart quantum program is equivalent to a single quantum program that repeatedly executes a quantum measurement and a subprogram, so long as the measurement outcome is true. Moreover, their expected runtime, variance, and general moments are the same. This theorem simplifies the quantum program's verification significantly.-We derive an analytical characterization of the termination problem for quantum programs in polynomial time. Our procedure is more efficient and accurate with much simpler techniques than the analysis of this problem, as described in [29].-We compute the expected runtime analytically and exactly for quantum programs in polynomial time. This result improves the methods based on the weakest precondition calculus for the question recently developed in [31, 34].-We show that a single loop rule is a relatively complete Hoare logic for quantum programs after applying our structured theorem. Although using fewer rules, our method verifies a broader class of quantum programs, compared with the results in [45] and [56]. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",flowchart language;  Quantum programming;  structure programming;  while-language,"Yu, N.",,10.1145/3587154,Association pour la Recherche sur le Cancer,France,This work is supported by ARC Discovery Early Career Researcher Award DE180100156 and ARC Discovery Program DP210102449.,"Flowcharting;  Polynomial approximation;  Quantum theory, Flowchart language;  ITS applications;  Polynomial-time;  Program applications;  Quantum measurement;  Quantum programming;  Runtimes;  Single quantum;  Structure programming;  While-language, Application programs",cited By 0,Structured Theorem for Quantum Programs and its Applications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team's ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR). © 2023 Association for Computing Machinery.",API Recommendation;  Incident Response Plan;  Security Operation Center;  Security Orchestration;  security tool API;  SOAR,"Sworna, Z.T. and Islam, C. and Babar, M.A.",,10.1145/3512768,Australian Government,Australia,,"Semantics, API recommendation;  Augmentation techniques;  Data availability;  Data heterogeneity;  Incident response plans;  Security operation center;  Security orchestration;  Security orchestration, automation, and response;  Security tool API;  Security tools, Automation",cited By 2,APIRO: A Framework for Automated Security Tools API Recommendation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how ""human aspects""- such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on - might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners' key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals' performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research. © 2023 Association for Computing Machinery.",Human aspects;  requirements engineering;  software engineering,"Hidellaarachchi, D. and Grundy, J. and Hoda, R. and Mueller, I.",,10.1145/3546943,Australian Research Council,Australia,,"Domain Knowledge;  Motivation;  Software engineering, Collaboration capabilities;  Communication and collaborations;  Communication capabilities;  Communication skills;  Domain knowledge;  Human aspects;  Knowledge communication;  Requirement engineering;  Software practitioners;  Working styles, Requirements engineering",cited By 5,The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners' Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"This article proves a structured program theorem for flowchart quantum programs. The theorem states that any flowchart quantum program is equivalent to a single quantum program that repeatedly executes a quantum measurement and a subprogram, so long as the measurement outcome is true. Moreover, their expected runtime, variance, and general moments are the same. This theorem simplifies the quantum program's verification significantly.-We derive an analytical characterization of the termination problem for quantum programs in polynomial time. Our procedure is more efficient and accurate with much simpler techniques than the analysis of this problem, as described in [29].-We compute the expected runtime analytically and exactly for quantum programs in polynomial time. This result improves the methods based on the weakest precondition calculus for the question recently developed in [31, 34].-We show that a single loop rule is a relatively complete Hoare logic for quantum programs after applying our structured theorem. Although using fewer rules, our method verifies a broader class of quantum programs, compared with the results in [45] and [56]. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",flowchart language;  Quantum programming;  structure programming;  while-language,"Yu, N.",,10.1145/3587154,Australian Research Council,Australia,,"Flowcharting;  Polynomial approximation;  Quantum theory, Flowchart language;  ITS applications;  Polynomial-time;  Program applications;  Quantum measurement;  Quantum programming;  Runtimes;  Single quantum;  Structure programming;  While-language, Application programs",cited By 0,Structured Theorem for Quantum Programs and its Applications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCode Search;  Duplicate questions;  paraphrase mining;  Stack Overflow,"Gao, Z. and Xia, X. and Lo, D. and Grundy, J. and Zhang, X. and Xing, Z.",,10.1145/3550150,Australian Research Council,Australia,,"Python;  Query processing, Additional key word and phrasescode search;  Duplicate question;  Key words;  Large volumes;  Paraphrase mining;  Performance;  Software developer;  Stack overflow;  User query;  Worked examples, Large dataset",cited By 3,I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed. © 2023 Association for Computing Machinery.",Android application;  hidden sensitive operations;  privacy leak;  program analysis,"Sun, X. and Chen, X. and Li, L. and Cai, H. and Grundy, J. and Samhi, J. and Bissyandé, T. and Klein, J.",,10.1145/3574158,Australian Research Council,Australia,,"Android (operating system);  Android malware;  Application programs;  Large dataset;  Mobile security;  Static analysis, Android applications;  Android apps;  Attack mechanism;  Behavior implementation;  Hidden sensitive operation;  Malicious behavior;  Malware writers;  Privacy leak;  Program analysis;  Sensitive datas, Sensitive data",cited By 1,Demystifying Hidden Sensitive Operations in Android Apps,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques. © 2023 Association for Computing Machinery.",adaptive partition testing;  Additional Key Words and PhrasesMetamorphic testing;  feedback control;  metamorphic relation;  random testing;  test execution,"Sun, C.-A. and Dai, H. and Liu, H. and Chen, T.Y.",,10.1145/3533314,Australian Research Council,Australia,,"Application programs;  Feedback control;  Information use;  Open source software;  Software testing;  Verification;  Well testing, Adaptive partition testing;  Adaptive partitions;  Additional key word and phrasesmetamorphic testing;  Key words;  Metamorphic relations;  Metamorphic testing;  Partition testing;  Random testing;  Test case;  Test execution, Cost effectiveness",cited By 1,Feedback-Directed Metamorphic Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems' dataset through statistical methods. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMicroservice architecture;  microservice security;  software architecture detectors;  software architecture metrics,"Zdun, U. and Queval, P.-J. and Simhandl, G. and Scandariato, R. and Chakravarty, S. and Jelic, M. and Jovanovic, A.",,10.1145/3532183,Austrian Science Fund,Austria,Our work has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 952647 (AssureMOSS project). This work was supported by: FWF (Austrian Science Fund) project API-ACE: I 4268; FWF (Austrian Science Fund) project IAC: I 4731-N 2,"Observability;  Open source software;  Open systems;  Secure communication, Additional key word and phrasesmicroservice architecture;  Application systems;  Architectural design decisions;  Best practices;  Identity management;  Key words;  Microservice security;  Security metrics;  Software architecture detector;  Software architecture metric, Software architecture",cited By 5,"Microservice Security Metrics for Secure Communication, Identity Management, and Observability",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system-decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain;  Ethereum;  malware detection;  Ponzi schemes,"Zheng, Z. and Chen, W. and Zhong, Z. and Chen, Z. and Lu, Y.",,10.1145/3571847,Basic and Applied Basic Research Foundation of Guangdong Province,China,"The research is supported by the National Key R&D Program of China (2020YFB1006002), the National Natural Science Foundation of China under project (62032025), Guangdong Basic and Applied Basic Research Foundation (2021A1515011939), the Youth Innovation Talent Program in Universities and Colleges of Guangdong Province (2018WQNCX301), and the Technology Program of Guangzhou, China (202103050004).","Application programs;  Ethereum;  Learning systems;  Losses;  Malware;  Semantics;  Smart contract, Block-chain;  Detection approach;  Financial loss;  Irreversible loss;  Large amounts;  Malware detection;  Ponzi scheme;  Software-systems;  Static features;  Transaction records, Blockchain",cited By 5,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMobile applications;  android;  energy bugs;  energy issues,"Li, X. and Chen, J. and Liu, Y. and Wu, K. and Gallagher, J.P.",,10.1145/3527851,Basic and Applied Basic Research Foundation of Guangdong Province,China,"This work is supported in part by National Natural Science Foundation of China under Grant (61902249, 61932021, 62102265), in part by Open Research Fund from Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), under Grant GML-KF-22-29, and in part by Guandong Basic and Applied Basic Research Fundation (Grant no. 2021A1515011562) and Guangdong Provincial Key Laboratory (Grant No. 2020B121201001).","Android (operating system);  Energy efficiency;  Energy utilization;  Program debugging;  Software testing, Additional key word and phrasesmobile application;  Android;  Empirical studies;  Energy;  Energy bug;  Energy issues;  Input sequence;  Key words;  Mobile app;  Mobile applications, Potential energy",cited By 2,Combatting Energy Issues for Mobile Applications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,Basic Research Program of Jiangsu Province,China,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners' opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners' perception of the relevance of MCR research, and analyzed the primary studies' research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor- and support systems-related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research. © 2023 Copyright held by the owner/author(s).",literature survey;  Modern code review;  practitioner survey,"Badampudi, D. and Unterkalmsteiner, M. and Britto, R.",,10.1145/3585004,Blekinge Tekniska Hoegskola,Sweden,,"Code inspections;  Code review;  Literature survey;  Modern code review;  Practitioner surveys;  Research analysis;  Research communities;  Research impacts;  State of the art;  Systematic mapping studies, Codes (symbols)",cited By 1,Modern Code Reviews - Survey of Literature and Practice,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGUI testing;  test automation;  test case robustness;  web element locators;  XPath locators,"Nass, M. and Alégroth, E. and Feldt, R. and Leotta, M. and Ricca, F.",,10.1145/3571855,Blekinge Tekniska Hoegskola,Sweden,,"Automation;  Efficiency, Additional key word and phrasesgui testing;  Key words;  Localisation;  Robust tests;  Test Automation;  Test case;  Test case robustness;  Test scripts;  Web element locator;  Xpath locator, Websites",cited By 4,Similarity-based Web Element Localization for Robust Test Automation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Method name prediction;  naming convention,"Wang, S. and Wen, M. and Lin, B. and Liu, Y. and Bissyandé, T.F. and Mao, X.",,10.1145/3597203,China Academy of Space Technology,China,,"Codes (symbols);  Deep learning;  Heuristic methods;  Object oriented programming, Automated tool support;  Development tasks;  Empirical analysis;  Large-scales;  Method implementations;  Method name prediction;  Naming convention;  Objectoriented programming (OOP);  Real world projects;  Research efforts, Forecasting",cited By 0,Pre-implementation Method Name Prediction for Object-oriented Programming,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. ""1-to-1""mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of ""1-to-n""(one binary function matches multiple source functions or binary functions) or even ""n-to-n""(multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the ""1-to-1""mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy. © 2023 Association for Computing Machinery.",1-to-1;  1-to-n;  Binary similarity analysis;  function inlining,"Jia, A. and Fan, M. and Jin, W. and Xu, X. and Zhou, Z. and Tang, Q. and Nie, S. and Wu, S. and Liu, T.",,10.1145/3561385,China Postdoctoral Science Foundation,China,,"Computer software reusability;  Open systems;  Software testing;  Supply chains, 1-to-1;  1-to-n;  Binary files;  Binary functions;  Binary similarity analyse;  Function inlining;  Inlining;  Performance;  Similarity analysis;  Simulation strategies, Open source software",cited By 1,1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,Chinese Academy of Sciences,China,"This research was supported in part by National Key R&D Program of China (Grant No. 2022YFB3103904), the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDC02020100), Joint Fund Cultivation Project of National Natural Science Foundation of China (Grant No. U1636120), the Science and Technology Project of State Grid Corporation of China (Grant No. 5700-202258499A-3-0-ZZ), National Natural Science Foundation of China (Grant No. U1766215), the Young Scientists Fund of the National Natural Science Foundation of China (Grant No. 62002342), and Chinese National Natural Science Foundation (Grant No. 62202462). Any opinions, findings, and conclusions in this article are those of the authors and do not necessarily reflect the views of the funding agencies.","Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×-446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186-1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification models;  model compression;  Model dissemination;  neural networks,"Tian, Y. and Zhang, W. and Wen, M. and Cheung, S.-C. and Sun, C. and Ma, S. and Jiang, Y.",,10.1145/3583564,Cisco Systems,United States,,"Black-box testing;  Classification (of information);  Deep neural networks;  Efficiency;  Iterative methods;  Markov processes;  Neural network models, Classification models;  Classification tasks;  Embedded device;  Image classification model;  Images classification;  Model compression;  Model dissemination;  Neural network model;  Neural-networks;  Original model, Image classification",cited By 1,Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,City University of Hong Kong,Hong Kong (China),,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We devised the L+1-layer divide & conquer approach to leads-to model checking (L+1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L+1-DCA2L2MC. In a temporal logic called UNITY, designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L+1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L+1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLeads-to properties;  master-worker model;  Maude;  parallel model checking;  state space explosion,"Do, C.M. and Phyo, Y. and Riesco, A. and Ogata, K.",,10.1145/3604610,Comunidad de Madrid,Spain,"This work was partially supported by JSPS KAKENHI grant JP19H04082, by grant PID2019-108528RB-C22 (ProCode-UCM) funded by MICIN, and by grant S2018/TCS-4339 (BLOQUES-CM) funded by Comunidad de Madrid co-funded by EIE Funds of the European Union.","Optimization, Additional key word and phraseslead-to property;  Key words;  Master/worker models;  Maude;  Models checking;  Optimization techniques;  Parallel model checking;  Performance;  Property;  State-space explosion, Model checking",cited By 1,Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages' builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider's version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breaking changes;  change impact;  dependency management;  npm;  Semantic Version,"Venturini, D. and Cogo, F.R. and Polato, I. and Gerosa, M.A. and Wiese, I.S.",,10.1145/3576037,Conselho Nacional de Desenvolvimento Cientifico e Tecnologico,Brazil,,"Recovery, Breaking change;  Breakings;  Bug fixes;  Change impacts;  Complex software systems;  Dependency management;  Down-stream;  Empirical studies;  Npm;  Semantic version, Semantics",cited By 0,I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConcept location;  automated query reformulation;  bug localization;  Internet-scale code search;  machine learning;  query quality analysis;  systematic literature review;  term weighting,"Rahman, M.M. and Roy, C.K.",,10.1145/3607179,Dalhousie University,Canada,,"Program debugging;  Quality control;  Search engines, Additional key word and phrasesconcept location;  Automated query reformulation;  Bug localizations;  Code search;  Internet-scale code search;  Key words;  Machine-learning;  Query quality analyse;  Query reformulation;  Systematic literature review;  Term weighting, Machine learning",cited By 0,A Systematic Review of Automated Query Reformulations in Source Code Search,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL's effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project ""as-is""and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. The goal of this work is to provide a comprehensive understanding of the internal mechanisms of AFL by performing experiments and by comparing different metrics used to evaluate fuzzers. This can help to show the effectiveness of some techniques and to clarify which aspects are instead outdated. To perform our study, we performed nine unique experiments that we carried out on the popular Fuzzbench platform. Each test focuses on a different aspect of AFL, ranging from its mutation approach to the feedback encoding scheme and its scheduling methodologies. Our findings show that each design choice affects different factors of AFL. Some of these are positively correlated with the number of detected bugs or the coverage of the target application, whereas other features are related to usability and reliability. Most important, we believe that the outcome of our experiments indicates which parts of AFL we should preserve in the design of modern fuzzers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AFL;  FuzzBench;  Fuzzing,"Fioraldi, A. and Mantovani, A. and Maier, D. and Balzarotti, D.",,10.1145/3580596,Defense Advanced Research Projects Agency,United States,This project was supported by the Defense Advanced Research Projects Agency (DARPA) under agreement number FA875019C0003.,"AFL;  Feedback encoding schemes;  Fuzzbench;  Fuzzing;  Internal design;  Performance;  Security practitioners;  Security testing;  Target application, Program debugging",cited By 1,Dissecting American Fuzzy Lop: A FuzzBench Evaluation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTest case selection;  regression testing;  search-based software testing,"Arrieta, A. and Valle, P. and Agirre, J.A. and Sagardui, G.",,10.1145/3532182,"Department of Education, Universities and Research of the Basque Country",Spain,"The authors are part of the Software and Systems Engineering research group of Mondragon Unibertsitatea (IT1326-19), supported by the Department of Education, Universities and Research of the Basque Country.","Budget control;  Embedded systems;  Integration testing, Additional key word and phrasestest case selection;  Case selections;  Fitness functions;  Key words;  Regression testing;  Search-based;  Search-based software testing;  Seeding strategies;  Selection problems;  Test case selection, Cost effectiveness",cited By 6,Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",developer classification;  developer networks;  Open-source software projects,"Bock, T. and Alznauer, N. and Joblin, M. and Apel, S.",,10.1145/3593803,Deutsche Forschungsgemeinschaft,Germany,This work was supported by the German Research Foundation (Grant No. AP/206/14-1).,"Open source software;  Open systems, Classification methods;  Core developers;  Developer classification;  Developer network;  Open source software projects;  Self-organised;  State of the art;  Tacit knowledge;  Unsupervised classification;  Validation study, Classification (of information)",cited By 0,Automatic Core-Developer Identification on GitHub: A Validation Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software projects are complex technical and organizational systems involving large numbers of artifacts and developers. To understand and tame software complexity, a wide variety of program analysis techniques have been developed for bug detection, program comprehension, verification, and more. At the same time, repository mining techniques aim at obtaining insights into the inner socio-technical workings of software projects at a larger scale. While both program analysis and repository mining have been successful on their own, they are largely isolated, which leaves considerable potential for synergies untapped. We present SEAL, the first integrated approach that combines low-level program analysis with high-level repository information. SEAL maps repository information, mined from the development history of a project, onto a low-level intermediate program representation, making it available for state-of-the-art program analysis. SEAL's integrated approach allows us to efficiently address software engineering problems that span multiple levels of abstraction, from low-level data flow to high-level organizational information. To demonstrate its merits and practicality, we use SEAL to determine which code changes modify central parts of a given software project, how authors interact (indirectly) with each other through code, and we demonstrate that putting static analysis' results into a socio-technical context improves their expressiveness and interpretability. © 2023 Copyright held by the owner/author(s).",socio-technical software analytics;  software repository mining;  Static program analysis,"Sattler, F. and Böhm, S. and Schubert, P.D. and Siegmund, N. and Apel, S.",,10.1145/3585008,Deutsche Forschungsgemeinschaft,Germany,,"Codes (symbols);  Integrated control;  Verification, Complex technical systems;  Integrated approach;  Organizational system;  Program analysis;  Repository mining;  Socio-technical software analytic;  Sociotechnical;  Software project;  Software repository mining;  Static program analysis, Static analysis",cited By 0,SEAL: Integrating Program Analysis and Repository Mining,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20 popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers' positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1) hierarchy is a pervasive structural feature of developer networks of OSS projects; (2) OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3) the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a) provides a methodological basis for further investigations of hierarchy formation, (b) suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c) may ultimately guide the governance of organizational structures. © 2023 Association for Computing Machinery.",developer networks;  hierarchy;  Open-source software projects;  organizational structure,"Joblin, M. and Eckl, B. and Bock, T. and Schmid, A. and Siegmund, J. and Apel, S.",,10.1145/3569949,Deutsche Forschungsgemeinschaft,Germany,"This work was supported by the German Research Foundation (AP 206/14-1) as well as the Bavarian State Ministry of Education, Science, and the Arts in the framework of the Center Digitisation.Bavaria (ZD.B).","Open source software;  Open systems, Command and control;  Control structure;  Developer network;  Hierarchical organizations;  Hierarchy;  Hybrid structure;  Longitudinal study;  Open source software projects;  Organizational structures;  Project modelling, Random processes",cited By 1,Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system's (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems' feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set. © 2023 Association for Computing Machinery.",Deep Learning;  search based software engineering;  self-driving cars;  Software testing,"Zohdinasab, T. and Riccio, V. and Gambi, A. and Tonella, P.",,10.1145/3544792,Deutsche Forschungsgemeinschaft,Germany,"This work was partially supported by the H2020 project PRECRIME, funded under the ERC Advanced Grant 2017 Program (ERC Grant Agreement no. 787703), and the DFG project STUNT (DFG Grant Agreement no. FR 2955/4-1).","Autonomous vehicles;  Deep learning;  Learning systems;  Safety engineering;  Space research, Deep learning;  Feature space;  Generation techniques;  Misbehaviour;  Safety-critical domain;  Search based software engineering;  Search-based;  Software testings;  Space explorations;  Test inputs, Software testing",cited By 5,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholders' beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions. © 2023 Association for Computing Machinery.",belief;  belief fusion;  consensus;  decision-making;  domain models;  Information systems;  software;  subjective logic;  uncertainty;  vagueness,"Burgueño, L. and Muñoz, P. and Clarisó, R. and Cabot, J. and Gérard, S. and Vallecillo, A.",,10.1145/3542947,Electronic Components and Systems for European Leadership,EU,"This work is partially supported by the Spanish Government unders project LOCOSS (PID2020-114615RB-I00), CoSCA (PGC2018-094905-B-I00) and MBTI4A (P20-00067-FR); and TRANSACT, which has received funding from the ECSEL Joint Undertaking (JU) under grant agreement No 101007260. The JU receives support from the European Union’s Horizon 2020 research and innovation programme and Netherlands, Finland, Germany, Poland, Austria, Spain, Belgium. Denmark, Norway.","Information systems;  Information use, Belief;  Belief fusion;  Consensus;  Decisions makings;  Domain model;  Software;  Subjective Logic;  Uncertain informations;  Uncertainty;  Vagueness, Decision making",cited By 2,Dealing with Belief Uncertainty in Domain Models,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSecurity;  developers;  software engineering,"Lopez, T. and Sharp, H. and Bandara, A. and Tun, T. and Levine, M. and Nuseibeh, B.",,10.1145/3563211,Engineering and Physical Sciences Research Council,United Kingdom,,"Human resource management;  Security of data, Additional key word and phrasessecurity;  Critical software;  Developer;  Ethnographic study;  Key words;  Secure software;  Security activities;  Security-critical;  Software developer, Software design",cited By 1,Security Responses in Software Development,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-mémoire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-mémoire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project's lifetime and continuously improve traceability. Aide-mémoire is tailored for two specific instances of the general traceability problem - namely, commit to issue and pull request to issue links, with a focus on the latter - and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-mémoire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",link inference;  missing link;  Traceability,"Partachi, P.-P. and White, D.R. and Barr, E.T.",,10.1145/3542937,Engineering and Physical Sciences Research Council,United Kingdom,This research is supported by the EPSRC Ref. EP/J017515/1.,Collective memory;  Commit history;  Link inference;  Missing link;  Mondrian;  Real- time;  Software project;  State-of-the-art approach;  Traceability;  Work-flows,cited By 0,Aide-mémoire: Improving a Project's Collective Memory via Pull Request-Issue Links,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality. © 2023 Association for Computing Machinery.",code structure;  Code summary;  multi-head attention;  Transformer,"Gao, S. and Gao, C. and He, Y. and Zeng, J. and Nie, L. and Xia, X. and Lyu, M.",,10.1145/3522674,Engineering and Physical Sciences Research Council,United Kingdom,,"Benchmarking;  Codes (symbols);  Data flow analysis;  Learning systems;  Syntactics, Code structure;  Code summary;  Learning techniques;  Multi-head attention;  Performance;  Sequence models;  Source codes;  Structure information;  Task domain;  Transformer, Deep learning",cited By 5,Code Structure-Guided Transformer for Source Code Summarization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"RESTful APIs are a type of web service that are widely used in industry. In the past few years, a lot of effort in the research community has been spent in designing novel techniques to automatically fuzz those APIs to find faults in them. Many real faults were automatically found in a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no analysis of what is actually covered in these systems is done. Therefore, although these fuzzers are clearly useful for practitioners, we do not know their current limitations and actual effectiveness. Solving this is a necessary step to be able to design better, more efficient, and effective techniques. To address this issue, in this article we compare seven state-of-the-art fuzzers on 18 open source - 1 industrial and 1 artificial - RESTful APIs. We then analyze the source code for which parts of these APIs the fuzzers fail to generate tests. This analysis points to clear limitations of these current fuzzers, listing concrete follow-up challenges for the research community. © 2023 Copyright held by the owner/author(s).",Automated test generation;  comparison;  fuzzing;  REST;  SBST,"Zhang, M. and Arcuri, A.",,10.1145/3597205,European Commission,EU,"This work was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (EAST project, grant agreement 864972).","Open source software;  Open systems, Automated test generations;  Black boxes;  Comparison;  Current limitation;  Fuzzing;  Novel techniques;  Research communities;  REST;  SBST;  Webs services, Web services",cited By 3,Open Problems in Fuzzing RESTful APIs: A Comparison of Tools,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We devised the L+1-layer divide & conquer approach to leads-to model checking (L+1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L+1-DCA2L2MC. In a temporal logic called UNITY, designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L+1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L+1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLeads-to properties;  master-worker model;  Maude;  parallel model checking;  state space explosion,"Do, C.M. and Phyo, Y. and Riesco, A. and Ogata, K.",,10.1145/3604610,European Commission,EU,,"Optimization, Additional key word and phraseslead-to property;  Key words;  Master/worker models;  Maude;  Models checking;  Optimization techniques;  Parallel model checking;  Performance;  Property;  State-space explosion, Model checking",cited By 1,Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training;  Empirical study;  software engineering,"Liu, X. and Gu, D. and Chen, Z. and Wen, J. and Zhang, Z. and Ma, Y. and Wang, H. and Jin, X.",,10.1145/3597204,European Commission,EU,,"Cell proliferation;  Deep learning;  Large dataset;  Learning systems;  Life cycle;  Program processors;  Software testing, Distributed training;  Empirical studies;  Engineering perspective;  Fault symptoms;  Large datasets;  Learning models;  Learning software;  Rich features;  Software life cycles;  Training process, Program debugging",cited By 0,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Method name prediction;  naming convention,"Wang, S. and Wen, M. and Lin, B. and Liu, Y. and Bissyandé, T.F. and Mao, X.",,10.1145/3597203,European Commission,EU,,"Codes (symbols);  Deep learning;  Heuristic methods;  Object oriented programming, Automated tool support;  Development tasks;  Empirical analysis;  Large-scales;  Method implementations;  Method name prediction;  Naming convention;  Objectoriented programming (OOP);  Real world projects;  Research efforts, Forecasting",cited By 0,Pre-implementation Method Name Prediction for Object-oriented Programming,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"JavaScript is one of the most popular programming languages. However, its dynamic nature poses several challenges to automated testing techniques. In this paper, we propose an approach and open-source tool support to enable white-box testing of JavaScript applications using Search-Based Software Testing (SBST) techniques. We provide an automated approach to collect search-based heuristics like the common Branch Distance and to enable Testability Transformations. To empirically evaluate our results, we integrated our technique into the EvoMaster test generation tool, and carried out analyses on the automated system testing of RESTful and GraphQL APIs. Experiments on eight Web APIs running on NodeJS show that our technique leads to significantly better results than existing black-box and grey-box testing tools, in terms of code coverage and fault detection. © 2023 Copyright held by the owner/author(s).",Babel;  fuzzer;  JavaScript instrumentation;  NodeJS;  SBST;  white-box test generation,"Zhang, M. and Belhadi, A. and Arcuri, A.",,10.1145/3593801,European Commission,EU,This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 864972).,"Application programs;  Automation;  Black-box testing;  High level languages;  Open source software, Babel;  Dynamic nature;  Fuzzer;  Javascript;  Javascript instrumentation;  Nodejs;  Search-based software testing;  Test generations;  White box;  White-box test generation, Fault detection",cited By 0,JavaScript SBST Heuristics to Enable Effective Fuzzing of NodeJS Web APIs,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML. © 2023 Copyright held by the owner/author(s).",audio keyword spotting;  Bias;  design choices;  embedded machine learning;  fairness;  on-device machine learning;  personal data,"Hutiri, W.(. and Ding, A.Y. and Kawsar, F. and Mathur, A.",,10.1145/3591867,European Commission,EU,This research is partially supported by SPATIAL project that has received funding from the European Union’s Horizon 2020 research and innovation programme under Grant Agreement No. 101021808.,"Software engineering, Audio keyword spotting;  Audio keywords;  Bias;  Design choice;  Embedded machine learning;  Embedded machines;  Fairness;  Keyword spotting;  Machine-learning;  On-device machine learning, Machine learning",cited By 1,"Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection;  curriculum learning;  Cyber-physical system;  deep learning;  digital twin,"Xu, Q. and Ali, S. and Yue, T.",,10.1145/3582571,European Commission,EU,"The SWaT, WADI, and BATADAL datasets were provided by iTrust, Centre for Research in Cyber Security, Singapore University of Technology and Design. The research presented in this paper has benefited from the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract 270053. Qinghua Xu is supported by a project funded by the Norwegian Ministry of Education and Research. Shaukat Ali and Tao Yue are supported by Horizon 2020 project ADEPTNESS (871319) funded by the European Commission.","Anomaly detection;  Curricula;  Cybersecurity;  Deep learning;  E-learning;  Embedded systems;  Learning systems, Anomaly detection;  Anomaly detection methods;  Curriculum learning;  Cybe-physical systems;  Cyber-physical systems;  Deep learning;  Human learning;  Learning models;  Learning process;  Real-time data, Cyber Physical System",cited By 4,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start;  optional function elimination;  performance optimization;  Serverless computing,"Liu, X. and Wen, J. and Chen, Z. and Li, D. and Chen, J. and Liu, Y. and Wang, H. and Jin, X.",,10.1145/3585007,European Commission,EU,,"Loading, Application level;  Code-loading;  Cold-start;  General applications;  Latency optimizations;  Optional function elimination;  Performance optimizations;  Serverless computing;  Services applications;  Total response, High level languages",cited By 8,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation. © 2023 Copyright held by the owner/author(s).",fuzzing;  gRPC;  Mircoservices;  RPC;  SBST;  test generation;  Thrift,"Zhang, M. and Arcuri, A. and Li, Y. and Liu, Y. and Xue, K.",,10.1145/3585009,European Commission,EU,This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant Agreement No. 864972).,"Application programming interfaces (API);  Codes (symbols);  Fault detection;  Open source software;  Open systems;  Pipelines;  Web services, Fuzzing;  GRPC;  Industrial case study;  Industrial partners;  Mircoservice;  Remote Procedure Call;  SBST;  Test generations;  Thrift;  White box, Extraction",cited By 3,White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",literature view;  Serverless computing,"Wen, J. and Chen, Z. and Jin, X. and Liu, X.",,10.1145/3579643,European Commission,EU,,"Computation theory;  Program debugging, Application logic;  Cloud-computing;  Computing paradigm;  Error prones;  Infrastructure managements;  Literature view;  Research efforts;  Serverless computing;  Software applications;  Systematic Review, Application programs",cited By 10,Rise of the Planet of Serverless Computing: A Systematic Review,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bias mitigation;  fairness-performance trade-off;  Machine Learning,"Chen, Z. and Zhang, J.M. and Sarro, F. and Harman, M.",,10.1145/3583561,European Commission,EU,"Zhenpeng Chen, Federica Sarro, and Mark Harman are supported by the ERC Advanced Grant under the grant number 741278 (EPIC: Evolutionary Program Improvement Collaborators). Jie M. Zhang is partially supported by the UKRI Trustworthy Autonomous Systems Node in Verifiability, with Grant Award Reference EP/V026801/2.","Economic and social effects, Bias mitigation;  Empirical studies;  Fairness performance;  Fairness-performance trade-off;  Learning classifiers;  Learning performance;  Machine-learning;  Mitigation methods;  Performance metrices;  Performance tradeoff, Machine learning",cited By 3,A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Industrial elevator systems are commonly used software systems in our daily lives, which operate in uncertain environments such as unpredictable passenger traffic, uncertain passenger attributes and behaviors, and hardware delays. Understanding and assessing the robustness of such systems under various uncertainties enable system designers to reason about uncertainties, especially those leading to low system robustness, and consequently improve their designs and implementations in terms of handling uncertainties. To this end, we present a comprehensive empirical study conducted with industrial elevator systems provided by our industrial partner Orona, which focuses on assessing the robustness of a dispatcher - that is, a software component responsible for elevators' optimal scheduling. In total, we studied 90 industrial dispatchers in our empirical study. Based on the experience gained from the study, we derived an uncertainty-aware robustness assessment method (named UncerRobua) comprising a set of guidelines on how to conduct the robustness assessment and a newly proposed ranking algorithm, for supporting the robustness assessment of industrial elevator systems against uncertainties. © 2023 Association for Computing Machinery.",empirical study;  Uncertainty-aware robustness assessment,"Han, L. and Ali, S. and Yue, T. and Arrieta, A. and Arratibel, M.",,10.1145/3576041,European Commission,EU,"This work was supported by the Adeptness project funded by the European Union’s Horizon 2020 programme under grant agreement 871319. A. Arrieta is part of the Software and Systems Engineering research group of Mondragon Unibertsitatea (IT1519-22), supported by the Department of Education, Universities and Research of the Basque Country.","Traffic surveys;  Uncertainty analysis, Daily lives;  Elevator systems;  Empirical studies;  Passenger traffic;  Robustness assessment;  Software-systems;  System designers;  Uncertain environments;  Uncertainty;  Uncertainty-aware robustness assessment, Elevators",cited By 1,Uncertainty-Aware Robustness Assessment of Industrial Elevator Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction. © 2023 Copyright held by the owner/author(s).",distributed representation learning;  embeddings;  explanation;  features combination;  machine learning;  patch correctness;  Program repair,"Tian, H. and Liu, K. and Li, Y. and Kaboré, A.K. and Koyuncu, A. and Habib, A. and Li, L. and Wen, J. and Klein, J. and Bissyandé, T.F.",,10.1145/3576039,European Commission,EU,,"Classification (of information);  Codes (symbols);  Embeddings;  Forecasting;  Software testing, Distributed representation;  Distributed representation learning;  Dynamic information;  Embeddings;  Explanation;  Feature combination;  Machine-learning;  Patch correctness;  Program repair;  State of the art, Machine learning",cited By 3,The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points. © 2023 Association for Computing Machinery.",DNN debugging;  DNN explanation;  DNN functional safety analysis;  explainable AI;  heatmaps,"Fahmy, H. and Pastore, F. and Briand, L. and Stifter, T.",,10.1145/3569935,European Commission,EU,,"Embedded systems;  Genetic algorithms;  Hazards;  Image enhancement;  Program debugging;  Safety testing, Deep neural network debugging;  Deep neural network explanation;  Deep neural network functional safety analyse;  Explainable AI;  Functional Safety;  Heatmaps;  Network debugging;  Real-world image;  Safety analysis;  Safety critical systems, Deep neural networks",cited By 0,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,European Commission,EU,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesUnmanned aerial vehicles;  empirical study;  issue management;  machine learning;  safety issues,"Di Sorbo, A. and Zampetti, F. and Visaggio, A. and Di Penta, M. and Panichella, S.",,10.1145/3564821,European Commission,EU,"We gratefully acknowledge the Horizon 2020 (EU Commission) support for the project COSMOS (DevOps for Complex Cyber-physical Systems), Project No. 957254-COSMOS.","Aircraft accidents;  Aircraft detection;  Antennas;  Hazards;  Life cycle;  Risk management;  Unmanned aerial vehicles (UAV), Additional key word and phrasesunmanned aerial vehicle;  Aerial vehicle;  Automated identification;  Empirical studies;  Issue managements;  Key words;  Machine-learning;  Safety issues;  Safety-Related;  Unmanned aerial vehicle systems, Machine learning",cited By 2,Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesContinuous Integration and Delivery;  Cyber-Physical Systems;  empirical software engineering,"Zampetti, F. and Tamburri, D. and Panichella, S. and Panichella, A. and Canfora, G. and Di Penta, M.",,10.1145/3571854,European Commission,EU,"This work was supported by Horizon 2020 (EU Commission) for the project COSMOS (DevOps for Complex Cyber-physical Systems), project number 957254-COSMOS.","Balancing;  Embedded systems;  Hardware-in-the-loop simulation;  Integration;  Software design;  Synthetic apertures, Additional key word and phrasescontinuous integration and delivery;  Continuous integrations;  Cybe-physical systems;  Cyber-physical systems;  Development and operations;  Empirical Software Engineering;  Hardware in the loops;  Hardware/software;  Key words;  Release cycles, Cyber Physical System",cited By 0,Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value <=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible. © 2023 Copyright held by the owner/author(s).",Autonomous systems;  software simulation;  test case prioritization,"Birchler, C. and Khatiri, S. and Derakhshanfar, P. and Panichella, S. and Panichella, A.",,10.1145/3533818,European Commission,EU,"We gratefully acknowledge the Horizon 2020 (EU Commission) support for the project COSMOS (DevOps for Complex Cyber-physical Systems), Project No. 957254-COSMOS.","Autonomous vehicles;  Economic and social effects;  Fault detection;  Safety engineering;  Software testing;  Virtual reality, Autonomous system;  In-field;  Multi-objective tests;  Simulation environment;  Single objective;  Software simulation;  Static features;  Test case;  Test case prioritization;  Virtual tests, Genetic algorithms",cited By 7,Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholders' beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions. © 2023 Association for Computing Machinery.",belief;  belief fusion;  consensus;  decision-making;  domain models;  Information systems;  software;  subjective logic;  uncertainty;  vagueness,"Burgueño, L. and Muñoz, P. and Clarisó, R. and Cabot, J. and Gérard, S. and Vallecillo, A.",,10.1145/3542947,European Commission,EU,,"Information systems;  Information use, Belief;  Belief fusion;  Consensus;  Decisions makings;  Domain model;  Software;  Subjective Logic;  Uncertain informations;  Uncertainty;  Vagueness, Decision making",cited By 2,Dealing with Belief Uncertainty in Domain Models,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it ""blindly""injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program's relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%. © 2023 Copyright held by the owner/author(s).",bug reports;  Fault injection;  information retrieval;  mutation,"Khanfir, A. and Koyuncu, A. and Papadakis, M. and Cordy, M. and Bissyandé, T.F. and Klein, J. and Le Traon, Y.",,10.1145/3542946,European Commission,EU,,"Cosine transforms;  Semantics;  Software testing, Bug reports;  Change patterns;  Code transformation;  Deep knowledge;  Fault injection;  Mutation;  Mutation testing;  Real-world;  Repair system;  Software fault, Fault detection",cited By 4,iBiR: Bug-report-driven Fault Injection,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed. © 2023 Association for Computing Machinery.",Android application;  hidden sensitive operations;  privacy leak;  program analysis,"Sun, X. and Chen, X. and Li, L. and Cai, H. and Grundy, J. and Samhi, J. and Bissyandé, T. and Klein, J.",,10.1145/3574158,European Commission,EU,,"Android (operating system);  Android malware;  Application programs;  Large dataset;  Mobile security;  Static analysis, Android applications;  Android apps;  Attack mechanism;  Behavior implementation;  Hidden sensitive operation;  Malicious behavior;  Malware writers;  Privacy leak;  Program analysis;  Sensitive datas, Sensitive data",cited By 1,Demystifying Hidden Sensitive Operations in Android Apps,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system's (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems' feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set. © 2023 Association for Computing Machinery.",Deep Learning;  search based software engineering;  self-driving cars;  Software testing,"Zohdinasab, T. and Riccio, V. and Gambi, A. and Tonella, P.",,10.1145/3544792,European Commission,EU,,"Autonomous vehicles;  Deep learning;  Learning systems;  Safety engineering;  Space research, Deep learning;  Feature space;  Generation techniques;  Misbehaviour;  Safety-critical domain;  Search based software engineering;  Search-based;  Software testings;  Space explorations;  Test inputs, Software testing",cited By 5,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Estimating worst-case execution time (WCET) is an important activity at early design stages of real-time systems. Based on WCET estimates, engineers make design and implementation decisions to ensure that task executions always complete before their specified deadlines. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines and, hence, operate safely with a probabilistic guarantee. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring probabilistic safe WCET ranges. We evaluated our approach by applying it to three industrial systems from different domains and several synthetic systems. Our approach efficiently and accurately estimates probabilistic safe WCET ranges within which deadlines are likely to be satisfied with a high degree of confidence. © 2023 Association for Computing Machinery.",machine learning;  meta-heuristic search;  Schedulability analysis;  search-based software engineering;  worst-case execution time,"Lee, J. and Shin, S.Y. and Nejati, S. and Briand, L. and Parache, Y.I.",,10.1145/3546941,European Commission,EU,,"Heuristic algorithms;  Interactive computer systems;  Machine learning;  Software engineering, Design stage;  Machine-learning;  Meta-heuristic search;  Probabilistics;  Real - Time system;  Schedulability analysis;  Search-based;  Search-based software engineering;  Time range;  Worst-case execution time, Real time systems",cited By 0,Estimating Probabilistic Safe WCET Ranges of Real-Time Systems at Design Stages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems' dataset through statistical methods. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMicroservice architecture;  microservice security;  software architecture detectors;  software architecture metrics,"Zdun, U. and Queval, P.-J. and Simhandl, G. and Scandariato, R. and Chakravarty, S. and Jelic, M. and Jovanovic, A.",,10.1145/3532183,European Commission,EU,,"Observability;  Open source software;  Open systems;  Secure communication, Additional key word and phrasesmicroservice architecture;  Application systems;  Architectural design decisions;  Best practices;  Identity management;  Key words;  Microservice security;  Security metrics;  Software architecture detector;  Software architecture metric, Software architecture",cited By 5,"Microservice Security Metrics for Secure Communication, Identity Management, and Observability",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,European Regional Development Fund,EU,,"Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers' profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCollaborative software engineering;  model-driven engineering;  search-based software engineering,"Pérez, F. and Lapeña, R. and Marcén, A. and Cetina, C.",,10.1145/3561384,European Regional Development Fund,EU,,"Computer software maintenance, Additional key word and phrasescollaborative software engineering;  Bug localizations;  Feature location;  Key words;  Link recoveries;  Maintenance tasks;  Model-driven Engineering;  Search-based;  Search-based software engineering;  Traceability links, Engineers",cited By 0,How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,Excellent Young Scientists Fund,China,,"Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training;  Empirical study;  software engineering,"Liu, X. and Gu, D. and Chen, Z. and Wen, J. and Zhang, Z. and Ma, Y. and Wang, H. and Jin, X.",,10.1145/3597204,Excellent Young Scientists Fund,China,"This work was supported by the National Natural Science Foundation of China under the grant numbers 62172008 and 62102009, the National Natural Science Fund for the Excellent Young Scientists Fund Program (Overseas), and Center for Data Space Technology and System, Peking University. Zhenpeng Chen was supported by the ERC Advanced Grant under the grant number 741278 (EPIC: Evolutionary Program Improvement Collaborators).","Cell proliferation;  Deep learning;  Large dataset;  Learning systems;  Life cycle;  Program processors;  Software testing, Distributed training;  Empirical studies;  Engineering perspective;  Fault symptoms;  Large datasets;  Learning models;  Learning software;  Rich features;  Software life cycles;  Training process, Program debugging",cited By 0,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start;  optional function elimination;  performance optimization;  Serverless computing,"Liu, X. and Wen, J. and Chen, Z. and Li, D. and Chen, J. and Liu, Y. and Wang, H. and Jin, X.",,10.1145/3585007,Excellent Young Scientists Fund,China,"This work was supported by the National Key Research and Development Program of China (Grant No. 2020YFB2104100), the National Natural Science Foundation of China under the Grants No. 62172008 and No. 62172009, and the National Natural Science Fund for the Excellent Young Scientists Fund Program (Overseas). Zhenpeng Chen is supported by the ERC Advanced Grant under the Grant No. 741278 (EPIC: Evolutionary Program Improvement Collaborators).","Loading, Application level;  Code-loading;  Cold-start;  General applications;  Latency optimizations;  Optional function elimination;  Performance optimizations;  Serverless computing;  Services applications;  Total response, High level languages",cited By 8,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",literature view;  Serverless computing,"Wen, J. and Chen, Z. and Jin, X. and Liu, X.",,10.1145/3579643,Excellent Young Scientists Fund,China,This work is supported in part by the R&D Projects in Key Areas of Guangdong Province under the grant number 2020B010164002. Zhenpeng Chen is supported by the ERC Advanced Grant under the grant number 741278 (EPIC: Evolutionary Program Improvement Collaborators). Xin Jin is supported by the National Natural Science Foundation of China under the grant number 62172008 and the National Natural Science Fund for the Excellent Young Scientists Fund Program (Overseas).,"Computation theory;  Program debugging, Application logic;  Cloud-computing;  Computing paradigm;  Error prones;  Infrastructure managements;  Literature view;  Research efforts;  Serverless computing;  Software applications;  Systematic Review, Application programs",cited By 10,Rise of the Planet of Serverless Computing: A Systematic Review,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,Exploratory Research for Advanced Technology,Japan,,"Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes. © 2023 Association for Computing Machinery.",industry;  machine learning;  Regression testing;  taxonomy;  test case prioritisation,"Ramírez, A. and Feldt, R. and Romero, J.R.",,10.1145/3511805,Federacion Espanola de Enfermedades Raras,Spain,,"Cost benefit analysis;  Software testing;  Transmission control protocol, Adverse effect;  Classifieds;  Industry needs;  Machine-learning;  Re-runs;  Regression testing;  Software company;  Systems under tests;  Test case;  Test case prioritization, Machine learning",cited By 2,"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker's on-execution validation of DNN-based program's properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMD's offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs. © 2023 Association for Computing Machinery.",Neural networks;  property-based debugging;  training programs,"Ben Braiek, H. and Khomh, F.",,10.1145/3529318,Fonds de Recherche du Quebec,Canada,,"Aircraft accidents;  Errors;  Feedforward neural networks;  Neural network models;  Program debugging;  Software testing;  Training aircraft, Aircraft collision avoidance systems;  Car collisions;  Neural network model;  Neural networks trainings;  Neural-networks;  Performance;  Property-based;  Property-based debugging;  Safety critical systems;  Training program, Deep neural networks",cited By 2,Testing Feedforward Neural Networks Training Programs,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks. © 2023 Association for Computing Machinery.",Automatic program repair;  deep learning,"Sohn, J. and Kang, S. and Yoo, S.",,10.1145/3563210,Fonds National de la Recherche Luxembourg,Luxembourg,"S. Kang and S. Yoo were supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-2018R1A5A1059921), as well as the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (no. 2018-0-00769, Neuromorphic Computing Software Platform for Artificial Intelligence Systems). J. Sohn was supported by the Luxembourg National Research Funds (FNR) through CORE project C18/IS/12669767/STELLAR.","Evolutionary algorithms;  Optimization;  Repair, Automatic program repair;  Automatic programs;  Data collection;  Deep learning;  Differential Evolution;  Input-output;  Misbehaviour;  Neural weights;  Repair techniques;  Search-based, Deep neural networks",cited By 5,Arachne: Search-Based Repair of Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points. © 2023 Association for Computing Machinery.",DNN debugging;  DNN explanation;  DNN functional safety analysis;  explainable AI;  heatmaps,"Fahmy, H. and Pastore, F. and Briand, L. and Stifter, T.",,10.1145/3569935,Fonds National de la Recherche Luxembourg,Luxembourg,,"Embedded systems;  Genetic algorithms;  Hazards;  Image enhancement;  Program debugging;  Safety testing, Deep neural network debugging;  Deep neural network explanation;  Deep neural network functional safety analyse;  Explainable AI;  Functional Safety;  Heatmaps;  Network debugging;  Real-world image;  Safety analysis;  Safety critical systems, Deep neural networks",cited By 0,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDNN explanation;  clustering;  DNN debugging;  DNN functional safety analysis;  transfer learning,"Attaoui, M. and Fahmy, H. and Pastore, F. and Briand, L.",,10.1145/3550271,Fonds National de la Recherche Luxembourg,Luxembourg,,"Clustering algorithms;  Errors;  Learning systems;  Risk analysis;  Risk assessment;  Safety engineering, Additional key word and phrasesdnn explanation;  Clusterings;  Deep neural network debugging;  Deep neural network functional safety analyse;  Functional Safety;  Key words;  Network debugging;  Root cause;  Safety analysis;  Transfer learning, Deep neural networks",cited By 2,Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it ""blindly""injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program's relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%. © 2023 Copyright held by the owner/author(s).",bug reports;  Fault injection;  information retrieval;  mutation,"Khanfir, A. and Koyuncu, A. and Papadakis, M. and Cordy, M. and Bissyandé, T.F. and Klein, J. and Le Traon, Y.",,10.1145/3542946,Fonds National de la Recherche Luxembourg,Luxembourg,"This work was supported by the Luxembourg National Research Fund (FNR) TestFast Project, ref. 12630949 and partially supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 949014 for project NATURAL).","Cosine transforms;  Semantics;  Software testing, Bug reports;  Change patterns;  Code transformation;  Deep knowledge;  Fault injection;  Mutation;  Mutation testing;  Real-world;  Repair system;  Software fault, Fault detection",cited By 4,iBiR: Bug-report-driven Fault Injection,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed. © 2023 Association for Computing Machinery.",Android application;  hidden sensitive operations;  privacy leak;  program analysis,"Sun, X. and Chen, X. and Li, L. and Cai, H. and Grundy, J. and Samhi, J. and Bissyandé, T. and Klein, J.",,10.1145/3574158,Fonds National de la Recherche Luxembourg,Luxembourg,"This work was partly supported by the Australian Research Council ( ARC ) under a Laureate Fellowship project FL190100035, a Discovery Early Career Researcher Award ( DECRA ) project DE200100016, and a Discovery project DP200100020, by the Luxembourg National Research Fund ( FNR ) (under project CHARACTERIZE C17/IS/11693861), by the SPARTA project, which has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No. 830892.","Android (operating system);  Android malware;  Application programs;  Large dataset;  Mobile security;  Static analysis, Android applications;  Android apps;  Attack mechanism;  Behavior implementation;  Hidden sensitive operation;  Malicious behavior;  Malware writers;  Privacy leak;  Program analysis;  Sensitive datas, Sensitive data",cited By 1,Demystifying Hidden Sensitive Operations in Android Apps,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class - such as making it public - can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSerialization;  deserialization;  gadget;  remote code execution RCE;  vulnerabilities,"Sayar, I. and Bartel, A. and Bodden, E. and Le Traon, Y.",,10.1145/3554732,Fonds National de la Recherche Luxembourg,Luxembourg,"This work was supported by the Luxembourg National Research Fund (FNR) ONNIVA Project, ref. 12696663. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.","Codes (symbols);  Java programming language;  Security of data, Additional key word and phrasesserialization;  Code execution;  Deserialization;  Gadget;  Key words;  Remote code;  Remote code execution remote code execution;  Vulnerability, Libraries",cited By 3,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Context:When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing, a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique.Objective: We conduct an exploratory study to investigate the properties of commit-relevant mutants, i.e., the test elements of commit-aware mutation testing, by proposing a general definition and an experimental approach to identify them. We thus aim at investigating the prevalence, location, and comparative advantages of commit-aware mutation testing over time (i.e., the program evolution). We also investigate the predictive power of several commit-related features in identifying and selecting commit-relevant mutants to understand the essential properties for its best-effort application case.Method: Our commit-relevant definition relies on the notion of observational slicing, approximated by higher-order mutation. Specifically, our approach utilizes the impact of mutants, effects of one mutant on another in capturing and analyzing the implicit interactions between the changed and unchanged code parts. The study analyses millions of mutants (over 10 million), 288 commits, five (5) different open-source software projects involving over 68,213 CPU days of computation and sets a ground truth where we perform our analysis.Results: Our analysis shows that commit-relevant mutants are located mainly outside of program commit change (81%), suggesting a limitation in previous work. We also note that effective selection of commit-relevant mutants has the potential of reducing the number of mutants by up to 93%. In addition, we demonstrate that commit relevant mutation testing is significantly more effective and efficient than state-of-the-art baselines, i.e., random mutant selection and analysis of only mutants within the program change. In our analysis of the predictive power of mutants and commit-related features (e.g., number of mutants within a change, mutant type, and commit size) in predicting commit-relevant mutants, we found that most proxy features do not reliably predict commit-relevant mutants.Conclusion: This empirical study highlights the properties of commit-relevant mutants and demonstrates the importance of identifying and selecting commit-relevant mutants when testing evolving software systems. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  continuous integration;  evolving-systems;  mutation testing,"Ojdanic, M. and Soremekun, E. and Degiovanni, R. and Papadakis, M. and Traon, Y.L.",,10.1145/3530786,Fonds National de la Recherche Luxembourg,Luxembourg,This work is supported by PayPal and the Luxembourg National Research Funds (FNR) through the CORE project grant C17/IS/11686509/CODEMATES.,"Codes (symbols);  Integration;  Integration testing;  Open source software;  Open systems;  Verification, Additional key word and phrasessoftware testing;  Continuous integrations;  Evolving systems;  Key words;  Mutation testing;  Predictive power;  Program behavior;  Property;  Testing process;  Testing technique, Semantics",cited By 2,Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages' builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider's version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breaking changes;  change impact;  dependency management;  npm;  Semantic Version,"Venturini, D. and Cogo, F.R. and Polato, I. and Gerosa, M.A. and Wiese, I.S.",,10.1145/3576037,Fundacao de Amparo a Pesquisa do Estado de Sao Paulo,Brazil,"This work is partially supported by the National Science Foundation under Grant Number IIS-1815503, CNPq/MCTI/FNDCT (grant #408812/2021-4), and MCTIC/CGI/FAPESP (grant #2021/06662-1).","Recovery, Breaking change;  Breakings;  Bug fixes;  Change impacts;  Complex software systems;  Dependency management;  Down-stream;  Empirical studies;  Npm;  Semantic version, Semantics",cited By 0,I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  mining software repositories;  software evolution;  software testing,"Sun, W. and Yan, M. and Liu, Z. and Xia, X. and Lei, Y. and Lo, D.",,10.1145/3607183,Fundamental Research Funds for the Central Universities,China,,"Codes (symbols);  Computer software maintenance;  Corrective maintenance;  Just in time production, Additional key word and phrasesempirical software engineering;  Co-evolution;  Key words;  Mining software;  Mining software repository;  Production test;  Software Evolution;  Software repositories;  Software testings;  Test code, Software testing",cited By 0,Revisiting the Identification of the Co-evolution of Production and Test Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing - that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality - that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  fairness;  robustness;  testing,"Chen, J. and Wang, J. and Ma, X. and Sun, Y. and Sun, J. and Zhang, P. and Cheng, P.",,10.1145/3582573,Fundamental Research Funds for the Central Universities,China,,"Deep learning;  Image enhancement;  Quality assurance;  Well testing, Deep learning;  Engineering techniques;  Fairness;  Guided search;  Model properties;  Modeling quality;  Property;  Robustness;  Test case;  Testing framework, Learning systems",cited By 1,QuoTe: Quality-oriented Testing for Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. ""1-to-1""mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of ""1-to-n""(one binary function matches multiple source functions or binary functions) or even ""n-to-n""(multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the ""1-to-1""mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy. © 2023 Association for Computing Machinery.",1-to-1;  1-to-n;  Binary similarity analysis;  function inlining,"Jia, A. and Fan, M. and Jin, W. and Xu, X. and Zhou, Z. and Tang, Q. and Nie, S. and Wu, S. and Liu, T.",,10.1145/3561385,Fundamental Research Funds for the Central Universities,China,,"Computer software reusability;  Open systems;  Software testing;  Supply chains, 1-to-1;  1-to-n;  Binary files;  Binary functions;  Binary similarity analyse;  Function inlining;  Inlining;  Performance;  Similarity analysis;  Simulation strategies, Open source software",cited By 1,1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques. © 2023 Association for Computing Machinery.",adaptive partition testing;  Additional Key Words and PhrasesMetamorphic testing;  feedback control;  metamorphic relation;  random testing;  test execution,"Sun, C.-A. and Dai, H. and Liu, H. and Chen, T.Y.",,10.1145/3533314,Fundamental Research Funds for the Central Universities,China,,"Application programs;  Feedback control;  Information use;  Open source software;  Software testing;  Verification;  Well testing, Adaptive partition testing;  Adaptive partitions;  Additional key word and phrasesmetamorphic testing;  Key words;  Metamorphic relations;  Metamorphic testing;  Partition testing;  Random testing;  Test case;  Test execution, Cost effectiveness",cited By 1,Feedback-Directed Metamorphic Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Trace data is critical for fault localization (FL) to analyze suspicious statements potentially responsible for a failure. However, existing trace representation meets its bottleneck mainly in two aspects: (1) the trace information of a statement is restricted to a local context (i.e., a test case) without the consideration of a global context (i.e., all test cases of a test suite); (2) it just uses the goccurrence' for representation without strong FL semantics. Thus, we propose UNITE: an inflUential coNtext-GuIded Trace rEpresentation, representing the trace from both global and local contexts with influential semantics for FL. UNITE embodies and implements two key ideas: (1) UNITE leverages the widely used weighting capability from local and global contexts of information retrieval to reflect how important a statement (a word) is to a test case (a document) in all test cases of a test suite (a collection), where a test case (a document) and all test cases of a test suite (a collection) represent local and global contexts respectively; (2) UNITE further elaborates the trace representation from goccurrence' (weak semantics) to ginfluence' (strong semantics) by combing program dependencies. The large-scale experiments on 12 FL techniques and 20 programs show that UNITE significantly improves FL effectiveness. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFault localization;  program dependence;  statement weighting;  suspiciousness;  trace representation,"Zhang, Z. and Lei, Y. and Su, T. and Yan, M. and Mao, X. and Yu, Y.",,10.1145/3576043,Fundamental Research Funds for the Central Universities,China,,"Software testing, Additional key word and phrasesfault localization;  Fault localization;  Global context;  Key words;  Localisation;  Program dependence;  Statement weighting;  Suspiciousness;  Test case;  Trace representations, Semantics",cited By 5,Influential Global and Local Contexts Guided Trace Representation for Fault Localization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,General Research Fund of Shanghai Normal University,China,,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,Glaucoma Research Foundation,United States,,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers' profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCollaborative software engineering;  model-driven engineering;  search-based software engineering,"Pérez, F. and Lapeña, R. and Marcén, A. and Cetina, C.",,10.1145/3561384,Gobierno de Aragn,Spain,,"Computer software maintenance, Additional key word and phrasescollaborative software engineering;  Bug localizations;  Feature location;  Key words;  Link recoveries;  Maintenance tasks;  Model-driven Engineering;  Search-based;  Search-based software engineering;  Traceability links, Engineers",cited By 0,How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code knowledge graph;  code recommendation;  crowd-scale coding practice checking;  knowledge deduction;  Smart contract,"Huang, Q. and Liao, D. and Xing, Z. and Zuo, Z. and Wang, C. and Xia, X.",,10.1145/3597206,Graduate Innovative Special Fund Projects of Jiangxi Province,China,"The work was partly supported by the National Nature Science Foundation of China (grant 62262031), the Science and Technology Key Project of Education Department of Jiangxi Province (GJJ210307, GJJ2200302), and the Graduate Innovative Special Fund Projects of Jiangxi Province (YC2021-S308, YC2022-S258).","Cloning;  Codes (symbols);  Flow graphs;  Knowledge graph;  Semantics;  Software engineering, Code knowledge graph;  Code recommendation;  Code reuse;  Crowd-scale coding practice checking;  Current codes;  Functional failure;  Knowledge deduction;  Knowledge graphs;  Mining methods;  Non-functional, Smart contract",cited By 0,Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSource code editing;  deep learning;  Pre-training,"Li, J. and Li, G. and Li, Z. and Jin, Z. and Hu, X. and Zhang, K. and Fu, Z.",,10.1145/3597207,"Hangzhou High-Flyer AI Fundamental Research Co., Ltd.",China,"The AI training platform supporting this work was provided by High-Flyer AI (Hangzhou High-Flyer AI Fundamental Research Co., Ltd.).","Codes (symbols);  Learning systems;  Modeling languages;  Natural language processing systems;  Software design;  Zero-shot learning, Additional key word and phrasessource code editing;  Automatic codes;  Deep learning;  Fine tuning;  Ground truth;  Infilling;  Key words;  Pre-training;  State of the art;  Test data, Deep learning",cited By 2,CodeEditor: Learning to Edit Source Code with Pre-trained Models,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software projects are complex technical and organizational systems involving large numbers of artifacts and developers. To understand and tame software complexity, a wide variety of program analysis techniques have been developed for bug detection, program comprehension, verification, and more. At the same time, repository mining techniques aim at obtaining insights into the inner socio-technical workings of software projects at a larger scale. While both program analysis and repository mining have been successful on their own, they are largely isolated, which leaves considerable potential for synergies untapped. We present SEAL, the first integrated approach that combines low-level program analysis with high-level repository information. SEAL maps repository information, mined from the development history of a project, onto a low-level intermediate program representation, making it available for state-of-the-art program analysis. SEAL's integrated approach allows us to efficiently address software engineering problems that span multiple levels of abstraction, from low-level data flow to high-level organizational information. To demonstrate its merits and practicality, we use SEAL to determine which code changes modify central parts of a given software project, how authors interact (indirectly) with each other through code, and we demonstrate that putting static analysis' results into a socio-technical context improves their expressiveness and interpretability. © 2023 Copyright held by the owner/author(s).",socio-technical software analytics;  software repository mining;  Static program analysis,"Sattler, F. and Böhm, S. and Schubert, P.D. and Siegmund, N. and Apel, S.",,10.1145/3585008,Heinz Nixdorf Stiftung,Germany,"This work was partially supported by the Heinz Nixdorf Foundation and the German Research Foundation (DFG) within the Collaborative Research Center 901 “On-The-Fly Computing” (grant no. 160364472) and Collaborative Research Center TRR 248 “Perspicuous Computing” (grant no. 389792660), project “Pervolution” (grant no. AP 206/11-1, AP 206/11-2, SI 2171/2-1, and SI 2171/2-2), project “Green Configuration” (grant no. SI 2171/3-1), and project “Congruence” (grant no. AP 206/14-1).","Codes (symbols);  Integrated control;  Verification, Complex technical systems;  Integrated approach;  Organizational system;  Program analysis;  Repository mining;  Socio-technical software analytic;  Sociotechnical;  Software project;  Software repository mining;  Static program analysis, Static analysis",cited By 0,SEAL: Integrating Program Analysis and Repository Mining,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation;  data structure analysis;  value-flow analysis,"Wang, C. and Wang, W. and Yao, P. and Shi, Q. and Zhou, J. and Xiao, X. and Zhang, C.",,10.1145/3565800,Huawei,China,,"Data structures;  Flow graphs;  Graphic methods;  Value engineering, Additional key word and phrasesabstract interpretation;  Data structure analyse;  False positive;  Flow-graphs;  Key words;  Memory layout;  Structure analysis;  Ubiquitous data;  Value flow;  Value flow analysis, Containers",cited By 0,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation;  data structure analysis;  value-flow analysis,"Wang, C. and Wang, W. and Yao, P. and Shi, Q. and Zhou, J. and Xiao, X. and Zhang, C.",,10.1145/3565800,Innovation and Technology Commission,Hong Kong (China),,"Data structures;  Flow graphs;  Graphic methods;  Value engineering, Additional key word and phrasesabstract interpretation;  Data structure analyse;  False positive;  Flow-graphs;  Key words;  Memory layout;  Structure analysis;  Ubiquitous data;  Value flow;  Value flow analysis, Containers",cited By 0,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum;  bookkeeping recognition;  smart contract;  token,"He, Z. and Song, S. and Bai, Y. and Luo, X. and Chen, T. and Zhang, W. and He, P. and Li, H. and Lin, X. and Zhang, X.",,10.1145/3560263,Innovation and Technology Fund,Hong Kong (China),,"Blockchain, Additional key word and phrasesethereum;  Behavior detection;  Block-chain;  Bookkeeping recognition;  Bytecodes;  Key words;  Malicious behavior;  Optimisations;  Token;  Type information, Smart contract",cited By 3,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks. © 2023 Association for Computing Machinery.",Automatic program repair;  deep learning,"Sohn, J. and Kang, S. and Yoo, S.",,10.1145/3563210,Institute for Information and Communications Technology Promotion,South Korea,,"Evolutionary algorithms;  Optimization;  Repair, Automatic program repair;  Automatic programs;  Data collection;  Deep learning;  Differential Evolution;  Input-output;  Misbehaviour;  Neural weights;  Repair techniques;  Search-based, Deep neural networks",cited By 5,Arachne: Search-Based Repair of Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning systems;  Test adequacy,"Kim, J. and Feldt, R. and Yoo, S.",,10.1145/3546947,Institute for Information and Communications Technology Promotion,South Korea,,"Autonomous vehicles;  Large dataset;  Learning systems;  Medical imaging;  Safety engineering;  Software testing;  Statistical tests, Autonomous driving;  Coverage metrics;  Deep learning system;  Safety-critical domain;  Software testings;  System testing;  Test adequacies;  Test adequacy criteria;  Training data;  Training dataset, Deep neural networks",cited By 0,Evaluating Surprise Adequacy for Deep Learning System Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch;  certified accuracy;  deep learning;  neural networks;  robustness certification;  vision transformer,"Huang, Y. and Ma, L. and Li, Y.",,10.1145/3591870,Japan Science and Technology Agency/Corporation,Japan,,"Accident prevention;  Convolutional neural networks;  Deep neural networks;  Electric transformer testing;  Learning systems, Adversarial patch;  Certified accuracy;  Classical neural networks;  Deep learning;  Exhaustive testing;  Neural-networks;  Performance;  Record-breaking performance;  Robustness certification;  Vision transformer, Cameras",cited By 2,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  DNN repair;  neural architecture search,"Qi, H. and Wang, Z. and Guo, Q. and Chen, J. and Juefei-Xu, F. and Zhang, F. and Ma, L. and Zhao, J.",,10.1145/3585005,Japan Science and Technology Agency/Corporation,Japan,,"Multilayer neural networks;  Network architecture;  Network layers;  Repair;  Spectrum analysis, Applications domains;  Deep learning;  Deep neural network repair;  Industrial tasks;  Localisation;  Network repairs;  Neural architecture search;  Neural architectures;  Overfitting;  Real-world, Deep neural networks",cited By 0,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,Japan Science and Technology Agency/Corporation,Japan,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,Japan Science and Technology Agency/Corporation,Japan,,"Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, ""a given action is always followed by another action before a given deadline"". This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant. © 2023 Association for Computing Machinery.",Monitoring;  parametric timed automata;  real-time systems,"Waga, M. and André, É. and Hasuo, I.",,10.1145/3517194,Japan Science and Technology Agency/Corporation,Japan,"This work is partially supported by JST ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603), by JST ACT-X Grant No. JPMJAX200U, by JSPS Grants-in-Aid No. 15KT0012 & 18J22498, by JST CEREST Grant No. JPMJCR2012, by the ANR national research program PACS (ANR-14-CE28-0002) and by the ANR-NRF French-Singaporean research program ProMiS (ANR-19-CE25-0015).","Interactive computer systems;  Model checking;  Pattern matching;  Specifications;  Uncertainty analysis, Optimisations;  Parametric timed automata;  Pattern-matching;  Property;  Real - Time system;  Timed model checking;  Timing parameters, Real time systems",cited By 3,Parametric Timed Pattern Matching,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch;  certified accuracy;  deep learning;  neural networks;  robustness certification;  vision transformer,"Huang, Y. and Ma, L. and Li, Y.",,10.1145/3591870,Japan Society for the Promotion of Science,Japan,,"Accident prevention;  Convolutional neural networks;  Deep neural networks;  Electric transformer testing;  Learning systems, Adversarial patch;  Certified accuracy;  Classical neural networks;  Deep learning;  Exhaustive testing;  Neural-networks;  Performance;  Record-breaking performance;  Robustness certification;  Vision transformer, Cameras",cited By 2,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  DNN repair;  neural architecture search,"Qi, H. and Wang, Z. and Guo, Q. and Chen, J. and Juefei-Xu, F. and Zhang, F. and Ma, L. and Zhao, J.",,10.1145/3585005,Japan Society for the Promotion of Science,Japan,,"Multilayer neural networks;  Network architecture;  Network layers;  Repair;  Spectrum analysis, Applications domains;  Deep learning;  Deep neural network repair;  Industrial tasks;  Localisation;  Network repairs;  Neural architecture search;  Neural architectures;  Overfitting;  Real-world, Deep neural networks",cited By 0,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,Japan Society for the Promotion of Science,Japan,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,Japan Society for the Promotion of Science,Japan,,"Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, ""a given action is always followed by another action before a given deadline"". This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant. © 2023 Association for Computing Machinery.",Monitoring;  parametric timed automata;  real-time systems,"Waga, M. and André, É. and Hasuo, I.",,10.1145/3517194,Japan Society for the Promotion of Science,Japan,,"Interactive computer systems;  Model checking;  Pattern matching;  Specifications;  Uncertainty analysis, Optimisations;  Parametric timed automata;  Pattern-matching;  Property;  Real - Time system;  Timed model checking;  Timing parameters, Real time systems",cited By 3,Parametric Timed Pattern Matching,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call ""subtle"") group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness;  fairness improvement;  fairness testing;  machine learning,"Zhang, M. and Sun, J. and Wang, J. and Sun, B.",,10.1145/3591869,Key Research and Development Program of Zhejiang Province,China,,"Learning systems, Condition;  Fairness;  Fairness improvement;  Fairness testing;  Input space;  Interpretable rules;  Machine learning applications;  Machine-learning;  Neural-networks;  Sensitive attribute, Machine learning",cited By 2,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code knowledge graph;  code recommendation;  crowd-scale coding practice checking;  knowledge deduction;  Smart contract,"Huang, Q. and Liao, D. and Xing, Z. and Zuo, Z. and Wang, C. and Xia, X.",,10.1145/3597206,Key Science and Technology Research Project in Jiangxi Province Department of Education,China,,"Cloning;  Codes (symbols);  Flow graphs;  Knowledge graph;  Semantics;  Software engineering, Code knowledge graph;  Code recommendation;  Code reuse;  Crowd-scale coding practice checking;  Current codes;  Functional failure;  Knowledge deduction;  Knowledge graphs;  Mining methods;  Non-functional, Smart contract",cited By 0,Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model Driven Engineering (MDE) is a general-purpose engineering methodology to elevate system design, maintenance, and analysis to corresponding activities on models. Models (graphical and/or textual) of a target application are automatically transformed into source code, performance models, Promela files (for model checking), and so on for system analysis and construction.Models are instances of metamodels. One form an MDE metamodel can take is a [class diagram, constraints] pair: the class diagram defines all object diagrams that could be metamodel instances; object constraint language (OCL) constraints eliminate semantically undesirable instances.A metamodel refactoring is an invertible semantics-preserving co-transformation, i.e., it transforms both a metamodel and its models without losing data. This article addresses a subproblem of metamodel refactoring: how to prove the correctness of refactorings of class diagrams without OCL constraints using the Coq Proof Assistant. © 2023 Association for Computing Machinery.",Class diagram refactorings;  Coq;  object diagram refactorings,"Altoyan, N. and Batory, D.",,10.1145/3549541,King Abdulaziz City for Science and Technology,Saudi Arabia,Altoyan was supported by King Abdulaziz City for Science and Technology (KACST). This work was also supported by NSF Grant no. 26-1005-25 (Award no. 1212683).,"Metadata;  Model checking;  Semantics, Class diagram refactoring;  Class diagrams;  Coq;  Meta model;  Model-driven Engineering;  Object Constraint Language;  Object diagram refactoring;  Object diagrams;  Refactorings, Systems analysis",cited By 1,On Proving the Correctness of Refactoring Class Diagrams of MDE Metamodels,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection;  curriculum learning;  Cyber-physical system;  deep learning;  digital twin,"Xu, Q. and Ali, S. and Yue, T.",,10.1145/3582571,"Kirke-, Utdannings- og Forskningsdepartementet",Norway,,"Anomaly detection;  Curricula;  Cybersecurity;  Deep learning;  E-learning;  Embedded systems;  Learning systems, Anomaly detection;  Anomaly detection methods;  Curriculum learning;  Cybe-physical systems;  Cyber-physical systems;  Deep learning;  Human learning;  Learning models;  Learning process;  Real-time data, Cyber Physical System",cited By 4,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software bloat is code that is packaged in an application but is actually not necessary to run the application. The presence of software bloat is an issue for security, performance, and for maintenance. In this article, we introduce a novel technique for debloating, which we call coverage-based debloating. We implement the technique for one single language: Java bytecode. We leverage a combination of state-of-the-art Java bytecode coverage tools to precisely capture what parts of a project and its dependencies are used when running with a specific workload. Then, we automatically remove the parts that are not covered, in order to generate a debloated version of the project. We succeed to debloat 211 library versions from a dataset of 94 unique open-source Java libraries. The debloated versions are syntactically correct and preserve their original behaviour according to the workload. Our results indicate that 68.3% of the libraries' bytecode and 20.3% of their total dependencies can be removed through coverage-based debloating.For the first time in the literature on software debloating, we assess the utility of debloated libraries with respect to client applications that reuse them. We select 988 client projects that either have a direct reference to the debloated library in their source code or which test suite covers at least one class of the libraries that we debloat. Our results show that 81.5% of the clients, with at least one test that uses the library, successfully compile and pass their test suite when the original library is replaced by its debloated version. © 2023 Association for Computing Machinery.",bytecode;  code coverage;  program specialization;  Software bloat;  software maintenance,"Soto-Valero, C. and Durieux, T. and Harrand, N. and Baudry, B.",,10.1145/3546948,Knut och Alice Wallenbergs Stiftelse,Sweden,,"Application programs;  Computer software maintenance;  Computer software reusability;  Java programming language;  Open source software, Bytecodes;  Code coverage;  Java byte codes;  Java library;  Novel techniques;  Open-source;  Program specialization;  Security performance;  Software bloat;  State of the art, Libraries",cited By 2,Coverage-Based Debloating for Java Bytecode,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class - such as making it public - can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSerialization;  deserialization;  gadget;  remote code execution RCE;  vulnerabilities,"Sayar, I. and Bartel, A. and Bodden, E. and Le Traon, Y.",,10.1145/3554732,Knut och Alice Wallenbergs Stiftelse,Sweden,,"Codes (symbols);  Java programming language;  Security of data, Additional key word and phrasesserialization;  Code execution;  Deserialization;  Gadget;  Key words;  Remote code;  Remote code execution remote code execution;  Vulnerability, Libraries",cited By 3,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,Kyungpook National University,South Korea,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware testing;  deep neural networks;  generative models;  test coverage,"Dola, S. and Dwyer, M.B. and Soffa, M.L.",,10.1145/3576040,Lockheed Martin,United States,,"Black-box testing;  Cost effectiveness, Additional key word and phrasessoftware testing;  Black box test;  Feature interactions;  Generative model;  Input distributions;  Key words;  Neural-networks;  Test adequacies;  Test inputs;  Test-coverage, Deep neural networks",cited By 2,Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we offer a handful of such types), then (2) lookup those types in our clusters, and then (3) just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering;  empirical analysis;  fairness metrics;  Software fairness;  theoretical analysis,"Majumder, S. and Chakraborty, J. and Bai, G.R. and Stolee, K.T. and Menzies, T.",,10.1145/3585006,Louisiana Academy of Sciences,United States,The work was partially funded by LAS and NSF Grant No. 1908762.,"Learning systems;  Software testing, 'current;  Clusterings;  Empirical analysis;  Fairness metric;  Machine learning software;  Pressung;  Recent researches;  Software fairness;  Testing machine;  Theoretical analyse, Classification (of information)",cited By 1,Fair Enough: Searching for Sufficient Measures of Fairness,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,Meta,United States,"This work was supported, in part, by Science Foundation Ireland grants 20/FFP-P/8818 and 13/RC/2094_P2 and co-funded under the European Regional Development Fund through the Southern & Eastern Regional Operational Programme to Lero - the Science Foundation Ireland Research Centre for Software (www.lero.ie). Stefan Klikovits, Paolo Arcaini, and Fuyuki Ishikawa are supported by the ERATO HASUO Metamathematics for Systems Design Project JPMJER1603 JPMJER1603; Funding Reference number 10.13039/501100009024 Japan Society for the Promotion of Science (JSPS) 20K23334. Stefan Klikovits is supported by Japan Society for the Promotion of Science (JSPS) 20K23334 Grant-in-Aid for Research Activity Start-up No 20K23334 20K23334. Paolo Arcaini and Fuyuki Ishikawa are also supported by Engineerable AI Techniques for Practical Applications of High-Quality Machine Learning-based Systems Project (Grant Number JPMJMI20B8), JST-Mirai.","Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation;  data structure analysis;  value-flow analysis,"Wang, C. and Wang, W. and Yao, P. and Shi, Q. and Zhou, J. and Xiao, X. and Zhang, C.",,10.1145/3565800,Microsoft,United States,,"Data structures;  Flow graphs;  Graphic methods;  Value engineering, Additional key word and phrasesabstract interpretation;  Data structure analyse;  False positive;  Flow-graphs;  Key words;  Memory layout;  Structure analysis;  Ubiquitous data;  Value flow;  Value flow analysis, Containers",cited By 0,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages' builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider's version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breaking changes;  change impact;  dependency management;  npm;  Semantic Version,"Venturini, D. and Cogo, F.R. and Polato, I. and Gerosa, M.A. and Wiese, I.S.",,10.1145/3576037,"Ministerio da Ciencia, Tecnologia e Inovacao",Brazil,,"Recovery, Breaking change;  Breakings;  Bug fixes;  Change impacts;  Complex software systems;  Dependency management;  Down-stream;  Empirical studies;  Npm;  Semantic version, Semantics",cited By 0,I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We devised the L+1-layer divide & conquer approach to leads-to model checking (L+1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L+1-DCA2L2MC. In a temporal logic called UNITY, designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L+1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L+1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLeads-to properties;  master-worker model;  Maude;  parallel model checking;  state space explosion,"Do, C.M. and Phyo, Y. and Riesco, A. and Ogata, K.",,10.1145/3604610,"Ministerio de Ciencia, Innovacion y Universidades",Spain,,"Optimization, Additional key word and phraseslead-to property;  Key words;  Master/worker models;  Maude;  Models checking;  Optimization techniques;  Parallel model checking;  Performance;  Property;  State-space explosion, Model checking",cited By 1,Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes. © 2023 Association for Computing Machinery.",industry;  machine learning;  Regression testing;  taxonomy;  test case prioritisation,"Ramírez, A. and Feldt, R. and Romero, J.R.",,10.1145/3511805,"Ministerio de Ciencia, Innovacion y Universidades",Spain,"This work was supported by the Spanish Ministry of Science and Innovation (project PID2020-115832GB-I00), the University of Cordoba (postdoctoral grant Plan propio 2019 - mod. 2.4 and Plan propio 2020 - mod. 3.1), the Andalusian Regional Government (DOC_00944), and FEDER funds","Cost benefit analysis;  Software testing;  Transmission control protocol, Adverse effect;  Classifieds;  Industry needs;  Machine-learning;  Re-runs;  Regression testing;  Software company;  Systems under tests;  Test case;  Test case prioritization, Machine learning",cited By 2,"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers' profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCollaborative software engineering;  model-driven engineering;  search-based software engineering,"Pérez, F. and Lapeña, R. and Marcén, A. and Cetina, C.",,10.1145/3561384,Ministerio de Economia y Competitividad,Spain,"This work was supported in part by the Ministry of Economy and Competitiveness (MINECO) through the Spanish National R+D+i Plan and ERDF funds under the Project VARIATIVA under grant no. PID2021-128695OB-I00, and in part by the Gobierno de Aragón (Spain) (Research Group S05_20D).","Computer software maintenance, Additional key word and phrasescollaborative software engineering;  Bug localizations;  Feature location;  Key words;  Link recoveries;  Maintenance tasks;  Model-driven Engineering;  Search-based;  Search-based software engineering;  Traceability links, Engineers",cited By 0,How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units. © 2023 Copyright held by the owner/author(s).",architectural patterns;  decomposition process;  Microservices;  performance analysis;  scalability assessment,"Camilli, M. and Colarusso, C. and Russo, B. and Zimeo, E.",,10.1145/3583563,"Ministero dell'struzione, dell'Universita e della Ricerca",Italy,"This work has been partly supported by the SARDECH project funded by the Free University of Bozen, Italy, and by the SISMA (Solutions for Engineering Microservices Architectures) project funded by the Italian Ministry of Research, PRIN no. 201752ENYB_002.","Benchmarking;  Decision making;  Digital storage;  Iterative methods;  Modular construction;  Open source software;  Software architecture, Architectural pattern;  Architectural style;  Decisions makings;  Decomposition process;  Microservice;  Multilevels;  Performance and scalabilities;  Performances analysis;  Scalability assessment;  Software architects, Scalability",cited By 1,Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We introduce a novel approach to source code representation to be used in combination with neural networks. Such a representation is designed to permit the production of a continuous vector for each code statement. In particular, we present how the representation is produced in the case of Java source code. We test our representation for three tasks: code summarization, statement separation, and code search. We compare with the state-of-the-art non-autoregressive and end-to-end models for these tasks. We conclude that all tasks benefit from the proposed representation to boost their performance in terms of F1-score, accuracy, and mean reciprocal rank, respectively. Moreover, we show how models trained on code summarization and models trained on statement separation can be combined to address methods with tangled responsibilities, meaning that these models can be used to detect code misconduct. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Big code;  intent identification;  learning representations;  method name suggestion,"Bertolotti, F. and Cazzola, W.",,10.1145/3514232,"Ministero dell'struzione, dell'Universita e della Ricerca",Italy,This work was partially funded by the MUR projects DSURF (PRIN 2015B8TRFM) and T-LADIES (PRIN 2020TL3X8X),Big code;  Code comprehension;  Code search;  Intent identification;  Java source codes;  Learning representation;  Method name suggestion;  Neural-networks;  Source code representations;  State of the art,cited By 1,Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call ""subtle"") group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness;  fairness improvement;  fairness testing;  machine learning,"Zhang, M. and Sun, J. and Wang, J. and Sun, B.",,10.1145/3591869,Ministry of Education - Singapore,Singapore,"This research is supported by the Ministry of Education, Singapore under its Academic Research Fund Tier 3 (Award ID: MOET32020-0004). This research is supported by the Key R&D Program of Zhejiang (2022C01018) and the NSFC Program (62102359).","Learning systems, Condition;  Fairness;  Fairness improvement;  Fairness testing;  Input space;  Interpretable rules;  Machine learning applications;  Machine-learning;  Neural-networks;  Sensitive attribute, Machine learning",cited By 2,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck, an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that CompCheck successfully revealed incompatibility issues on 76 call sites, 72.7% and 94.9% more than two existing techniques, confirming CompCheck's applicability and effectiveness. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compatibility;  Software upgrade;  test generation,"Zhu, C. and Zhang, M. and Wu, X. and Xu, X. and Li, Y.",,10.1145/3582569,Ministry of Education - Singapore,Singapore,"This research was supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (MOE2019-T2-1-040).","Open source software;  Software testing, Client specific;  Compatibility;  Offline;  Open source projects;  Software upgrades;  Software-systems;  Test generations, Knowledge based systems",cited By 1,Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks. © 2023 Association for Computing Machinery.",Automatic program repair;  deep learning,"Sohn, J. and Kang, S. and Yoo, S.",,10.1145/3563210,"Ministry of Science, ICT and Future Planning",South Korea,,"Evolutionary algorithms;  Optimization;  Repair, Automatic program repair;  Automatic programs;  Data collection;  Deep learning;  Differential Evolution;  Input-output;  Misbehaviour;  Neural weights;  Repair techniques;  Search-based, Deep neural networks",cited By 5,Arachne: Search-Based Repair of Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,"Ministry of Science, ICT and Future Planning",South Korea,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning systems;  Test adequacy,"Kim, J. and Feldt, R. and Yoo, S.",,10.1145/3546947,"Ministry of Science, ICT and Future Planning",South Korea,"Jinhan Kim and Shin Yoo have been supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-2018R1A5A1059921), as well as the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2018-0-00769, Neuromorphic Computing Software Platform for Artificial Intelligence Systems). Robert Feldt has been supported by the Swedish Scientific Council (No. 2015-04913, ‘Basing Software Testing on Information Theory’ and No. 2020-05272, ‘Automated boundary testing for QUality of AI/ML modelS’).","Autonomous vehicles;  Large dataset;  Learning systems;  Medical imaging;  Safety engineering;  Software testing;  Statistical tests, Autonomous driving;  Coverage metrics;  Deep learning system;  Safety-critical domain;  Software testings;  System testing;  Test adequacies;  Test adequacy criteria;  Training data;  Training dataset, Deep neural networks",cited By 0,Evaluating Surprise Adequacy for Deep Learning System Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how ""human aspects""- such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on - might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners' key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals' performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research. © 2023 Association for Computing Machinery.",Human aspects;  requirements engineering;  software engineering,"Hidellaarachchi, D. and Grundy, J. and Hoda, R. and Mueller, I.",,10.1145/3546943,Monash University,Australia,This work is supported by Monash Faculty of IT PhD scholarships. Grundy is supported by ARC Laureate Fellowship FL190100035 and this work is also partially supported by ARC Discovery Project DP200100020.,"Domain Knowledge;  Motivation;  Software engineering, Collaboration capabilities;  Communication and collaborations;  Communication capabilities;  Communication skills;  Domain knowledge;  Human aspects;  Knowledge communication;  Requirement engineering;  Software practitioners;  Working styles, Requirements engineering",cited By 5,The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners' Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIdentifier renaming;  code refactoring;  mining code repository;  source code analysis,"Zhang, J. and Luo, J. and Liang, J. and Gong, L. and Huang, Z.",,10.1145/3603109,Nanjing University,China,,"Android (operating system);  Java programming language;  Semantics, Additional key word and phrasesidentifier renaming;  Change history;  Code conventions;  Code re-factoring;  Key words;  Mining code repository;  Mining codes;  Source code analysis;  Source codes;  State-of-the-art approach, Forecasting",cited By 0,An Accurate Identifier Renaming Prediction and Suggestion Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware engineering for artificial intelligence;  model contract;  model reuse;  pre-trained models,"Gong, L. and Zhang, J. and Wei, M. and Zhang, H. and Huang, Z.",,10.1145/3569934,Nanjing University,China,,"Artificial intelligence;  Codes (symbols);  Contracts;  Knowledge management;  Learning systems, Additional key word and phrasessoftware engineering for artificial intelligence;  Condition;  Exploratory studies;  Key words;  Model contract;  Model repositories;  Model reuse;  Pre-trained model;  Pre/post conditions;  Reuse, Computer software reusability",cited By 0,What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBinarized neural networks;  binary decision diagrams;  formal verification;  interpretability;  robustness,"Zhang, Y. and Zhao, Z. and Chen, G. and Song, F. and Chen, T.",,10.1145/3563212,Nanjing University,China,,"Binary decision diagrams;  Boolean functions;  Encoding (symbols);  Formal verification;  Knowledge representation;  Safety engineering;  Signal encoding, Additional key word and phrasesbinarized neural network;  Encodings;  Interpretability;  Key words;  Network-based;  Neural-networks;  Programming paradigms;  Property;  Quantitative verification;  Robustness, Deep learning",cited By 2,Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIdentifier renaming;  code refactoring;  mining code repository;  source code analysis,"Zhang, J. and Luo, J. and Liang, J. and Gong, L. and Huang, Z.",,10.1145/3603109,Nanjing University of Aeronautics and Astronautics,China,This work was supported in part by the National Natural Science Foundation of China under grant 62272225 and the Fund of Prospective Layout of Scientific Research for NUAA (Nanjing University of Aeronautics and Astronautics).,"Android (operating system);  Java programming language;  Semantics, Additional key word and phrasesidentifier renaming;  Change history;  Code conventions;  Code re-factoring;  Key words;  Mining code repository;  Mining codes;  Source code analysis;  Source codes;  State-of-the-art approach, Forecasting",cited By 0,An Accurate Identifier Renaming Prediction and Suggestion Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware engineering for artificial intelligence;  model contract;  model reuse;  pre-trained models,"Gong, L. and Zhang, J. and Wei, M. and Zhang, H. and Huang, Z.",,10.1145/3569934,Nanjing University of Aeronautics and Astronautics,China,"This work was supported by the National Natural Science Foundation of China under grant 62202223; the Natural Science Foundation of Jiangsu Province, China, under grant BK20220881; the Foundation of the Key National Laboratory of New Technology in Computer Software (Nanjing University) under grant KFKT2022B20; and the Foundation of the Key Laboratory of Safety-Critical Software (Nanjing University of Aeronautics and Astronautics).","Artificial intelligence;  Codes (symbols);  Contracts;  Knowledge management;  Learning systems, Additional key word and phrasessoftware engineering for artificial intelligence;  Condition;  Exploratory studies;  Key words;  Model contract;  Model repositories;  Model reuse;  Pre-trained model;  Pre/post conditions;  Reuse, Computer software reusability",cited By 0,What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  mining software repositories;  software evolution;  software testing,"Sun, W. and Yan, M. and Liu, Z. and Xia, X. and Lei, Y. and Lo, D.",,10.1145/3607183,National Key Research and Development Program of China,China,,"Codes (symbols);  Computer software maintenance;  Corrective maintenance;  Just in time production, Additional key word and phrasesempirical software engineering;  Co-evolution;  Key words;  Mining software;  Mining software repository;  Production test;  Software Evolution;  Software repositories;  Software testings;  Test code, Software testing",cited By 0,Revisiting the Identification of the Co-evolution of Production and Test Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,National Key Research and Development Program of China,China,,"Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch;  certified accuracy;  deep learning;  neural networks;  robustness certification;  vision transformer,"Huang, Y. and Ma, L. and Li, Y.",,10.1145/3591870,National Key Research and Development Program of China,China,,"Accident prevention;  Convolutional neural networks;  Deep neural networks;  Electric transformer testing;  Learning systems, Adversarial patch;  Certified accuracy;  Classical neural networks;  Deep learning;  Exhaustive testing;  Neural-networks;  Performance;  Record-breaking performance;  Robustness certification;  Vision transformer, Cameras",cited By 2,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"As an essential component responsible for communication, network services are security critical, thus, it is vital to find their vulnerabilities. Fuzzing is currently one of the most popular software vulnerability discovery techniques, widely adopted due to its high efficiency and low false positives. However, existing coverage-guided fuzzers mainly aim at stateless local applications, leaving stateful network services underexplored. Recently, some fuzzers targeting network services have been proposed but have certain limitations, for example, insufficient or inaccurate state representation and low testing efficiency.In this article, we propose a new fuzzing solution NSFuzz for stateful network services. We studied typical implementations of network service programs to determine how they represent states and interact with clients. Accordingly, we propose (1) a program variable-based state representation scheme and (2) an efficient interaction synchronization mechanism to improve fuzzing efficiency. We implemented a prototype of NSFuzz, which uses static analysis and annotation application programming interfaces (APIs) to identify synchronization points and state variables within the services. It then achieves fast I/O synchronization and accurate service state tracing to carry out efficient state-aware fuzzing via lightweight compile-time instrumentation. The evaluation results show that compared with other network service fuzzers, including AFLnet and StateAFL, our solution NSFuzz could infer a more accurate state model during fuzzing and improve fuzzing throughput by up to 200×. In addition, NSFuzz could improve code coverage by up to 25% and trigger more crashes in less time. We also performed a fuzzing campaign to find new bugs in the latest version of the target services; 8 zero-day vulnerabilities have been found by NSFuzz. © 2023 Copyright held by the owner/author(s).",fuzzing;  Network service;  vulnerability discovery,"Qin, S. and Hu, F. and Ma, Z. and Zhao, B. and Yin, T. and Zhang, C.",,10.1145/3580598,National Key Research and Development Program of China,China,,"Application programming interfaces (API);  Static analysis;  Synchronization;  Zero-day attack, Communications networks;  False positive;  Fuzzing;  High-low;  Higher efficiency;  Networks services;  Security-critical;  Software vulnerabilities;  State representation;  Vulnerability discovery, Efficiency",cited By 1,NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing - that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality - that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  fairness;  robustness;  testing,"Chen, J. and Wang, J. and Ma, X. and Sun, Y. and Sun, J. and Zhang, P. and Cheng, P.",,10.1145/3582573,National Key Research and Development Program of China,China,,"Deep learning;  Image enhancement;  Quality assurance;  Well testing, Deep learning;  Engineering techniques;  Fairness;  Guided search;  Model properties;  Modeling quality;  Property;  Robustness;  Test case;  Testing framework, Learning systems",cited By 1,QuoTe: Quality-oriented Testing for Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system-decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain;  Ethereum;  malware detection;  Ponzi schemes,"Zheng, Z. and Chen, W. and Zhong, Z. and Chen, Z. and Lu, Y.",,10.1145/3571847,National Key Research and Development Program of China,China,,"Application programs;  Ethereum;  Learning systems;  Losses;  Malware;  Semantics;  Smart contract, Block-chain;  Detection approach;  Financial loss;  Irreversible loss;  Large amounts;  Malware detection;  Ponzi scheme;  Software-systems;  Static features;  Transaction records, Blockchain",cited By 5,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start;  optional function elimination;  performance optimization;  Serverless computing,"Liu, X. and Wen, J. and Chen, Z. and Li, D. and Chen, J. and Liu, Y. and Wang, H. and Jin, X.",,10.1145/3585007,National Key Research and Development Program of China,China,,"Loading, Application level;  Code-loading;  Cold-start;  General applications;  Latency optimizations;  Optional function elimination;  Performance optimizations;  Serverless computing;  Services applications;  Total response, High level languages",cited By 8,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer sequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET covers 3.4% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning testing;  library testing;  model diversity;  model generation,"Li, M. and Cao, J. and Tian, Y. and Li, T.O. and Wen, M. and Cheung, S.-C.",,10.1145/3583566,National Key Research and Development Program of China,China,,"Deep learning;  Learning systems;  Libraries;  Parameter estimation, API calls;  Deep learning testing;  Input parameter;  Layer parameters;  Layer sequence;  Learning models;  Library testing;  Model diversity;  Model generation;  Test effectiveness, Quality assurance",cited By 2,COMET: Coverage-guided Model Generation For Deep Learning Library Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bugginess;  Code line;  defect prediction;  quality assurance;  static analysis tool,"Guo, Z. and Liu, S. and Liu, X. and Lai, W. and Ma, M. and Zhang, X. and Ni, C. and Yang, Y. and Li, Y. and Chen, L. and Zhou, G. and Zhou, Y.",,10.1145/3582572,National Key Research and Development Program of China,China,,"Codes (symbols);  Computer software selection and evaluation;  Defects;  Forecasting;  Natural language processing systems;  Open source software;  Quality assurance;  Quality control, Analysis tools;  Bugginess;  Code line;  Defect prediction;  Identification approach;  Identification tools;  Performance;  Simple++;  State of the art;  Static analyse tool, Static analysis",cited By 0,"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,National Key Research and Development Program of China,China,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches that provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). In this paper, we propose a novel learnable deep Graph for Code Search (called deGraphCS) to transfer source code into variable-based flow graphs based on an intermediate representation technique, which can model code semantics more precisely than directly processing the code as text or using the syntax tree representation. Furthermore, we propose a graph optimization mechanism to refine the code representation and apply an improved gated graph neural network to model variable-based flow graphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale dataset from GitHub containing 41,152 code snippets written in the C language and reproduce several typical deep code search methods for comparison. The experimental results show that deGraphCS can achieve state-of-the-art performance and accurately retrieve code snippets satisfying the needs of the users. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code search;  deep learning;  graph neural networks;  Intermediate representation,"Zeng, C. and Yu, Y. and Li, S. and Xia, X. and Wang, Z. and Geng, M. and Bai, L. and Dong, W. and Liao, X.",,10.1145/3546066,National Key Research and Development Program of China,China,,"Deep neural networks;  Flow graphs;  Graphic methods;  Large dataset;  Modeling languages;  Natural language processing systems;  Semantics;  Trees (mathematics), Code representation;  Code search;  Deep learning;  Embeddings;  Flow-graphs;  Graph neural networks;  Intermediate representations;  Learning-based approach;  Natural languages;  Neural code, Graph neural networks",cited By 2,deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs) - comment changes committed independently of code changes - which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice - especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio - proportion of ICCs among all comment changes - is ∼15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools. © 2023 Association for Computing Machinery.",Code comments;  coding guidelines;  software documentation;  software evolution,"Wang, C. and He, H. and Pal, U. and Marinov, D. and Zhou, M.",,10.1145/3546949,National Key Research and Development Program of China,China,,"Codes (symbols);  Computer software maintenance;  Java programming language;  Open source software;  Software design, 'current;  Change ratio;  Code comment;  Coding guideline;  High quality source;  Low qualities;  Software development and maintenances;  Software documentation;  Software Evolution;  Source code comments, Taxonomies",cited By 0,Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector, an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research. © 2023 Association for Computing Machinery.",incompatibility detection;  license;  Open source software,"Xu, S. and Gao, Y. and Fan, L. and Liu, Z. and Liu, Y. and Ji, H.",,10.1145/3518994,National Key Research and Development Program of China,China,,"Computer software reusability;  Context free grammars;  Open systems, Condition;  Effective tool;  Financial risks;  Incompatibility detection;  Legal risks;  License;  Open-source softwares;  Reuse;  Software license;  Third parties, Open source software",cited By 4,LiDetector: License Incompatibility Detection for Open Source Software,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners' request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners' detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26%, 37.14%, 59.21%, 68.54% more pages, construct 12.95×, 1.13×, 15.03×, 52.66× more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them. © 2023 Association for Computing Machinery.",attack intent;  scanner;  synchronization;  Web security,"Yin, Z. and Xu, Y. and Ma, F. and Gao, H. and Qiao, L. and Jiang, Y.",,10.1145/3517036,National Key Research and Development Program of China,China,,"Benchmarking;  Program debugging;  Well testing, Attack intent;  Attack vector;  Performance;  Purification mechanisms;  Scanner;  Target application;  Vulnerability detection;  WEB application;  Web applications;  WEB security, Synchronization",cited By 0,Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum;  bookkeeping recognition;  smart contract;  token,"He, Z. and Song, S. and Bai, Y. and Luo, X. and Chen, T. and Zhang, W. and He, P. and Li, H. and Lin, X. and Zhang, X.",,10.1145/3560263,National Key Research and Development Program of China,China,,"Blockchain, Additional key word and phrasesethereum;  Behavior detection;  Block-chain;  Bookkeeping recognition;  Bytecodes;  Key words;  Malicious behavior;  Optimisations;  Token;  Type information, Smart contract",cited By 3,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  mining software repositories;  software evolution;  software testing,"Sun, W. and Yan, M. and Liu, Z. and Xia, X. and Lei, Y. and Lo, D.",,10.1145/3607183,National Natural Science Foundation of China,China,,"Codes (symbols);  Computer software maintenance;  Corrective maintenance;  Just in time production, Additional key word and phrasesempirical software engineering;  Co-evolution;  Key words;  Mining software;  Mining software repository;  Production test;  Software Evolution;  Software repositories;  Software testings;  Test code, Software testing",cited By 0,Revisiting the Identification of the Co-evolution of Production and Test Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,National Natural Science Foundation of China,China,,"Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code knowledge graph;  code recommendation;  crowd-scale coding practice checking;  knowledge deduction;  Smart contract,"Huang, Q. and Liao, D. and Xing, Z. and Zuo, Z. and Wang, C. and Xia, X.",,10.1145/3597206,National Natural Science Foundation of China,China,,"Cloning;  Codes (symbols);  Flow graphs;  Knowledge graph;  Semantics;  Software engineering, Code knowledge graph;  Code recommendation;  Code reuse;  Crowd-scale coding practice checking;  Current codes;  Functional failure;  Knowledge deduction;  Knowledge graphs;  Mining methods;  Non-functional, Smart contract",cited By 0,Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSource code editing;  deep learning;  Pre-training,"Li, J. and Li, G. and Li, Z. and Jin, Z. and Hu, X. and Zhang, K. and Fu, Z.",,10.1145/3597207,National Natural Science Foundation of China,China,"This research is supported by the National Natural Science Foundation of China under Grants No. 62192733, No. 62192731, No. 61751210, No. 62072007, No. 61832009, and No. 62192730.","Codes (symbols);  Learning systems;  Modeling languages;  Natural language processing systems;  Software design;  Zero-shot learning, Additional key word and phrasessource code editing;  Automatic codes;  Deep learning;  Fine tuning;  Ground truth;  Infilling;  Key words;  Pre-training;  State of the art;  Test data, Deep learning",cited By 2,CodeEditor: Learning to Edit Source Code with Pre-trained Models,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch;  certified accuracy;  deep learning;  neural networks;  robustness certification;  vision transformer,"Huang, Y. and Ma, L. and Li, Y.",,10.1145/3591870,National Natural Science Foundation of China,China,,"Accident prevention;  Convolutional neural networks;  Deep neural networks;  Electric transformer testing;  Learning systems, Adversarial patch;  Certified accuracy;  Classical neural networks;  Deep learning;  Exhaustive testing;  Neural-networks;  Performance;  Record-breaking performance;  Robustness certification;  Vision transformer, Cameras",cited By 2,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call ""subtle"") group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness;  fairness improvement;  fairness testing;  machine learning,"Zhang, M. and Sun, J. and Wang, J. and Sun, B.",,10.1145/3591869,National Natural Science Foundation of China,China,,"Learning systems, Condition;  Fairness;  Fairness improvement;  Fairness testing;  Input space;  Interpretable rules;  Machine learning applications;  Machine-learning;  Neural-networks;  Sensitive attribute, Machine learning",cited By 2,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training;  Empirical study;  software engineering,"Liu, X. and Gu, D. and Chen, Z. and Wen, J. and Zhang, Z. and Ma, Y. and Wang, H. and Jin, X.",,10.1145/3597204,National Natural Science Foundation of China,China,,"Cell proliferation;  Deep learning;  Large dataset;  Learning systems;  Life cycle;  Program processors;  Software testing, Distributed training;  Empirical studies;  Engineering perspective;  Fault symptoms;  Large datasets;  Learning models;  Learning software;  Rich features;  Software life cycles;  Training process, Program debugging",cited By 0,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIdentifier renaming;  code refactoring;  mining code repository;  source code analysis,"Zhang, J. and Luo, J. and Liang, J. and Gong, L. and Huang, Z.",,10.1145/3603109,National Natural Science Foundation of China,China,,"Android (operating system);  Java programming language;  Semantics, Additional key word and phrasesidentifier renaming;  Change history;  Code conventions;  Code re-factoring;  Key words;  Mining code repository;  Mining codes;  Source code analysis;  Source codes;  State-of-the-art approach, Forecasting",cited By 0,An Accurate Identifier Renaming Prediction and Suggestion Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such a wide effect demonstrates the necessity and importance of guaranteeing DL frameworks' quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating designing effective bug detection and debugging approaches. Hence, in this work, we conduct the most large-scale study on 1,000 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with five components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques, we obtain 12 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging. Finally, based on the guidelines, we design and implement a prototype DL-framework testing tool, called TenFuzz, which is evaluated to be effective and finds three unknown bugs on the latest TensorFlow framework in a preliminary study, indicating the significance of our guidelines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bug analysis;  Deep learning frameworks;  deep learning testing;  empirical study,"Chen, J. and Liang, Y. and Shen, Q. and Jiang, J. and Li, S.",,10.1145/3587155,National Natural Science Foundation of China,China,"This work was supported by the National Natural Science Foundation of China under Grant Nos. 62002256, 62232001, and 62202324.","Deep learning;  Program debugging;  Well testing, Bug analyse;  Bug detection;  Deep learning framework;  Deep learning testing;  Empirical studies;  Large-scale studies;  Learning frameworks;  Root cause;  State of the art;  Test-coverage, Quality assurance",cited By 1,Toward Understanding Deep Learning Framework Bugs,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Method name prediction;  naming convention,"Wang, S. and Wen, M. and Lin, B. and Liu, Y. and Bissyandé, T.F. and Mao, X.",,10.1145/3597203,National Natural Science Foundation of China,China,,"Codes (symbols);  Deep learning;  Heuristic methods;  Object oriented programming, Automated tool support;  Development tasks;  Empirical analysis;  Large-scales;  Method implementations;  Method name prediction;  Naming convention;  Objectoriented programming (OOP);  Real world projects;  Research efforts, Forecasting",cited By 0,Pre-implementation Method Name Prediction for Object-oriented Programming,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"When searching code, developers may express additional constraints (e.g., functional constraints and nonfunctional constraints) on the implementations of desired functionalities in the queries. Existing code search tools treat the queries as a whole and ignore the different implications of different parts of the queries. Moreover, these tools usually return a ranked list of candidate code snippets without any explanations. Therefore, the developers often find it hard to choose the desired results and build confidence on them. In this article, we conduct a developer survey to better understand and address these issues and induct some insights from the survey results. Based on the insights, we propose XCoS, an explainable code search approach based on query scoping and knowledge graph. XCoS extracts a background knowledge graph from general knowledge bases like Wikidata and Wikipedia. Given a code search query, XCoS identifies different parts (i.e., functionalities, functional constraints, nonfunctional constraints) from it and use the expressions of functionalities and functional constraints to search the codebase. It then links both the query and the candidate code snippets to the concepts in the background knowledge graph and generates explanations based on the association paths between these two parts of concepts together with relevant descriptions. XCoS uses an interactive user interface that allows the user to better understand the associations between candidate code snippets and the query from different aspects and choose the desired results. Our evaluation shows that the quality of the extracted background knowledge and the concept linkings in codebase is generally high. Furthermore, the generated explanations are considered complete, concise, and readable, and the approach can help developers find the desired code snippets more accurately and confidently. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code search;  concept;  explainability;  knowledge,"Wang, C. and Peng, X. and Xing, Z. and Zhang, Y. and Liu, M. and Luo, R. and Meng, X.",,10.1145/3593800,National Natural Science Foundation of China,China,This work was supported by the National Natural Science Foundation of China under grant 61972098.,"Codes (symbols);  Data mining;  User interfaces, Background knowledge;  Code developers;  Code search;  Concept;  Explainability;  Functional constraints;  Knowledge;  Knowledge graphs;  Query scoping;  Search-based, Knowledge graph",cited By 6,XCoS: Explainable Code Search Based on Query Scoping and Knowledge Graph,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"As an essential component responsible for communication, network services are security critical, thus, it is vital to find their vulnerabilities. Fuzzing is currently one of the most popular software vulnerability discovery techniques, widely adopted due to its high efficiency and low false positives. However, existing coverage-guided fuzzers mainly aim at stateless local applications, leaving stateful network services underexplored. Recently, some fuzzers targeting network services have been proposed but have certain limitations, for example, insufficient or inaccurate state representation and low testing efficiency.In this article, we propose a new fuzzing solution NSFuzz for stateful network services. We studied typical implementations of network service programs to determine how they represent states and interact with clients. Accordingly, we propose (1) a program variable-based state representation scheme and (2) an efficient interaction synchronization mechanism to improve fuzzing efficiency. We implemented a prototype of NSFuzz, which uses static analysis and annotation application programming interfaces (APIs) to identify synchronization points and state variables within the services. It then achieves fast I/O synchronization and accurate service state tracing to carry out efficient state-aware fuzzing via lightweight compile-time instrumentation. The evaluation results show that compared with other network service fuzzers, including AFLnet and StateAFL, our solution NSFuzz could infer a more accurate state model during fuzzing and improve fuzzing throughput by up to 200×. In addition, NSFuzz could improve code coverage by up to 25% and trigger more crashes in less time. We also performed a fuzzing campaign to find new bugs in the latest version of the target services; 8 zero-day vulnerabilities have been found by NSFuzz. © 2023 Copyright held by the owner/author(s).",fuzzing;  Network service;  vulnerability discovery,"Qin, S. and Hu, F. and Ma, Z. and Zhao, B. and Yin, T. and Zhang, C.",,10.1145/3580598,National Natural Science Foundation of China,China,This project has received funding in part from the National Key Research and Development Program of China (grant no. 2021YFB2701000) and the National Natural Science Foundation of China (grant no. 61972224).,"Application programming interfaces (API);  Static analysis;  Synchronization;  Zero-day attack, Communications networks;  False positive;  Fuzzing;  High-low;  Higher efficiency;  Networks services;  Security-critical;  Software vulnerabilities;  State representation;  Vulnerability discovery, Efficiency",cited By 1,NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones' impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3) Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning software;  co-changed clone;  code clone,"Mo, R. and Zhang, Y. and Wang, Y. and Zhang, S. and Xiong, P. and Li, Z. and Zhao, Y.",,10.1145/3607181,National Natural Science Foundation of China,China,,"Codes (symbols);  Computer software maintenance;  Deep learning, Additional key word and phrasesdeep learning software;  Co-changed clone;  Code clone;  Code cloning;  Key words;  Learning models;  Learning projects;  Learning software;  Reuse;  Software developer, Cloning",cited By 0,Exploring the Impact of Code Clones on Deep Learning Software,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×-446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186-1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification models;  model compression;  Model dissemination;  neural networks,"Tian, Y. and Zhang, W. and Wen, M. and Cheung, S.-C. and Sun, C. and Ma, S. and Jiang, Y.",,10.1145/3583564,National Natural Science Foundation of China,China,,"Black-box testing;  Classification (of information);  Deep neural networks;  Efficiency;  Iterative methods;  Markov processes;  Neural network models, Classification models;  Classification tasks;  Embedded device;  Image classification model;  Images classification;  Model compression;  Model dissemination;  Neural network model;  Neural-networks;  Original model, Image classification",cited By 1,Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,National Natural Science Foundation of China,China,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with the same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future. © 2023 Copyright held by the owner/author(s).",Code comment;  comment generation;  comparative study;  inline comment;  method comment,"Huang, Y. and Guo, H. and Ding, X. and Shu, J. and Chen, X. and Luo, X. and Zheng, Z. and Zhou, X.",,10.1145/3582570,National Natural Science Foundation of China,China,,"Codes (symbols), Automatic Generation;  Code comment;  Comment generation;  Comparatives studies;  Compare and analyze;  Inline comment;  Learning models;  Method comment;  Program comprehension;  Source codes, Deep learning",cited By 1,A Comparative Study on Method Comment and Inline Comment,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software vulnerabilities, once disclosed, can be documented in vulnerability databases, which have great potential to advance vulnerability analysis and security research. People describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerability knowledge embedded in text. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. In this article, we propose unsupervised methods to label and extract important vulnerability concepts in textual vulnerability descriptions (TVDs). We focus on six types of phrase-based vulnerability concepts (vulnerability type, vulnerable component, root cause, attacker type, impact, and attack vector) as they are much more difficult to label and extract than name- or number-based entities (i.e., vendor, product, and version). Our approach is based on a key observation that the same-type of phrases, no matter how they differ in sentence structures and phrase expressions, usually share syntactically similar paths in the sentence parsing trees. Specifically, we present a source-target neural architecture that learns the Part-of-Speech (POS) tagging to identify a token's functional role within TVDs, where the source neural model is trained to capture common features found in the TVD corpus, and the target model is trained to identify linguistically malformed words specific to the security domain. Our evaluation confirms that the proposed tagger outperforms (4.45%-5.98%) the taggers designed on natural language notions and identifies a broad set of TVDs and natural language contents. Then, based on the key observations, we propose two path representations (absolute paths and relative paths) and use an auto-encoder to encode such syntactic similarities. To address the discrete nature of our paths, we enhance the traditional Variational Auto-encoder (VAE) with Gumble-Max trick for categorical data distribution and thus create a Categorical VAE (CaVAE). In the latent space of absolute and relative paths, we further apply unsupervised clustering techniques to generate clusters of the same-type of concepts. Our evaluation confirms the effectiveness of our CaVAE, which achieves a small (85.85) log-likelihood for encoding path representations and the accuracy (83%-89%) of vulnerability concepts in the resulting clusters. The resulting clusters accurately label six types of vulnerability concepts from a TVD corpus in an unsupervised way. Furthermore, these labeled vulnerability concepts can be mapped back to the corresponding phrases in the original TVDs, which produce labels of six types of vulnerability concepts. The resulting labeled TVDs can be used to train concept extraction models for other TVD corpora. In this work, we present two concept extraction methods (concept classification and sequence labeling model) to demonstrate the utility of the unsupervisedly labeled concepts. Our study shows that models trained with our unsupervisedly labeled vulnerability concepts outperform (3.9%-5.14%) those trained with the two manually labeled TVD datasets from previous work due to the consistent boundary and typing by our unsupervised labeling method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering and concept labeling;  phrase-based vulnerability concepts;  supervised concept extraction;  Textual vulnerability descriptions;  unsupervised representation learning,"Yitagesu, S. and Xing, Z. and Zhang, X. and Feng, Z. and Li, X. and Han, L.",,10.1145/3579638,National Natural Science Foundation of China,China,This work is supported by the National Natural Science Foundation of China (NSFC) (61972455).,"Cluster analysis;  Computational linguistics;  Encoding (symbols);  Extraction;  Signal encoding, Clustering and concept labeling;  Clusterings;  Concept extraction;  Labelings;  Natural languages;  Phrase-based vulnerability concept;  Supervised concept extraction;  Textual vulnerability description;  Unsupervised representation learning;  Vulnerability description, Syntactics",cited By 0,Extraction of Phrase-based Concepts in Vulnerability Descriptions through Unsupervised Labeling,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing - that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality - that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  fairness;  robustness;  testing,"Chen, J. and Wang, J. and Ma, X. and Sun, Y. and Sun, J. and Zhang, P. and Cheng, P.",,10.1145/3582573,National Natural Science Foundation of China,China,"This work was supported by the National Key R&D Program of China (grant 2020YFB2010901), the Key R&D Program of Zhejiang (grant 2022C01018), the NSFC Program (grants 62102359, 61833015, 62293511, and U1911401), and the Fundamental Research Funds for Central Universities (Zhejiang University NGICS Platform).","Deep learning;  Image enhancement;  Quality assurance;  Well testing, Deep learning;  Engineering techniques;  Fairness;  Guided search;  Model properties;  Modeling quality;  Property;  Robustness;  Test case;  Testing framework, Learning systems",cited By 1,QuoTe: Quality-oriented Testing for Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system-decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain;  Ethereum;  malware detection;  Ponzi schemes,"Zheng, Z. and Chen, W. and Zhong, Z. and Chen, Z. and Lu, Y.",,10.1145/3571847,National Natural Science Foundation of China,China,,"Application programs;  Ethereum;  Learning systems;  Losses;  Malware;  Semantics;  Smart contract, Block-chain;  Detection approach;  Financial loss;  Irreversible loss;  Large amounts;  Malware detection;  Ponzi scheme;  Software-systems;  Static features;  Transaction records, Blockchain",cited By 5,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start;  optional function elimination;  performance optimization;  Serverless computing,"Liu, X. and Wen, J. and Chen, Z. and Li, D. and Chen, J. and Liu, Y. and Wang, H. and Jin, X.",,10.1145/3585007,National Natural Science Foundation of China,China,,"Loading, Application level;  Code-loading;  Cold-start;  General applications;  Latency optimizations;  Optional function elimination;  Performance optimizations;  Serverless computing;  Services applications;  Total response, High level languages",cited By 8,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer sequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET covers 3.4% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning testing;  library testing;  model diversity;  model generation,"Li, M. and Cao, J. and Tian, Y. and Li, T.O. and Wen, M. and Cheung, S.-C.",,10.1145/3583566,National Natural Science Foundation of China,China,"This work was supported by the National Natural Science Foundation of China (Grant Nos. 61932021, 62002125), the National Key Research and Development Program of China (Grant No. 2019YFE0198100), the Hong Kong RGC/GRF (Grant No. 16205722), the Hong Kong ITF (Grant No. MHP/055/19), and the MSRA Collaborative Research Grant.","Deep learning;  Learning systems;  Libraries;  Parameter estimation, API calls;  Deep learning testing;  Input parameter;  Layer parameters;  Layer sequence;  Learning models;  Library testing;  Model diversity;  Model generation;  Test effectiveness, Quality assurance",cited By 2,COMET: Coverage-guided Model Generation For Deep Learning Library Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Intelligent deep learning-based models have made significant progress for automated source code semantics embedding, and current research works mainly leverage natural language-based methods and graph-based methods. However, natural language-based methods do not capture the rich semantic structural information of source code, and graph-based methods do not utilize rich distant information of source code due to the high cost of message-passing steps.In this article, we propose a novel interpretable model, called graph tensor convolution neural network (GTCN), to generate accurate code embedding, which is capable of comprehensively capturing the distant information of code sequences and rich code semantics structural information. First, we propose to utilize a high-dimensional tensor to integrate various heterogeneous code graphs with node sequence features, such as control flow, data flow. Second, inspired by the current advantages of graph-based deep learning and efficient tensor computations, we propose a novel interpretable graph tensor convolution neural network for learning accurate code semantic embedding from the code graph tensor. Finally, we evaluate three popular applications on the GTCN model: variable misuse detection, source code prediction, and vulnerability detection. Compared with current state-of-the-art methods, our model achieves higher scores with respect to the top-1 accuracy while costing less training time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code embedding;  graph neural network;  Tensor computation,"Yang, J. and Fu, C. and Deng, F. and Wen, M. and Guo, X. and Wan, C.",,10.1145/3582574,National Natural Science Foundation of China,China,"Cai Fu is supported by China NSF (62072200), Jia Yang is supported by China NSF (62202146).","Codes (symbols);  Convolution;  Data flow analysis;  Deep learning;  Flow graphs;  Graph neural networks;  Graph structures;  Graphic methods;  Knowledge graph;  Message passing;  Network coding;  Semantic Web;  Semantics;  Tensors, 'current;  Code embedding;  Code semantics;  Convolution neural network;  Embeddings;  Graph neural networks;  Natural languages;  Semantic embedding;  Source codes;  Tensor computation, Embeddings",cited By 0,Toward Interpretable Graph Tensor Convolution Neural Network for Code Semantics Embedding,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software cross-modal retrieval is a popular yet challenging direction, such as bug localization and code search. Previous studies generally map natural language texts and codes into a homogeneous semantic space for similarity measurement. However, it is not easy to accurately capture their similar semantics in a homogeneous semantic space due to the semantic gap. Therefore, we propose to map the multi-modal data into heterogeneous semantic spaces to capture their unique semantics. Specifically, we propose a novel software cross-modal retrieval framework named Deep Hypothesis Testing (DeepHT). In DeepHT, to capture the unique semantics of the code's control flow structure, all control flow paths (CFPs) in the control flow graph are mapped to a CFP sample set in the sample space. Meanwhile, the text is mapped to a CFP correlation distribution in the distribution space to model its correlation with different CFPs. The matching score is calculated according to how well the sample set obeys the distribution using hypothesis testing. The experimental results on two text-to-code retrieval tasks (i.e., bug localization and code search) and two code-to-text retrieval tasks (i.e., vulnerability knowledge retrieval and historical patch retrieval) show that DeepHT outperforms the baseline methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning;  hypothesis testing;  Software cross-modal retrieval,"Wei, H. and Su, X. and Gao, C. and Zheng, W. and Tao, W.",,10.1145/3591868,National Natural Science Foundation of China,China,This work is supported by the National Natural Science Foundation of China (Grant No. 62272132).,"Codes (symbols);  Data flow analysis;  Deep learning;  Flow graphs;  Information retrieval;  Modal analysis;  Software testing;  Well testing, Bug localizations;  Code search;  Control-flow;  Cross-modal;  Deep learning;  Flow path;  Hypothesis testing;  Sample sets;  Semantic Space;  Software cross-modal retrieval, Semantics",cited By 0,A Hypothesis Testing-based Framework for Software Cross-modal Retrieval in Heterogeneous Semantic Spaces,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",literature view;  Serverless computing,"Wen, J. and Chen, Z. and Jin, X. and Liu, X.",,10.1145/3579643,National Natural Science Foundation of China,China,,"Computation theory;  Program debugging, Application logic;  Cloud-computing;  Computing paradigm;  Error prones;  Infrastructure managements;  Literature view;  Research efforts;  Serverless computing;  Software applications;  Systematic Review, Application programs",cited By 10,Rise of the Planet of Serverless Computing: A Systematic Review,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bugginess;  Code line;  defect prediction;  quality assurance;  static analysis tool,"Guo, Z. and Liu, S. and Liu, X. and Lai, W. and Ma, M. and Zhang, X. and Ni, C. and Yang, Y. and Li, Y. and Chen, L. and Zhou, G. and Zhou, Y.",,10.1145/3582572,National Natural Science Foundation of China,China,"This work is partially supported by the National Natural Science Foundation of China (62172205, 62172202, 62272221, 62072194) and by the National Program on Key Basic Research Project (2020YFA0713600).","Codes (symbols);  Computer software selection and evaluation;  Defects;  Forecasting;  Natural language processing systems;  Open source software;  Quality assurance;  Quality control, Analysis tools;  Bugginess;  Code line;  Defect prediction;  Identification approach;  Identification tools;  Performance;  Simple++;  State of the art;  Static analyse tool, Static analysis",cited By 0,"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction. © 2023 Copyright held by the owner/author(s).",distributed representation learning;  embeddings;  explanation;  features combination;  machine learning;  patch correctness;  Program repair,"Tian, H. and Liu, K. and Li, Y. and Kaboré, A.K. and Koyuncu, A. and Habib, A. and Li, L. and Wen, J. and Klein, J. and Bissyandé, T.F.",,10.1145/3576039,National Natural Science Foundation of China,China,"This work was supported by funding from the European Research Council ( ERC ) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 949014). Kui Liu was also supported by the National Natural Science Foundation of China (Grant No. 62172214), the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20210279), and the Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing (No. 2020A06).","Classification (of information);  Codes (symbols);  Embeddings;  Forecasting;  Software testing, Distributed representation;  Distributed representation learning;  Dynamic information;  Embeddings;  Explanation;  Feature combination;  Machine-learning;  Patch correctness;  Program repair;  State of the art, Machine learning",cited By 3,The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. ""1-to-1""mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of ""1-to-n""(one binary function matches multiple source functions or binary functions) or even ""n-to-n""(multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the ""1-to-1""mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy. © 2023 Association for Computing Machinery.",1-to-1;  1-to-n;  Binary similarity analysis;  function inlining,"Jia, A. and Fan, M. and Jin, W. and Xu, X. and Zhou, Z. and Tang, Q. and Nie, S. and Wu, S. and Liu, T.",,10.1145/3561385,National Natural Science Foundation of China,China,"This work was supported by National Natural Science Foundation of China (61902306, 62002280, 61721002, 61833015), and sponsored by the CCF-Tencent Open Research Fund, the Fundamental Research Funds for the Central Universities (xxj022019001, xzy012020009), China Postdoctoral Science Foundation (2020M683507, 2019TQ0251, 2020M673439), and Youth Talent Support Plan of Xi’an Association for Science and Technology (095920201303).","Computer software reusability;  Open systems;  Software testing;  Supply chains, 1-to-1;  1-to-n;  Binary files;  Binary functions;  Binary similarity analyse;  Function inlining;  Inlining;  Performance;  Similarity analysis;  Simulation strategies, Open source software",cited By 1,1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,National Natural Science Foundation of China,China,"This work was supported by funding from the National Key R&D Program of China (2020YFB1005500), the National Natural Science Foundation of China (Grant No. 62172214), the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20210279), the Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing (No. 2020A06), the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 949014), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2021R1A5A1021944 and 2021R1I1A3048013), Additionally, the research was partially supported by Kyungpook National University Research Fund, 2020.","Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware engineering for artificial intelligence;  model contract;  model reuse;  pre-trained models,"Gong, L. and Zhang, J. and Wei, M. and Zhang, H. and Huang, Z.",,10.1145/3569934,National Natural Science Foundation of China,China,,"Artificial intelligence;  Codes (symbols);  Contracts;  Knowledge management;  Learning systems, Additional key word and phrasessoftware engineering for artificial intelligence;  Condition;  Exploratory studies;  Key words;  Model contract;  Model repositories;  Model reuse;  Pre-trained model;  Pre/post conditions;  Reuse, Computer software reusability",cited By 0,What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBinarized neural networks;  binary decision diagrams;  formal verification;  interpretability;  robustness,"Zhang, Y. and Zhao, Z. and Chen, G. and Song, F. and Chen, T.",,10.1145/3563212,National Natural Science Foundation of China,China,,"Binary decision diagrams;  Boolean functions;  Encoding (symbols);  Formal verification;  Knowledge representation;  Safety engineering;  Signal encoding, Additional key word and phrasesbinarized neural network;  Encodings;  Interpretability;  Key words;  Network-based;  Neural-networks;  Programming paradigms;  Property;  Quantitative verification;  Robustness, Deep learning",cited By 2,Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAutomated program repair;  bias study;  empirical evaluation,"Yang, D. and Lei, Y. and Mao, X. and Qi, Y. and Yi, X.",,10.1145/3561382,National Natural Science Foundation of China,China,This work is supported by the National Natural Science Foundation of China (Nos. 61872445 and 62272072) and the Major Key Project of PCL (No. PCL2021A06).,"Open source software;  Petroleum reservoir evaluation, Additional key word and phrasesautomated program repair;  Bias studies;  Constraint-based;  Empirical evaluations;  Executables;  Key words;  Repair techniques;  Systematic exploration;  Systematic literature review;  Validation strategies, Repair",cited By 2,Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Trace data is critical for fault localization (FL) to analyze suspicious statements potentially responsible for a failure. However, existing trace representation meets its bottleneck mainly in two aspects: (1) the trace information of a statement is restricted to a local context (i.e., a test case) without the consideration of a global context (i.e., all test cases of a test suite); (2) it just uses the goccurrence' for representation without strong FL semantics. Thus, we propose UNITE: an inflUential coNtext-GuIded Trace rEpresentation, representing the trace from both global and local contexts with influential semantics for FL. UNITE embodies and implements two key ideas: (1) UNITE leverages the widely used weighting capability from local and global contexts of information retrieval to reflect how important a statement (a word) is to a test case (a document) in all test cases of a test suite (a collection), where a test case (a document) and all test cases of a test suite (a collection) represent local and global contexts respectively; (2) UNITE further elaborates the trace representation from goccurrence' (weak semantics) to ginfluence' (strong semantics) by combing program dependencies. The large-scale experiments on 12 FL techniques and 20 programs show that UNITE significantly improves FL effectiveness. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFault localization;  program dependence;  statement weighting;  suspiciousness;  trace representation,"Zhang, Z. and Lei, Y. and Su, T. and Yan, M. and Mao, X. and Yu, Y.",,10.1145/3576043,National Natural Science Foundation of China,China,"This work is partially supported by the National Natural Science Foundation of China (No. 62272072), the Fundamental Research Funds for the Central Universities (No. 2022CDJDX-005), the National Defense Basic Scientific Research Project (No. WDZC20205500308), and the Major Key Project of PCL (No. PCL2021A06).","Software testing, Additional key word and phrasesfault localization;  Fault localization;  Global context;  Key words;  Localisation;  Program dependence;  Statement weighting;  Suspiciousness;  Test case;  Trace representations, Semantics",cited By 5,Influential Global and Local Contexts Guided Trace Representation for Fault Localization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Test case prioritization (TCP) has been widely studied in regression testing, which aims to optimize the execution order of test cases so as to detect more faults earlier. TCP has been divided into white-box test case prioritization (WTCP) and black-box test case prioritization (BTCP). WTCP can achieve better prioritization effectiveness by utilizing source code information, but is not applicable in many practical scenarios (where source code is unavailable, e.g., outsourced testing). BTCP has the benefit of not relying on source code information, but tends to be less effective than WTCP. That is, both WTCP and BTCP suffer from limitations in the practical use.To improve the practicability of TCP, we aim to explore better BTCP, significantly bridging the effectiveness gap between BTCP and WTCP. In this work, instead of statically analyzing test cases themselves in existing BTCP techniques, we conduct the first study to explore whether this goal can be achieved via log analysis. Specifically, we propose to mine test logs produced during test execution to more sufficiently reflect test behaviors, and design a new BTCP framework (called LogTCP), including log pre-processing, log representation, and test case prioritization components. Based on the LogTCP framework, we instantiate seven log-based BTCP techniques by combining different log representation strategies with different prioritization strategies.We conduct an empirical study to explore the effectiveness of LogTCP. Based on 10 diverse open-source Java projects from GitHub, we compared LogTCP with three representative BTCP techniques and four representative WTCP techniques. Our results show that all of our LogTCP techniques largely perform better than all the BTCP techniques in average fault detection, to the extent that they become competitive to the WTCP techniques. That demonstrates the great potential of logs in practical TCP. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTest case prioritization;  log analysis;  regression testing,"Chen, Z. and Chen, J. and Wang, W. and Zhou, J. and Wang, M. and Chen, X. and Zhou, S. and Wang, J.",,10.1145/3569932,National Natural Science Foundation of China,China,This work was supported by the National Natural Science Foundation of China (Grant No. 62002256) and Fund projects in the technical field of the foundation strengthening plan (2020-JCJQ-JJ-490).,"Black-box testing;  Codes (symbols);  Open source software;  Transmission control protocol, Additional key word and phrasestest case prioritization;  Black box test;  Key words;  Log analysis;  Prioritization;  Prioritization techniques;  Regression testing;  Test case;  Test case prioritization;  White box, Fault detection",cited By 3,Exploring Better Black-Box Test Case Prioritization via Log Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance. © 2023 Association for Computing Machinery.",deep neural network;  highly configurable systems;  machine learning;  Software performance prediction,"Cheng, J. and Gao, C. and Zheng, Z.",,10.1145/3528100,National Natural Science Foundation of China,China,"This research was supported by the Key-Area Research and Development Program of Guangdong Province (2020B010165003), the National Natural Science Foundation of China (projects 62032025 and 62002084), and Stable support plan for colleges and universities in Shenzhen (project GXWD20201230155427003-20200730101839009).","Complex networks;  Forecasting;  Hierarchical systems, Configurable systems;  Configuration options;  Hierarchical interactions;  Highly configurable system;  Machine-learning;  Neural-networks;  Performance prediction;  Software performance;  Software performance prediction;  Systems performance, Deep neural networks",cited By 0,HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction refinement;  fault localization;  selective instrumentation;  Statistical debugging,"Zuo, Z. and Niu, X. and Zhang, S. and Fang, L. and Khoo, S.C. and Lu, S. and Sun, C. and Xu, G.H.",,10.1145/3544790,National Natural Science Foundation of China,China,"This work was partially supported by the National Natural Science Foundation of China (Nos. 62032010 and 62102176), the Natural Science Foundation of Jiangsu Province (No. BK20191247), the US National Science Foundation under grants CNS-1613023, CNS-1703598, CNS-1763172, CNS-2006437, CNS-2007737, and CNS-2106838, and the US Office of Naval Research under grants N00014-16-1-2913 and N00014-18-1-2037.","Abstracting;  Codes (symbols);  Cost benefit analysis;  Program debugging, Abstraction-refinement;  Analysis costs;  Automated debugging;  Fault localization;  Program code;  Program instrumentations;  Selective instrumentation;  Statistical debugging;  Statistical techniques;  Time-consuming tasks, Statistics",cited By 0,Toward More Efficient Statistical Debugging with Abstraction Refinement,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches that provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). In this paper, we propose a novel learnable deep Graph for Code Search (called deGraphCS) to transfer source code into variable-based flow graphs based on an intermediate representation technique, which can model code semantics more precisely than directly processing the code as text or using the syntax tree representation. Furthermore, we propose a graph optimization mechanism to refine the code representation and apply an improved gated graph neural network to model variable-based flow graphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale dataset from GitHub containing 41,152 code snippets written in the C language and reproduce several typical deep code search methods for comparison. The experimental results show that deGraphCS can achieve state-of-the-art performance and accurately retrieve code snippets satisfying the needs of the users. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code search;  deep learning;  graph neural networks;  Intermediate representation,"Zeng, C. and Yu, Y. and Li, S. and Xia, X. and Wang, Z. and Geng, M. and Bai, L. and Dong, W. and Liao, X.",,10.1145/3546066,National Natural Science Foundation of China,China,"This work is supported by National Key R&D Program of China (2020AAA0103504), National Natural Science Foundation of China (No. 61690203 and No. 61872373), and the Major Key Project of PCL.","Deep neural networks;  Flow graphs;  Graphic methods;  Large dataset;  Modeling languages;  Natural language processing systems;  Semantics;  Trees (mathematics), Code representation;  Code search;  Deep learning;  Embeddings;  Flow-graphs;  Graph neural networks;  Intermediate representations;  Learning-based approach;  Natural languages;  Neural code, Graph neural networks",cited By 2,deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,National Natural Science Foundation of China,China,,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs) - comment changes committed independently of code changes - which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice - especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio - proportion of ICCs among all comment changes - is ∼15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools. © 2023 Association for Computing Machinery.",Code comments;  coding guidelines;  software documentation;  software evolution,"Wang, C. and He, H. and Pal, U. and Marinov, D. and Zhou, M.",,10.1145/3546949,National Natural Science Foundation of China,China,"We acknowledge the support of the National Key R&D Program of China Grant 2018YFB1004201, the National Natural Science Foundation of China Grants 61825201, and the US National Science Foundation grant IIS-2016908.","Codes (symbols);  Computer software maintenance;  Java programming language;  Open source software;  Software design, 'current;  Change ratio;  Code comment;  Coding guideline;  High quality source;  Low qualities;  Software development and maintenances;  Software documentation;  Software Evolution;  Source code comments, Taxonomies",cited By 0,Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert: During 10,000 iterations, transcert reveals 71 unique validation differences, 12×, 1.4×, and 7× as many as those revealed by frankencert, NEZHA, and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280. © 2023 Association for Computing Machinery.",certificate validation;  certification mutation;  Coverage transfer graph;  differential testing,"Nie, P. and Wan, C. and Zhu, J. and Lin, Z. and Chen, Y. and Su, Z.",,10.1145/3510416,National Natural Science Foundation of China,China,This research is supported by National Natural Science Foundation of China (Grant No. 62032004). This work was supported by Alibaba Group through Alibaba Innovative Research (AIR) programme. Yuting Chen was also supported by CCF-Huawei Innovative Research programme (TC20210701006/CCF2021-admin-270).,"Public key cryptography, Certificate validations;  Certification mutation;  Coverage transfer graph;  Differential testing;  Real-world;  Secure protocols;  Secure sockets layers;  Testing technique;  Transport security;  X.509 certificates, Directed graphs",cited By 1,Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality. © 2023 Association for Computing Machinery.",code structure;  Code summary;  multi-head attention;  Transformer,"Gao, S. and Gao, C. and He, Y. and Zeng, J. and Nie, L. and Xia, X. and Lyu, M.",,10.1145/3522674,National Natural Science Foundation of China,China,,"Benchmarking;  Codes (symbols);  Data flow analysis;  Learning systems;  Syntactics, Code structure;  Code summary;  Learning techniques;  Multi-head attention;  Performance;  Sequence models;  Source codes;  Structure information;  Task domain;  Transformer, Deep learning",cited By 5,Code Structure-Guided Transformer for Source Code Summarization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector, an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research. © 2023 Association for Computing Machinery.",incompatibility detection;  license;  Open source software,"Xu, S. and Gao, Y. and Fan, L. and Liu, Z. and Liu, Y. and Ji, H.",,10.1145/3518994,National Natural Science Foundation of China,China,This work was supported by National Natural Science Foundation of China (No. 62102197) and National Key Research Project of China (No. 2021YFF0307202 and No. 2020YFB1005700).,"Computer software reusability;  Context free grammars;  Open systems, Condition;  Effective tool;  Financial risks;  Incompatibility detection;  Legal risks;  License;  Open-source softwares;  Reuse;  Software license;  Third parties, Open source software",cited By 4,LiDetector: License Incompatibility Detection for Open Source Software,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques. © 2023 Association for Computing Machinery.",adaptive partition testing;  Additional Key Words and PhrasesMetamorphic testing;  feedback control;  metamorphic relation;  random testing;  test execution,"Sun, C.-A. and Dai, H. and Liu, H. and Chen, T.Y.",,10.1145/3533314,National Natural Science Foundation of China,China,,"Application programs;  Feedback control;  Information use;  Open source software;  Software testing;  Verification;  Well testing, Adaptive partition testing;  Adaptive partitions;  Additional key word and phrasesmetamorphic testing;  Key words;  Metamorphic relations;  Metamorphic testing;  Partition testing;  Random testing;  Test case;  Test execution, Cost effectiveness",cited By 1,Feedback-Directed Metamorphic Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners' request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners' detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26%, 37.14%, 59.21%, 68.54% more pages, construct 12.95×, 1.13×, 15.03×, 52.66× more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them. © 2023 Association for Computing Machinery.",attack intent;  scanner;  synchronization;  Web security,"Yin, Z. and Xu, Y. and Ma, F. and Gao, H. and Qiao, L. and Jiang, Y.",,10.1145/3517036,National Natural Science Foundation of China,China,"This research is sponsored in part by the NSFC Program (No. 62022046, No.92167101, U1911401, 62021002, 61802223), National Key Research and Development Project (Grant No. 2019YFB1706203), and the Tsinghua-Webank Scholar Project (No. 20212001829).","Benchmarking;  Program debugging;  Well testing, Attack intent;  Attack vector;  Performance;  Purification mechanisms;  Scanner;  Target application;  Vulnerability detection;  WEB application;  Web applications;  WEB security, Synchronization",cited By 0,Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum;  bookkeeping recognition;  smart contract;  token,"He, Z. and Song, S. and Bai, Y. and Luo, X. and Chen, T. and Zhang, W. and He, P. and Li, H. and Lin, X. and Zhang, X.",,10.1145/3560263,National Natural Science Foundation of China,China,,"Blockchain, Additional key word and phrasesethereum;  Behavior detection;  Block-chain;  Bookkeeping recognition;  Bytecodes;  Key words;  Malicious behavior;  Optimisations;  Token;  Type information, Smart contract",cited By 3,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model-based design has become the predominant approach to the design of hybrid and cyber-physical systems (CPSs). It advocates the use of mathematically founded models to capture heterogeneous digital and analog behaviours from domain-specific formalisms, allowing all engineering tasks of verification, code synthesis, and validation to be performed within a single semantic body. Guaranteeing the consistency among the different views and heterogeneous models of a system at different levels of abstraction, however, poses significant challenges. To address these issues, Hoare and He's Unifying Theories of Programming (UTP) proposes a calculus to capture domain-specific programming and modelling paradigms into a unified semantic framework. Our goal is to extend UTP to form a semantic foundation for CPS design. Higher-order UTP (HUTP) is a conservative extension to Hoare and He's theory that supports the specification of discrete, real-time, and continuous dynamics, concurrency and communication, and higher-order quantification. Within HUTP, we define a calculus of normal hybrid designs to model, analyse, compose, refine, and verify heterogeneous hybrid system models. In addition, we define respective formal semantics for Hybrid Communicating Sequential Processes and Simulink using HUTP. © 2023 Association for Computing Machinery.",CPS;  model-based design;  semantic model;  UTP,"Xu, X. and Talpin, J.-P. and Wang, S. and Zhan, B. and Zhan, N.",,10.1145/3517192,National Natural Science Foundation of China,China,"This research is partly supported by NSFC under Grants No. 61625206, No. 62192732, No. 62032024, No. 61972385, and No. 61732001, and is also partly funded by Inria’s joint research project CONVEX.","Embedded systems;  Formal methods;  Hybrid systems;  Semantics, Analog behavior;  Cybe-physical systems;  Cyber-physical systems;  Domain specific;  High-order;  Higher-order;  Model-based design;  Semantic foundation;  Semantic modelling;  Unifying Theories of Programming, Cyber Physical System",cited By 3,Semantics Foundation for Cyber-physical Systems Using Higher-order UTP,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool's test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest's effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases. © 2023 Association for Computing Machinery.",Android apps;  Android testing;  preference-wise testing,"Pan, M. and Lu, Y. and Pei, Y. and Zhang, T. and Li, X.",,10.1145/3511804,National Natural Science Foundation of China,China,,"Automation;  Program debugging, Android apps;  Android testing;  Automatically generated;  Effective testing;  Experimental evaluation;  Mobile testing;  Preference-wise testing;  Test amplifications;  Test case;  Testing tools, Android (operating system)",cited By 2,Preference-wise Testing of Android Apps via Test Amplification,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool's test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest's effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases. © 2023 Association for Computing Machinery.",Android apps;  Android testing;  preference-wise testing,"Pan, M. and Lu, Y. and Pei, Y. and Zhang, T. and Li, X.",,10.1145/3511804,National Natural Science Foundation of China,China,,"Automation;  Program debugging, Android apps;  Android testing;  Automatically generated;  Effective testing;  Experimental evaluation;  Mobile testing;  Preference-wise testing;  Test amplifications;  Test case;  Testing tools, Android (operating system)",cited By 2,Preference-wise Testing of Android Apps via Test Amplification,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMobile applications;  android;  energy bugs;  energy issues,"Li, X. and Chen, J. and Liu, Y. and Wu, K. and Gallagher, J.P.",,10.1145/3527851,National Natural Science Foundation of China,China,,"Android (operating system);  Energy efficiency;  Energy utilization;  Program debugging;  Software testing, Additional key word and phrasesmobile application;  Android;  Empirical studies;  Energy;  Energy bug;  Energy issues;  Input sequence;  Key words;  Mobile app;  Mobile applications, Potential energy",cited By 2,Combatting Energy Issues for Mobile Applications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Method name prediction;  naming convention,"Wang, S. and Wen, M. and Lin, B. and Liu, Y. and Bissyandé, T.F. and Mao, X.",,10.1145/3597203,National Research Council Canada,Canada,"This work was supported by the National Natural Science Foundation of China, Nos. 62002125 and 61932021, the Young Elite Scientists Sponsorship Program by CAST (Grant No. 2021QNRC001), and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 949014).","Codes (symbols);  Deep learning;  Heuristic methods;  Object oriented programming, Automated tool support;  Development tasks;  Empirical analysis;  Large-scales;  Method implementations;  Method name prediction;  Naming convention;  Objectoriented programming (OOP);  Real world projects;  Research efforts, Forecasting",cited By 0,Pre-implementation Method Name Prediction for Object-oriented Programming,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks. © 2023 Association for Computing Machinery.",Automatic program repair;  deep learning,"Sohn, J. and Kang, S. and Yoo, S.",,10.1145/3563210,National Research Foundation of Korea,South Korea,,"Evolutionary algorithms;  Optimization;  Repair, Automatic program repair;  Automatic programs;  Data collection;  Deep learning;  Differential Evolution;  Input-output;  Misbehaviour;  Neural weights;  Repair techniques;  Search-based, Deep neural networks",cited By 5,Arachne: Search-Based Repair of Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,National Research Foundation of Korea,South Korea,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning systems;  Test adequacy,"Kim, J. and Feldt, R. and Yoo, S.",,10.1145/3546947,National Research Foundation of Korea,South Korea,,"Autonomous vehicles;  Large dataset;  Learning systems;  Medical imaging;  Safety engineering;  Software testing;  Statistical tests, Autonomous driving;  Coverage metrics;  Deep learning system;  Safety-critical domain;  Software testings;  System testing;  Test adequacies;  Test adequacy criteria;  Training data;  Training dataset, Deep neural networks",cited By 0,Evaluating Surprise Adequacy for Deep Learning System Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,National Research Foundation Singapore,Singapore,,"Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode. Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bug reports;  deep learning;  duplicate bug report detection;  empirical study,"Zhang, T. and Han, D. and Vinayakarao, V. and Irsan, I.C. and Xu, B. and Thung, F. and Lo, D. and Jiang, L.",,10.1145/3576042,National Research Foundation Singapore,Singapore,"This research/project is supported by the National Research Foundation, Singapore, under its Industry Alignment Fund - Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.","Bug reports;  Deep learning;  Duplicate bug report detection;  Duplicate bug reports;  Empirical studies;  Industry use;  Issue Tracking;  Mozilla;  Simple++;  Tracking system, Deep learning",cited By 1,Duplicate Bug Report Detection: How Far Are We?,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCode Search;  Duplicate questions;  paraphrase mining;  Stack Overflow,"Gao, Z. and Xia, X. and Lo, D. and Grundy, J. and Zhang, X. and Xing, Z.",,10.1145/3550150,National Research Foundation Singapore,Singapore,"This research was partially supported by ARC Laureate Fellowship FL190100035 and National Research Foundation, Singapore, under its Industry Alignment Fund – Pre-positioning (IAF-PP) Funding Initiative.","Python;  Query processing, Additional key word and phrasescode search;  Duplicate question;  Key words;  Large volumes;  Paraphrase mining;  Performance;  Software developer;  Stack overflow;  User query;  Worked examples, Large dataset",cited By 3,I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyze and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study, conducted on popular open-source projects, has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past. © 2023 Association for Computing Machinery.",Concurrency;  program repair;  static analysis,"Costea, A. and Tiwari, A. and Chianasta, S. and Kishore, R. and Roychoudhury, A. and Sergey, I.",,10.1145/3546942,National Research Foundation Singapore,Singapore,,"Open source software;  Program debugging;  Repair;  Software design, Bug detection;  Bug-free;  Code scaling;  Concurrency;  Concurrency bugs;  Concurrents programs;  Data races;  Program repair;  Repair techniques;  State of the art, Static analysis",cited By 1,Hippodrome: Data Race Repair Using Static Analysis Summaries,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,National Research Foundation Singapore,Singapore,,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we offer a handful of such types), then (2) lookup those types in our clusters, and then (3) just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clustering;  empirical analysis;  fairness metrics;  Software fairness;  theoretical analysis,"Majumder, S. and Chakraborty, J. and Bai, G.R. and Stolee, K.T. and Menzies, T.",,10.1145/3585006,National Science Foundation,United States,,"Learning systems;  Software testing, 'current;  Clusterings;  Empirical analysis;  Fairness metric;  Machine learning software;  Pressung;  Recent researches;  Software fairness;  Testing machine;  Theoretical analyse, Classification (of information)",cited By 1,Fair Enough: Searching for Sufficient Measures of Fairness,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Continuous integration (CI) practices encourage developers to frequently integrate code into a shared repository. Each integration is validated by automatic build and testing such that errors are revealed as early as possible. When CI failures or integration errors are reported, existing techniques are insufficient to automatically locate the root causes for two reasons. First, a CI failure may be triggered by faults in source code and/or build scripts, whereas current approaches consider only source code. Second, a tentative integration can fail because of build failures and/or test failures, whereas existing tools focus on test failures only. This article presents UniLoc, the first unified technique to localize faults in both source code and build scripts given a CI failure log, without assuming the failure's location (source code or build scripts) and nature (a test failure or not). Adopting the information retrieval (IR) strategy, UniLoc locates buggy files by treating source code and build scripts as documents to search and by considering build logs as search queries. However, instead of naïvely applying an off-the-shelf IR technique to these software artifacts, for more accurate fault localization, UniLoc applies various domain-specific heuristics to optimize the search queries, search space, and ranking formulas. To evaluate UniLoc, we gathered 700 CI failure fixes in 72 open source projects that are built with Gradle. UniLoc could effectively locate bugs with the average mean reciprocal rank value as 0.49, mean average precision value as 0.36, and normalized discounted cumulative gain value as 0.54. UniLoc outperformed the state-of-the-art IR-based tool BLUiR and Locus. UniLoc has the potential to help developers diagnose root causes for CI failures more accurately and efficiently. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CI failures;  Fault localization;  information retrieval (IR),"Hassan, F. and Meng, N. and Wang, X.",,10.1145/3593799,National Science Foundation,United States,"This material is based in part upon work supported by National Science Foundation awards CSPECC-1736209, CCF-1846467, CCF-2007718, CNS-2221843, CCF-1845446, CCF-2006278, and CCF-2152819.","Codes (symbols);  Computer programming languages;  Failure (mechanical);  Integration;  Integration testing;  Open source software, Continuous integration failure;  Continuous integrations;  Fault localization;  Information retrieval;  Integration error;  Root cause;  Search queries;  Shared repositories;  Source codes;  Test failure, Information retrieval",cited By 0,UniLoc: Unified Fault Localization of Continuous Integration Failures,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"A key goal of software engineering research is to improve the environments, tools, languages, and techniques programmers use to efficiently create quality software. Successfully designing these tools and demonstrating their effectiveness involves engaging with tool users - software engineers. Researchers often want to conduct user studies of software engineers to collect direct evidence. However, running user studies can be difficult, and researchers may lack solution strategies to overcome the barriers, so they may avoid user studies. To understand the challenges researchers face when conducting programmer user studies, we interviewed 26 researchers. Based on the analysis of interview data, we contribute (i) a taxonomy of 18 barriers researchers encounter; (ii) 23 solution strategies some researchers use to address 8 of the 18 barriers in their own studies; and (iii) 4 design ideas, which we adapted from the behavioral science community, that may lower 8 additional barriers. To validate the design ideas, we held an in-person all-day focus group with 16 researchers. © 2023 Copyright held by the owner/author(s).",Empirical software engineering;  experiments;  human participants;  human subjects;  meta study;  research methodology;  user study,"Davis, M.C. and Aghayi, E. and Latoza, T.D. and Wang, X. and Myers, B.A. and Sunshine, J.",,10.1145/3587157,National Science Foundation,United States,"This material is supported in part by NSF grants 2016600, 2016586, and 2016604. Any opinions, findings, or conclusions expressed in this material are those of the authors and do not necessarily reflect those of any of the sponsors.","Software engineering, Design ideas;  Empirical Software Engineering;  Human participant;  Human subjects;  Meta-study;  Quality software;  Research methodologies;  Software engineering research;  Solution strategy;  User study, Behavioral research",cited By 2,What's (Not) Working in Programmer User Studies?,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Mobile apps are widely used and often process users' sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party library's methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies. © 2023 Association for Computing Machinery.",mobile application;  natural language processing;  Privacy leak,"Zhang, X. and Heaps, J. and Slavin, R. and Niu, J. and Breaux, T. and Wang, X.",,10.1145/3569936,National Science Foundation,United States,"This work was supported by the NSF Awards 1948244, 1736209, 2007718, 1846467, and 2221843.","Android (operating system);  Libraries;  Natural language processing systems, Dynamics analysis;  Language processing;  Mobile app;  Mobile applications;  Natural language processing;  Natural languages;  Privacy leak;  Sensitive datas;  Sensitive informations;  Third parties, Sensitive data",cited By 0,DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests.Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests (SLI-KUBE) to quantify the frequency of the identified security misconfigurations.Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext, and activation of hostIPC. Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us.Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Configuration;  container orchestration;  devops;  devsecops;  empirical study;  Kubernetes;  misconfiguration;  security,"Rahman, A. and Shamim, S.I. and Bose, D.B. and Pandita, R.",,10.1145/3579639,National Science Foundation,United States,,"Containers;  Open source software;  Open systems, Business organizations;  Configuration;  Container orchestration;  Devops;  Devsecop;  Empirical studies;  Kubernetes;  Misconfigurations;  Open-source;  Security, Static analysis",cited By 6,Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages' builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider's version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Breaking changes;  change impact;  dependency management;  npm;  Semantic Version,"Venturini, D. and Cogo, F.R. and Polato, I. and Gerosa, M.A. and Wiese, I.S.",,10.1145/3576037,National Science Foundation,United States,,"Recovery, Breaking change;  Breakings;  Bug fixes;  Change impacts;  Complex software systems;  Dependency management;  Down-stream;  Empirical studies;  Npm;  Semantic version, Semantics",cited By 0,I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice - Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",build selection;  Continuous Integration;  Software maintenance;  test selection,"Jin, X. and Servant, F.",,10.1145/3576038,National Science Foundation,United States,,"Computer software maintenance;  Cost benefit analysis;  Integration;  Safety engineering, Build selection;  Continuous integrations;  Cost saving;  Google+;  High costs;  High safety;  Integration systems;  Mozilla;  Selection techniques;  Test selection, Software testing",cited By 2,HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Core features (functionalities) of an app can often be accessed and invoked in several ways, i.e., through alternative sequences of user-interface (UI) interactions. Given the manual effort of writing tests, developers often only consider the typical way of invoking features when creating the tests (i.e., the ""sunny day scenario""). However, the alternative ways of invoking a feature are as likely to be faulty. These faults would go undetected without proper tests. To reduce the manual effort of creating UI tests and help developers more thoroughly examine the features of apps, we present Route, an automated tool for feature-based UI test augmentation for Android apps. Route first takes a UI test and the app under test as input. It then applies novel heuristics to find additional high-quality UI tests, consisting of both inputs and assertions, that verify the same feature as the original test in alternative ways. Application of Route on several dozen tests for popular apps on Google Play shows that for 96% of the existing tests, Route was able to generate at least one alternative test. Moreover, the fault detection effectiveness of augmented test suites in our experiments showed substantial improvements of up to 39% over the original test suites. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",GUI test augmentation;  mobile testing;  test amplification;  test reuse,"Lin, J.-W. and Salehnamadi, N. and Malek, S.",,10.1145/3571851,National Science Foundation,United States,"This work was supported in part by award numbers 2211790, 1823262, and 2106306 from the National Science Foundation.","Fault detection;  Software testing, Automated tools;  Core features;  GUI test augmentation;  Interface interaction;  Interface testings;  Mobile testing;  Reuse;  Sunny days;  Test amplifications;  Test reuse, User interfaces",cited By 0,Route: Roads Not Taken in UI Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware testing;  deep neural networks;  generative models;  test coverage,"Dola, S. and Dwyer, M.B. and Soffa, M.L.",,10.1145/3576040,National Science Foundation,United States,,"Black-box testing;  Cost effectiveness, Additional key word and phrasessoftware testing;  Black box test;  Feature interactions;  Generative model;  Input distributions;  Key words;  Neural-networks;  Test adequacies;  Test inputs;  Test-coverage, Deep neural networks",cited By 2,Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Due to the exploratory nature of computational notebook development, a notebook can be extensively evolved even though it is small, potentially incurring substantial technical debt. Indeed, in interview studies notebook authors have attested to performing ongoing tidying and big cleanups. However, many notebook authors are not trained as software developers, and environments like JupyterLab possess few features to aid notebook maintenance. As software refactoring is traditionally a critical tool for reducing technical debt, we sought to better understand the unique and growing ecology of computational notebooks by investigating the refactoring of public Jupyter notebooks. We randomly selected 15,000 Jupyter notebooks hosted on GitHub and studied 200 with meaningful commit histories. We found that notebook authors do refactor, favoring a few basic classic refactorings as well as those involving the notebook cell construct. Those with a computing background refactored differently than others, but not more so. Exploration-focused notebooks had a unique refactoring profile compared to more exposition-focused notebooks. Authors more often refactored their code as they went along, rather than deferring maintenance to big cleanups. These findings point to refactoring being intrinsic to notebook development. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesComputational notebooks;  end-user programming;  refactoring,"Liu, E.S. and Lukes, D.A. and Griswold, W.G.",,10.1145/3576036,National Science Foundation,United States,This research was supported in part by the National Science Foundation under Grant No. CCF-1719155.,"Additional key word and phrasescomputational notebook;  End-user programming;  End-users;  Interview study;  Key words;  Refactorings;  Software developer;  Software environments;  Software refactoring;  Technical debts, Computer programming",cited By 0,Refactoring in Computational Notebooks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating a 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers' ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. This suggests that Python developers are less likely to notice potential for vulnerabilities in complex code than in simple code, whereas Java developers are more likely to recognize the extra complexity and apply more care, but are more careless with simple code. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertise did not improve the developers' ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers' ability to reason about APIs with blindspots, but less extroverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSoftware vulnerabilities;  API blindspots;  APIs;  Java;  Python,"Brun, Y. and Lin, T. and Somerville, J.E. and Myers, E.M. and Ebner, N.",,10.1145/3571850,National Science Foundation,United States,"This work is supported by the National Science Foundation under grants CCF-1453474, CNS-1513055, CNS-1513457, CNS-1513572, and CCF-1564162.","Application programming interfaces (API);  Codes (symbols);  High level languages;  Java programming language, Additional key word and phrasessoftware vulnerability;  API;  API blindspot;  Complex codes;  Controlled experiment;  Java;  Java API;  Java developers;  Key words;  Simple++, Python",cited By 0,Blindspots in Python and Java APIs Result in Vulnerable Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The Linux Kernel is a world-class operating system controlling most of our computing infrastructure: mobile devices, Internet routers and services, and most of the supercomputers. Linux is also an example of low-level software with no comprehensive regression test suite (for good reasons). The kernel's tremendous societal importance imposes strict stability and correctness requirements. These properties make Linux a challenging and relevant target for static automated program repair (APR). Over the past decade, a significant progress has been made in dynamic APR. However, dynamic APR techniques do not translate naturally to systems without tests. We present a static APR technique addressing sequential locking API misuse bugs in the Linux Kernel. We attack the key challenge of static APR, namely, the lack of detailed program specification, by combining static analysis with machine learning to complement the information presented by the static analyzer. In experiments on historical real-world bugs in the kernel, we were able to automatically re-produce or propose equivalent patches in 85% of the human-made patches, and automatically rank them among the top three candidates for 64% of the cases and among the top five for 74%. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAutomated repair;  api misuse;  static program repair,"Cruz-Carlon, J. and Varshosaz, M. and Le Goues, C. and Wasowski, A.",,10.1145/3548684,National Science Foundation,United States,"This work is supported by Independent Research Fund Denmark DFF https://dff.dk/en under Grant Agreement 8022-00326B DFF and by National Science Foundation NSF https://www.nsf.gov/ , Grant No. CCF-1750116 NSF.","Linux;  Locks (fasteners);  Program debugging;  Repair;  Software testing;  Supercomputers, Additional key word and phrasesautomated repair;  Api misuse;  Computing infrastructures;  Internet-services;  Key words;  Linux kernel;  Repair techniques;  Static program;  Static program repair;  World class, Static analysis",cited By 1,Patching Locking Bugs Statically with Crayons,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model Driven Engineering (MDE) is a general-purpose engineering methodology to elevate system design, maintenance, and analysis to corresponding activities on models. Models (graphical and/or textual) of a target application are automatically transformed into source code, performance models, Promela files (for model checking), and so on for system analysis and construction.Models are instances of metamodels. One form an MDE metamodel can take is a [class diagram, constraints] pair: the class diagram defines all object diagrams that could be metamodel instances; object constraint language (OCL) constraints eliminate semantically undesirable instances.A metamodel refactoring is an invertible semantics-preserving co-transformation, i.e., it transforms both a metamodel and its models without losing data. This article addresses a subproblem of metamodel refactoring: how to prove the correctness of refactorings of class diagrams without OCL constraints using the Coq Proof Assistant. © 2023 Association for Computing Machinery.",Class diagram refactorings;  Coq;  object diagram refactorings,"Altoyan, N. and Batory, D.",,10.1145/3549541,National Science Foundation,United States,,"Metadata;  Model checking;  Semantics, Class diagram refactoring;  Class diagrams;  Coq;  Meta model;  Model-driven Engineering;  Object Constraint Language;  Object diagram refactoring;  Object diagrams;  Refactorings, Systems analysis",cited By 1,On Proving the Correctness of Refactoring Class Diagrams of MDE Metamodels,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction refinement;  fault localization;  selective instrumentation;  Statistical debugging,"Zuo, Z. and Niu, X. and Zhang, S. and Fang, L. and Khoo, S.C. and Lu, S. and Sun, C. and Xu, G.H.",,10.1145/3544790,National Science Foundation,United States,,"Abstracting;  Codes (symbols);  Cost benefit analysis;  Program debugging, Abstraction-refinement;  Analysis costs;  Automated debugging;  Fault localization;  Program code;  Program instrumentations;  Selective instrumentation;  Statistical debugging;  Statistical techniques;  Time-consuming tasks, Statistics",cited By 0,Toward More Efficient Statistical Debugging with Abstraction Refinement,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE's overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model. © 2023 Copyright held by the owner/author(s).",data provenance;  decompilation;  Machine learning,"Dramko, L. and Lacomis, J. and Yin, P. and Schwartz, E. and Allamanis, M. and Neubig, G. and Vasilescu, B. and Le Goues, C.",,10.1145/3546946,National Science Foundation,United States,This material is based upon work supported in part by the Software Engineering Institute and National Science Foundation (NSF awards CCF-1815287 and CCF-1910067). We also gratefully acknowledge hardware support (Quadro P6000 GPU) from the NVIDIA Corporation.,"Quality control, Data provenance;  Decompilation;  Decompilers;  Domain specific;  Executables;  ITS data;  Machine learning models;  Machine-learning;  Performance;  Training data, Machine learning",cited By 0,DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs) - comment changes committed independently of code changes - which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice - especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio - proportion of ICCs among all comment changes - is ∼15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools. © 2023 Association for Computing Machinery.",Code comments;  coding guidelines;  software documentation;  software evolution,"Wang, C. and He, H. and Pal, U. and Marinov, D. and Zhou, M.",,10.1145/3546949,National Science Foundation,United States,,"Codes (symbols);  Computer software maintenance;  Java programming language;  Open source software;  Software design, 'current;  Change ratio;  Code comment;  Coding guideline;  High quality source;  Low qualities;  Software development and maintenances;  Software documentation;  Software Evolution;  Source code comments, Taxonomies",cited By 0,Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests.Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests (SLI-KUBE) to quantify the frequency of the identified security misconfigurations.Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext, and activation of hostIPC. Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us.Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Configuration;  container orchestration;  devops;  devsecops;  empirical study;  Kubernetes;  misconfiguration;  security,"Rahman, A. and Shamim, S.I. and Bose, D.B. and Pandita, R.",,10.1145/3579639,National Security Agency,United States,"This research was partially funded by the U.S. National Science Foundation (NSF) Award # 2247141, Award # 2310179, and the U.S. National Security Agency (NSA) Award # H98230-21-1-0175.","Containers;  Open source software;  Open systems, Business organizations;  Configuration;  Container orchestration;  Devops;  Devsecop;  Empirical studies;  Kubernetes;  Misconfigurations;  Open-source;  Security, Static analysis",cited By 6,Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyze and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study, conducted on popular open-source projects, has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past. © 2023 Association for Computing Machinery.",Concurrency;  program repair;  static analysis,"Costea, A. and Tiwari, A. and Chianasta, S. and Kishore, R. and Roychoudhury, A. and Sergey, I.",,10.1145/3546942,National University of Singapore,Singapore,This work was partially supported by the National Research Foundation Singapore (National Satellite of Excellence in Trustworthy Software Systems) and by Singapore MoE Tier 1 grant no. IG18-SG102.,"Open source software;  Program debugging;  Repair;  Software design, Bug detection;  Bug-free;  Code scaling;  Concurrency;  Concurrency bugs;  Concurrents programs;  Data races;  Program repair;  Repair techniques;  State of the art, Static analysis",cited By 1,Hippodrome: Data Race Repair Using Static Analysis Summaries,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,National University of Singapore,Singapore,"This work is supported in part by the General Research Fund (GRF) of the Research Grants Council of Hong Kong, and the industry research funds of City University of Hong Kong (7005217, 9220097, 9220103, 9229029, 9229098, 9678149). The National Natural Science Foundation of China under Grant Nos. 62192731 and 61751210. The Singapore National Research Foundation and National University of Singapore through its National Satellite of Excellence in Trustworthy Software Systems (NSOE-TSS) under the Trustworthy Software Systems Core Technologies Grant (TSSCTG) NSOE-TSS2019-05. The Natural Science Foundation of Chongqing City (cstc2021jcyj-msxmX1115).","Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify ""good""metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques. © 2023 Association for Computing Machinery.",adaptive partition testing;  Additional Key Words and PhrasesMetamorphic testing;  feedback control;  metamorphic relation;  random testing;  test execution,"Sun, C.-A. and Dai, H. and Liu, H. and Chen, T.Y.",,10.1145/3533314,Natural Science Foundation of Beijing Municipality,China,"This research is supported by the National Natural Science Foundation of China (Grant No. 61872039), the Australian Research Council Discovery Project (Grant No. DP210102447), the Beijing Natural Science Foundation (Grant No. 4162040), the Aeronautical Science Foundation of China (Grant No. 2016ZD74004), and the Fundamental Research Funds for the Central Universities (Grant No. FRF-GF-19-B19).","Application programs;  Feedback control;  Information use;  Open source software;  Software testing;  Verification;  Well testing, Adaptive partition testing;  Adaptive partitions;  Additional key word and phrasesmetamorphic testing;  Key words;  Metamorphic relations;  Metamorphic testing;  Partition testing;  Random testing;  Test case;  Test execution, Cost effectiveness",cited By 1,Feedback-Directed Metamorphic Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  mining software repositories;  software evolution;  software testing,"Sun, W. and Yan, M. and Liu, Z. and Xia, X. and Lei, Y. and Lo, D.",,10.1145/3607183,Natural Science Foundation of Chongqing,China,,"Codes (symbols);  Computer software maintenance;  Corrective maintenance;  Just in time production, Additional key word and phrasesempirical software engineering;  Co-evolution;  Key words;  Mining software;  Mining software repository;  Production test;  Software Evolution;  Software repositories;  Software testings;  Test code, Software testing",cited By 0,Revisiting the Identification of the Co-evolution of Production and Test Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,Natural Science Foundation of Chongqing,China,,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifiCation metHod Of production-teSt co-EvolutioN) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method's performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  mining software repositories;  software evolution;  software testing,"Sun, W. and Yan, M. and Liu, Z. and Xia, X. and Lei, Y. and Lo, D.",,10.1145/3607183,Natural Science Foundation of Chongqing Municipality,China,"This work was supported in part by the National Key Research and Development Project (No. 2021YFB1714200), the Fundamental Research Funds for the Central Universities (No. 2022CDJDX-005), the Chongqing Technology Innovation and Application Development Project (No. CSTB2022TIAD-STX0007 and No. CSTB2022TIAD-KPX0067), the National Natural Science Foundation of China (No. 62002034), and the Natural Science Foundation of Chongqing (No. cstc2021jcyj-msxmX0538).","Codes (symbols);  Computer software maintenance;  Corrective maintenance;  Just in time production, Additional key word and phrasesempirical software engineering;  Co-evolution;  Key words;  Mining software;  Mining software repository;  Production test;  Software Evolution;  Software repositories;  Software testings;  Test code, Software testing",cited By 0,Revisiting the Identification of the Co-evolution of Production and Test Code,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones' impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3) Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning software;  co-changed clone;  code clone,"Mo, R. and Zhang, Y. and Wang, Y. and Zhang, S. and Xiong, P. and Li, Z. and Zhao, Y.",,10.1145/3607181,Natural Science Foundation of Hubei Province,China,"This work is supported by the National Natural Science Foundation of China under Grant No. 62002129, the Natural Science Foundation of Hubei Province of China under Grant No. 2021CFB577, the Knowledge Innovation Program of Wuhan-Shuguang Project under Grant No. 2022010801020280.","Codes (symbols);  Computer software maintenance;  Deep learning, Additional key word and phrasesdeep learning software;  Co-changed clone;  Code clone;  Code cloning;  Key words;  Learning models;  Learning projects;  Learning software;  Reuse;  Software developer, Cloning",cited By 0,Exploring the Impact of Code Clones on Deep Learning Software,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction. © 2023 Copyright held by the owner/author(s).",distributed representation learning;  embeddings;  explanation;  features combination;  machine learning;  patch correctness;  Program repair,"Tian, H. and Liu, K. and Li, Y. and Kaboré, A.K. and Koyuncu, A. and Habib, A. and Li, L. and Wen, J. and Klein, J. and Bissyandé, T.F.",,10.1145/3576039,Natural Science Foundation of Jiangsu Province,China,,"Classification (of information);  Codes (symbols);  Embeddings;  Forecasting;  Software testing, Distributed representation;  Distributed representation learning;  Dynamic information;  Embeddings;  Explanation;  Feature combination;  Machine-learning;  Patch correctness;  Program repair;  State of the art, Machine learning",cited By 3,The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated program repair;  fix pattern;  static analysis,"Liu, K. and Zhang, J. and Li, L. and Koyuncu, A. and Kim, D. and Ge, C. and Liu, Z. and Klein, J. and Bissyandé, T.F.",,10.1145/3579637,Natural Science Foundation of Jiangsu Province,China,,"Automation;  Defects;  Genetic algorithms;  Genetic programming;  Program debugging;  Repair;  Semantics, Automated program repair;  Bug-fixing;  Development history;  Fault localization;  Fix pattern;  Mutation operators;  Performance;  Repair system;  Repair tools;  Test case, Static analysis",cited By 5,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware engineering for artificial intelligence;  model contract;  model reuse;  pre-trained models,"Gong, L. and Zhang, J. and Wei, M. and Zhang, H. and Huang, Z.",,10.1145/3569934,Natural Science Foundation of Jiangsu Province,China,,"Artificial intelligence;  Codes (symbols);  Contracts;  Knowledge management;  Learning systems, Additional key word and phrasessoftware engineering for artificial intelligence;  Condition;  Exploratory studies;  Key words;  Model contract;  Model repositories;  Model reuse;  Pre-trained model;  Pre/post conditions;  Reuse, Computer software reusability",cited By 0,What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction refinement;  fault localization;  selective instrumentation;  Statistical debugging,"Zuo, Z. and Niu, X. and Zhang, S. and Fang, L. and Khoo, S.C. and Lu, S. and Sun, C. and Xu, G.H.",,10.1145/3544790,Natural Science Foundation of Jiangsu Province,China,,"Abstracting;  Codes (symbols);  Cost benefit analysis;  Program debugging, Abstraction-refinement;  Analysis costs;  Automated debugging;  Fault localization;  Program code;  Program instrumentations;  Selective instrumentation;  Statistical debugging;  Statistical techniques;  Time-consuming tasks, Statistics",cited By 0,Toward More Efficient Statistical Debugging with Abstraction Refinement,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Rust is an emerging programming language that aims to prevent memory-safety bugs. However, the current design of Rust also brings side effects, which may increase the risk of memory-safety issues. In particular, it employs ownership-based resource management and enforces automatic deallocation of unused resources without using the garbage collector. It may therefore falsely deallocate reclaimed memory and lead to use-after-free or double-free issues. In this article, we study the problem of invalid memory deallocation and propose SafeDrop, a static path-sensitive data-flow analysis approach to detect such bugs. Our approach analyzes each function of a Rust crate iteratively in a flow-sensitive and field-sensitive way. It leverages a modified Tarjan algorithm to achieve scalable path-sensitive analysis and a cache-based strategy for efficient inter-procedural analysis. We have implemented our approach and integrated it into the Rust compiler. Experiment results show that the approach can successfully detect all such bugs in our experiments with a limited number of false positives and incurs a very small overhead compared to the original compilation time. © 2023 Association for Computing Machinery.",data-flow analysis;  meet over path;  path sensitivity;  Rust,"Cui, M. and Chen, C. and Xu, H. and Zhou, Y.",,10.1145/3542948,Natural Science Foundation of Shanghai,China,"This work was supported by Alibaba Group, CCF-ANT Fund (Project No. RF20210017) and the Natural Science Foundation of Shanghai (No. 22ZR1407900).","Data transfer;  Iterative methods;  Program debugging;  Safety engineering;  Sensitive data, 'current;  Data-flow analysis;  Meet over path;  Memory de-allocation;  Memory safety;  Path sensitivity;  Rust;  Safety issues;  Side effect;  Static data-flow analysis, Data flow analysis",cited By 1,SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static Data-flow Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum;  bookkeeping recognition;  smart contract;  token,"He, Z. and Song, S. and Bai, Y. and Luo, X. and Chen, T. and Zhang, W. and He, P. and Li, H. and Lin, X. and Zhang, X.",,10.1145/3560263,Natural Science Foundation of Sichuan Province,China,,"Blockchain, Additional key word and phrasesethereum;  Behavior detection;  Block-chain;  Bookkeeping recognition;  Bytecodes;  Key words;  Malicious behavior;  Optimisations;  Token;  Type information, Smart contract",cited By 3,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial patch;  certified accuracy;  deep learning;  neural networks;  robustness certification;  vision transformer,"Huang, Y. and Ma, L. and Li, Y.",,10.1145/3591870,Natural Sciences and Engineering Research Council of Canada,Canada,"Yuanchun Li is supported in part by the National Key R&D Program of China (Grant No. 2022YFF0604501) and NSFC (Grant No. 62272261). Yuheng Huang and Lei Ma are supported in part by Canada CIFAR AI Chairs Program, the Natural Sciences and Engineering Research Council of Canada (NSERC Grants No. RGPIN-2021-02549, No. RGPAS-2021-00034, and No. DGECR-2021-00019). Lei Ma’s research is also supported by JST-Mirai Program Grant No. JPMJMI20B8 and JSPS KAKENHI Grants No. JP20H04168 and No. JP21H04877.","Accident prevention;  Convolutional neural networks;  Deep neural networks;  Electric transformer testing;  Learning systems, Adversarial patch;  Certified accuracy;  Classical neural networks;  Deep learning;  Exhaustive testing;  Neural-networks;  Performance;  Record-breaking performance;  Robustness certification;  Vision transformer, Cameras",cited By 2,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConcept location;  automated query reformulation;  bug localization;  Internet-scale code search;  machine learning;  query quality analysis;  systematic literature review;  term weighting,"Rahman, M.M. and Roy, C.K.",,10.1145/3607179,Natural Sciences and Engineering Research Council of Canada,Canada,"This research was supported by a Tenure-track startup grant, Dalhousie University, an International Dean’s Scholarship from University of Saskatchewan (2014–2017), a Saskatchewan Innovation & Opportunity Scholarship (2017–2018), and the Natural Sciences and Engineering Research Council of Canada (NSERC).","Program debugging;  Quality control;  Search engines, Additional key word and phrasesconcept location;  Automated query reformulation;  Bug localizations;  Code search;  Internet-scale code search;  Key words;  Machine-learning;  Query quality analyse;  Query reformulation;  Systematic literature review;  Term weighting, Machine learning",cited By 0,A Systematic Review of Automated Query Reformulations in Source Code Search,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  DNN repair;  neural architecture search,"Qi, H. and Wang, Z. and Guo, Q. and Chen, J. and Juefei-Xu, F. and Zhang, F. and Ma, L. and Zhao, J.",,10.1145/3585005,Natural Sciences and Engineering Research Council of Canada,Canada,"This work was supported in part by JST-Mirai Program Grant No. JPMJMI20B8, JSPS KAKENHI Grant No. JP20H04168, No. JP21H04877, JST the establishment of university fellowships toward the creation of science technology innovation, Grant No. JPMJFS2132, as well as Canada CIFAR AI Chairs Program and the Natural Sciences and Engineering Research Council of Canada (NSERC No. RGPIN-2021-02549, No. RGPAS-2021-00034, No. DGECR-2021-00019). This work was also supported by A*STAR Centre for Frontier AI Research.","Multilayer neural networks;  Network architecture;  Network layers;  Repair;  Spectrum analysis, Applications domains;  Deep learning;  Deep neural network repair;  Industrial tasks;  Localisation;  Network repairs;  Neural architecture search;  Neural architectures;  Overfitting;  Real-world, Deep neural networks",cited By 0,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×-446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186-1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification models;  model compression;  Model dissemination;  neural networks,"Tian, Y. and Zhang, W. and Wen, M. and Cheung, S.-C. and Sun, C. and Ma, S. and Jiang, Y.",,10.1145/3583564,Natural Sciences and Engineering Research Council of Canada,Canada,"The authors at the Hong Kong University of Science and Technology are supported by National Natural Science Foundation of China (Grant No: 61932021), Hong Kong RGC/GRF (Grant No: 16207120), Hong Kong RGC/RIF (Grant No: R5034-18), Hong Kong ITF (Grant No: MHP/055/19), Hong Kong PhD Fellowship Scheme, HKUST RedBird Academic Excellence Award, and the MSRA Collaborative Research Grant. The authors at University of Waterloo are supported by Cisco Research Gift, Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grant, and CFI-JELF Project #40736.","Black-box testing;  Classification (of information);  Deep neural networks;  Efficiency;  Iterative methods;  Markov processes;  Neural network models, Classification models;  Classification tasks;  Embedded device;  Image classification model;  Images classification;  Model compression;  Model dissemination;  Neural network model;  Neural-networks;  Original model, Image classification",cited By 1,Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ADS testing;  module-level testing;  system security;  system-level testing,"Tang, S. and Zhang, Z. and Zhang, Y. and Zhou, J. and Guo, Y. and Liu, S. and Guo, S. and Li, Y.-F. and Ma, L. and Xue, Y. and Liu, Y.",,10.1145/3579642,Natural Sciences and Engineering Research Council of Canada,Canada,"This work was supported in part by the Anhui Provincial Department of Science and Technology under Grant 202103a05020009, in part by National Natural Science Foundation of China under Grant 61972373, the Basic Research Program of Jiangsu Province under Grant BK20201192 and the National Research Foundation Singapore under its NSoE Programme (Award Number: NSOE-TSS2019-03). The research of Dr. Xue is also supported by CAS Pioneer Hundred Talents Program of China. Lei Ma’s research was supported in part by Canada CIFAR AI Chairs Program, NSERC (No. RGPIN-2021-02549, No. RGPAS-2021-00034, No. DGECR-2021-00019), as well as JSPS KAKENHI Grant No. JP20H04168, No. JP21H04877, JST-Mirai Program Grant No. JPMJMI20B8.","Deceleration, Automated driving system testing;  Automated driving systems;  Different domains;  Module-level testing;  Perception planning;  Planning and control;  System level testing;  System security;  System testing;  Testing technique, Software testing",cited By 4,A Survey on Automated Driving System Testing: Landscapes and Trends,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points. © 2023 Association for Computing Machinery.",DNN debugging;  DNN explanation;  DNN functional safety analysis;  explainable AI;  heatmaps,"Fahmy, H. and Pastore, F. and Briand, L. and Stifter, T.",,10.1145/3569935,Natural Sciences and Engineering Research Council of Canada,Canada,"This project has received funding from IEE Luxembourg, Luxembourg’s National Research Fund (FNR) under grant BRIDGES2020/IS/14711346/FUNTASY, the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no. 694277), and NSERC of Canada under the Discovery and CRC programs.","Embedded systems;  Genetic algorithms;  Hazards;  Image enhancement;  Program debugging;  Safety testing, Deep neural network debugging;  Deep neural network explanation;  Deep neural network functional safety analyse;  Explainable AI;  Functional Safety;  Heatmaps;  Network debugging;  Real-world image;  Safety analysis;  Safety critical systems, Deep neural networks",cited By 0,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker's on-execution validation of DNN-based program's properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMD's offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs. © 2023 Association for Computing Machinery.",Neural networks;  property-based debugging;  training programs,"Ben Braiek, H. and Khomh, F.",,10.1145/3529318,Natural Sciences and Engineering Research Council of Canada,Canada,This work is partly funded by the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Fonds de Recherche du Quebec (FRQ).,"Aircraft accidents;  Errors;  Feedforward neural networks;  Neural network models;  Program debugging;  Software testing;  Training aircraft, Aircraft collision avoidance systems;  Car collisions;  Neural network model;  Neural networks trainings;  Neural-networks;  Performance;  Property-based;  Property-based debugging;  Safety critical systems;  Training program, Deep neural networks",cited By 2,Testing Feedforward Neural Networks Training Programs,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDNN explanation;  clustering;  DNN debugging;  DNN functional safety analysis;  transfer learning,"Attaoui, M. and Fahmy, H. and Pastore, F. and Briand, L.",,10.1145/3550271,Natural Sciences and Engineering Research Council of Canada,Canada,"This project has received funding from IEE Luxembourg, Luxembourg’s National Research Fund ( FNR ) under grant BRIDGES2020/IS/14711346/FUNTASY, and NSERC of Canada under the Discovery and CRC programs.","Clustering algorithms;  Errors;  Learning systems;  Risk analysis;  Risk assessment;  Safety engineering, Additional key word and phrasesdnn explanation;  Clusterings;  Deep neural network debugging;  Deep neural network functional safety analyse;  Functional Safety;  Key words;  Network debugging;  Root cause;  Safety analysis;  Transfer learning, Deep neural networks",cited By 2,Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Estimating worst-case execution time (WCET) is an important activity at early design stages of real-time systems. Based on WCET estimates, engineers make design and implementation decisions to ensure that task executions always complete before their specified deadlines. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines and, hence, operate safely with a probabilistic guarantee. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring probabilistic safe WCET ranges. We evaluated our approach by applying it to three industrial systems from different domains and several synthetic systems. Our approach efficiently and accurately estimates probabilistic safe WCET ranges within which deadlines are likely to be satisfied with a high degree of confidence. © 2023 Association for Computing Machinery.",machine learning;  meta-heuristic search;  Schedulability analysis;  search-based software engineering;  worst-case execution time,"Lee, J. and Shin, S.Y. and Nejati, S. and Briand, L. and Parache, Y.I.",,10.1145/3546941,Natural Sciences and Engineering Research Council of Canada,Canada,"This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant no. 694277), and NSERC of Canada under the Discovery and CRC programs.","Heuristic algorithms;  Interactive computer systems;  Machine learning;  Software engineering, Design stage;  Machine-learning;  Meta-heuristic search;  Probabilistics;  Real - Time system;  Schedulability analysis;  Search-based;  Search-based software engineering;  Time range;  Worst-case execution time, Real time systems",cited By 0,Estimating Probabilistic Safe WCET Ranges of Real-Time Systems at Design Stages,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. In this article, we perform a large-scale empirical study of Bash usage, based on analyses over one million open source Bash scripts found in Github repositories. We identify and discuss which features and utilities of Bash are most often used. Using static analysis, we find that Bash scripts are often error-prone, and the error-proneness has a moderately positive correlation with the size of the scripts. We also find that the most common problem areas concern quoting, resource management, command options, permissions, and error handling. We envision that these findings can be beneficial for learning Bash and future research that aims to improve shell and command-line productivity and reliability. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bash;  bugs;  code smells;  Empirical studies;  language features;  shell scripts,"Dong, Y. and Li, Z. and Tian, Y. and Sun, C. and Godfrey, M.W. and Nagappan, M.",,10.1145/3517193,Natural Sciences and Engineering Research Council of Canada,Canada,This work was funded by the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grant.,"Codes (symbols);  Computer operating systems;  Errors;  Odors;  Open source software;  Program debugging;  Semantics;  Shells (structures), Bash;  Bug;  Code smell;  Empirical studies;  GNU-Linux;  Language features;  Managing resources;  Scripting languages;  Shell command;  Shell script, Static analysis",cited By 1,"Bash in the Wild: Language Usage, Code Smells, and Bugs",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection;  curriculum learning;  Cyber-physical system;  deep learning;  digital twin,"Xu, Q. and Ali, S. and Yue, T.",,10.1145/3582571,Norges forskningsrad,Norway,,"Anomaly detection;  Curricula;  Cybersecurity;  Deep learning;  E-learning;  Embedded systems;  Learning systems, Anomaly detection;  Anomaly detection methods;  Curriculum learning;  Cybe-physical systems;  Cyber-physical systems;  Deep learning;  Human learning;  Learning models;  Learning process;  Real-time data, Cyber Physical System",cited By 4,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE's overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model. © 2023 Copyright held by the owner/author(s).",data provenance;  decompilation;  Machine learning,"Dramko, L. and Lacomis, J. and Yin, P. and Schwartz, E. and Allamanis, M. and Neubig, G. and Vasilescu, B. and Le Goues, C.",,10.1145/3546946,Nvidia,United States,This material is based upon work supported in part by the Software Engineering Institute and National Science Foundation (NSF awards CCF-1815287 and CCF-1910067). We also gratefully acknowledge hardware support (Quadro P6000 GPU) from the NVIDIA Corporation.,"Quality control, Data provenance;  Decompilation;  Decompilers;  Domain specific;  Executables;  ITS data;  Machine learning models;  Machine-learning;  Performance;  Training data, Machine learning",cited By 0,DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstraction refinement;  fault localization;  selective instrumentation;  Statistical debugging,"Zuo, Z. and Niu, X. and Zhang, S. and Fang, L. and Khoo, S.C. and Lu, S. and Sun, C. and Xu, G.H.",,10.1145/3544790,Office of Naval Research,United States,,"Abstracting;  Codes (symbols);  Cost benefit analysis;  Program debugging, Abstraction-refinement;  Analysis costs;  Automated debugging;  Fault localization;  Program code;  Program instrumentations;  Selective instrumentation;  Statistical debugging;  Statistical techniques;  Time-consuming tasks, Statistics",cited By 0,Toward More Efficient Statistical Debugging with Abstraction Refinement,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training;  Empirical study;  software engineering,"Liu, X. and Gu, D. and Chen, Z. and Wen, J. and Zhang, Z. and Ma, Y. and Wang, H. and Jin, X.",,10.1145/3597204,Peking University,China,,"Cell proliferation;  Deep learning;  Large dataset;  Learning systems;  Life cycle;  Program processors;  Software testing, Distributed training;  Empirical studies;  Engineering perspective;  Fault symptoms;  Large datasets;  Learning models;  Learning software;  Rich features;  Software life cycles;  Training process, Program debugging",cited By 0,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×-446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186-1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image classification models;  model compression;  Model dissemination;  neural networks,"Tian, Y. and Zhang, W. and Wen, M. and Cheung, S.-C. and Sun, C. and Ma, S. and Jiang, Y.",,10.1145/3583564,Research Grants Council of Hong Kong,Hong Kong (China),,"Black-box testing;  Classification (of information);  Deep neural networks;  Efficiency;  Iterative methods;  Markov processes;  Neural network models, Classification models;  Classification tasks;  Embedded device;  Image classification model;  Images classification;  Model compression;  Model dissemination;  Neural network model;  Neural-networks;  Original model, Image classification",cited By 1,Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer sequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET covers 3.4% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning testing;  library testing;  model diversity;  model generation,"Li, M. and Cao, J. and Tian, Y. and Li, T.O. and Wen, M. and Cheung, S.-C.",,10.1145/3583566,Research Grants Council of Hong Kong,Hong Kong (China),,"Deep learning;  Learning systems;  Libraries;  Parameter estimation, API calls;  Deep learning testing;  Input parameter;  Layer parameters;  Layer sequence;  Learning models;  Library testing;  Model diversity;  Model generation;  Test effectiveness, Quality assurance",cited By 2,COMET: Coverage-guided Model Generation For Deep Learning Library Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAbstract interpretation;  data structure analysis;  value-flow analysis,"Wang, C. and Wang, W. and Yao, P. and Shi, Q. and Zhou, J. and Xiao, X. and Zhang, C.",,10.1145/3565800,Research Grants Council of Hong Kong,Hong Kong (China),"The authors are supported by the RGC16206517, ITS/440/18FP and PRP/004/21FX grants from the Hong Kong Research Grant Council and the Innovation and Technology Commission, Ant Group, and the donations from Microsoft and Huawei. This work was finished when Qingkai Shi was with Ant Group. He is currently with Purdue University and is available via email at shi553@purdue.edu.","Data structures;  Flow graphs;  Graphic methods;  Value engineering, Additional key word and phrasesabstract interpretation;  Data structure analyse;  False positive;  Flow-graphs;  Key words;  Memory layout;  Structure analysis;  Ubiquitous data;  Value flow;  Value flow analysis, Containers",cited By 0,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,Research Grants Council of Hong Kong,Hong Kong (China),,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality. © 2023 Association for Computing Machinery.",code structure;  Code summary;  multi-head attention;  Transformer,"Gao, S. and Gao, C. and He, Y. and Zeng, J. and Nie, L. and Xia, X. and Lyu, M.",,10.1145/3522674,Research Grants Council of Hong Kong,Hong Kong (China),"This research was supported by the National Natural Science Foundation of China under grant no. 62002084, the Stable Support Plan for Colleges and Universities in Shenzhen under grant no. GXWD20201230155427003-20200730101839009, and the Research Grants Council of the Hong Kong Special Administrative Region, China (grant no. CUHK 14210920 of the General Research Fund). This research was also partly funded by the UK Engineering and Physical Sciences Research Council (grant no. EP/V048597/1, EP/T017112/1). Yulan He is supported by a Turing AI Fellowship funded by the UK Research and Innovation (grant no. EP/V020579/1).","Benchmarking;  Codes (symbols);  Data flow analysis;  Learning systems;  Syntactics, Code structure;  Code summary;  Learning techniques;  Multi-head attention;  Performance;  Sequence models;  Source codes;  Structure information;  Task domain;  Transformer, Deep learning",cited By 5,Code Structure-Guided Transformer for Source Code Summarization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool's test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest's effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases. © 2023 Association for Computing Machinery.",Android apps;  Android testing;  preference-wise testing,"Pan, M. and Lu, Y. and Pei, Y. and Zhang, T. and Li, X.",,10.1145/3511804,Research Grants Council of Hong Kong,Hong Kong (China),This research is supported by the Leading-edge Technology Program of Jiangsu Natural Science Foundation (No. BK20202001) and the National Natural Science Foundation of China (Nos. 61972193 and 62032010). This work is also supported in part by the Hong Kong RGC General Research Fund (GRF) PolyU 152002/18E.,"Automation;  Program debugging, Android apps;  Android testing;  Automatically generated;  Effective testing;  Experimental evaluation;  Mobile testing;  Preference-wise testing;  Test amplifications;  Test case;  Testing tools, Android (operating system)",cited By 2,Preference-wise Testing of Android Apps via Test Amplification,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,Science and Technology Project of State Grid,China,,"Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSecurity;  developers;  software engineering,"Lopez, T. and Sharp, H. and Bandara, A. and Tun, T. and Levine, M. and Nuseibeh, B.",,10.1145/3563211,Science Foundation Ireland,Ireland,"This work greatly benefited from the expertise and insight given by Charles Weir and Hannah Cooper and was supported by the UK NCSC, UKRI/EPSRC (EP/R013144/1, EP/T017465/1), and SFI (13/RC/2094_P2).","Human resource management;  Security of data, Additional key word and phrasessecurity;  Critical software;  Developer;  Ethnographic study;  Key words;  Secure software;  Security activities;  Security-critical;  Software developer, Software design",cited By 1,Security Responses in Software Development,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS's final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS's parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSoftware testing;  autonomous driving;  coverage criteria;  mutation analysis,"Laurent, T. and Klikovits, S. and Arcaini, P. and Ishikawa, F. and Ventresque, A.",,10.1145/3550270,Science Foundation Ireland,Ireland,,"Cost functions;  Software testing;  Uncertainty analysis, Additional key word and phrasessoftware testing;  Autonomous driving;  Coverage criteria;  Driving systems;  Key words;  Mutation analysis;  Parameterized;  Rules based systems;  Simulated environment;  Uncertainty, Autonomous vehicles",cited By 4,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with the same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future. © 2023 Copyright held by the owner/author(s).",Code comment;  comment generation;  comparative study;  inline comment;  method comment,"Huang, Y. and Guo, H. and Ding, X. and Shu, J. and Chen, X. and Luo, X. and Zheng, Z. and Zhou, X.",,10.1145/3582570,Shenzhen Research and Development Program,China,"This research is supported by the Guangdong Key Area R&D Program (2020B010164002), National Natural Science Foundation of China (61902441, 61902105, 62032025), Hong Kong RGC Project (No. PolyU15224121), Hong Kong ITF Project (GHP/052/19SZ), and the Research and Development Program of Shenzhen under Grants SGDX20190918101201696.","Codes (symbols), Automatic Generation;  Code comment;  Comment generation;  Comparatives studies;  Compare and analyze;  Inline comment;  Learning models;  Method comment;  Program comprehension;  Source codes, Deep learning",cited By 1,A Comparative Study on Method Comment and Inline Comment,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEthereum;  bookkeeping recognition;  smart contract;  token,"He, Z. and Song, S. and Bai, Y. and Luo, X. and Chen, T. and Zhang, W. and He, P. and Li, H. and Lin, X. and Zhang, X.",,10.1145/3560263,Shenzhen Research and Development Program,China,"This work is partially supported by Hong Kong ITF Project (Grant No. GHP/052/19SZ), the Research and Development Program of Shenzhen (Grant No. SGDX20190918101201696), Hong Kong RGC Projects (Grants No. PolyU15219319 and No. PolyU15224121), National Natural Science Foundation of China (Grants No. 61872057 and No. U19A2066), National Key R&D Program of China (Grant No. 2018YFB0804100), Natural Science Foundation of Sichuan Province (Grant No. 2022NSFSC0871).","Blockchain, Additional key word and phrasesethereum;  Behavior detection;  Block-chain;  Bookkeeping recognition;  Bytecodes;  Key words;  Malicious behavior;  Optimisations;  Token;  Type information, Smart contract",cited By 3,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection;  curriculum learning;  Cyber-physical system;  deep learning;  digital twin,"Xu, Q. and Ali, S. and Yue, T.",,10.1145/3582571,Singapore University of Technology and Design,Singapore,,"Anomaly detection;  Curricula;  Cybersecurity;  Deep learning;  E-learning;  Embedded systems;  Learning systems, Anomaly detection;  Anomaly detection methods;  Curriculum learning;  Cybe-physical systems;  Cyber-physical systems;  Deep learning;  Human learning;  Learning models;  Learning process;  Real-time data, Cyber Physical System",cited By 4,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance. © 2023 Association for Computing Machinery.",deep neural network;  highly configurable systems;  machine learning;  Software performance prediction,"Cheng, J. and Gao, C. and Zheng, Z.",,10.1145/3528100,Special Project for Research and Development in Key areas of Guangdong Province,China,,"Complex networks;  Forecasting;  Hierarchical systems, Configurable systems;  Configuration options;  Hierarchical interactions;  Highly configurable system;  Machine-learning;  Neural-networks;  Performance prediction;  Software performance;  Software performance prediction;  Systems performance, Deep neural networks",cited By 0,HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBinarized neural networks;  binary decision diagrams;  formal verification;  interpretability;  robustness,"Zhang, Y. and Zhao, Z. and Chen, G. and Song, F. and Chen, T.",,10.1145/3563212,State Key Laboratory of Novel Software Technology,China,"This work is supported by the National Key Research Program (2020AAA0107800); the National Natural Science Foundation of China (NSFC) under Grants No. 62072309 and No. 61872340; an overseas grant from the State Key Laboratory of Novel Software Technology, Nanjing University (KFKT2022A03); and the Birkbeck BEI School Project (EFFECT)","Binary decision diagrams;  Boolean functions;  Encoding (symbols);  Formal verification;  Knowledge representation;  Safety engineering;  Signal encoding, Additional key word and phrasesbinarized neural network;  Encodings;  Interpretability;  Key words;  Network-based;  Neural-networks;  Programming paradigms;  Property;  Quantitative verification;  Robustness, Deep learning",cited By 2,Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners' opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners' perception of the relevance of MCR research, and analyzed the primary studies' research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor- and support systems-related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research. © 2023 Copyright held by the owner/author(s).",literature survey;  Modern code review;  practitioner survey,"Badampudi, D. and Unterkalmsteiner, M. and Britto, R.",,10.1145/3585004,Stiftelsen foer Kunskaps- och Kompetensutveckling,Sweden,"We acknowledge that this work was supported by the Knowledge Foundation through the projects SERT – Software Engineering ReThought and OSIR Open-source inspired reuse (reference number 20190081) at Blekinge Institute of Technology, Sweden.","Code inspections;  Code review;  Literature survey;  Modern code review;  Practitioner surveys;  Research analysis;  Research communities;  Research impacts;  State of the art;  Systematic mapping studies, Codes (symbols)",cited By 1,Modern Code Reviews - Survey of Literature and Practice,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software bloat is code that is packaged in an application but is actually not necessary to run the application. The presence of software bloat is an issue for security, performance, and for maintenance. In this article, we introduce a novel technique for debloating, which we call coverage-based debloating. We implement the technique for one single language: Java bytecode. We leverage a combination of state-of-the-art Java bytecode coverage tools to precisely capture what parts of a project and its dependencies are used when running with a specific workload. Then, we automatically remove the parts that are not covered, in order to generate a debloated version of the project. We succeed to debloat 211 library versions from a dataset of 94 unique open-source Java libraries. The debloated versions are syntactically correct and preserve their original behaviour according to the workload. Our results indicate that 68.3% of the libraries' bytecode and 20.3% of their total dependencies can be removed through coverage-based debloating.For the first time in the literature on software debloating, we assess the utility of debloated libraries with respect to client applications that reuse them. We select 988 client projects that either have a direct reference to the debloated library in their source code or which test suite covers at least one class of the libraries that we debloat. Our results show that 81.5% of the clients, with at least one test that uses the library, successfully compile and pass their test suite when the original library is replaced by its debloated version. © 2023 Association for Computing Machinery.",bytecode;  code coverage;  program specialization;  Software bloat;  software maintenance,"Soto-Valero, C. and Durieux, T. and Harrand, N. and Baudry, B.",,10.1145/3546948,Stiftelsen foer Strategisk Forskning,Sweden,"This work is partially supported by the Wallenberg AI, Autonomous Systems, and Software Program (WASP) funded by Knut and Alice Wallenberg Foundation, as well as by the TrustFull and the Chains projects funded by the Swedish Foundation for Strategic Research.","Application programs;  Computer software maintenance;  Computer software reusability;  Java programming language;  Open source software, Bytecodes;  Code coverage;  Java byte codes;  Java library;  Novel techniques;  Open-source;  Program specialization;  Security performance;  Software bloat;  State of the art, Libraries",cited By 2,Coverage-Based Debloating for Java Bytecode,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGUI testing;  test automation;  test case robustness;  web element locators;  XPath locators,"Nass, M. and Alégroth, E. and Feldt, R. and Leotta, M. and Ricca, F.",,10.1145/3571855,Swedish Research Council,Sweden,"This work was supported by the KKS Foundation through the S.E.R.T. Research Profile project at Blekinge Institute of Technology. Robert Feldt has also been supported by the Swedish Scientific Council (No. 2015-04913, “Basing Software Testing on Information Theory,” and No. 2020-05272, “Automated boundary testing for QUality of AI/ML modelS”).","Automation;  Efficiency, Additional key word and phrasesgui testing;  Key words;  Localisation;  Robust tests;  Test Automation;  Test case;  Test case robustness;  Test scripts;  Web element locator;  Xpath locator, Websites",cited By 4,Similarity-based Web Element Localization for Robust Test Automation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning systems;  Test adequacy,"Kim, J. and Feldt, R. and Yoo, S.",,10.1145/3546947,Swedish Research Council,Sweden,,"Autonomous vehicles;  Large dataset;  Learning systems;  Medical imaging;  Safety engineering;  Software testing;  Statistical tests, Autonomous driving;  Coverage metrics;  Deep learning system;  Safety-critical domain;  Software testings;  System testing;  Test adequacies;  Test adequacy criteria;  Training data;  Training dataset, Deep neural networks",cited By 0,Evaluating Surprise Adequacy for Deep Learning System Testing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bias mitigation;  fairness-performance trade-off;  Machine Learning,"Chen, Z. and Zhang, J.M. and Sarro, F. and Harman, M.",,10.1145/3583561,UK Research and Innovation,United Kingdom,,"Economic and social effects, Bias mitigation;  Empirical studies;  Fairness performance;  Fairness-performance trade-off;  Learning classifiers;  Learning performance;  Machine-learning;  Mitigation methods;  Performance metrices;  Performance tradeoff, Machine learning",cited By 3,A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSecurity;  developers;  software engineering,"Lopez, T. and Sharp, H. and Bandara, A. and Tun, T. and Levine, M. and Nuseibeh, B.",,10.1145/3563211,UK Research and Innovation,United Kingdom,,"Human resource management;  Security of data, Additional key word and phrasessecurity;  Critical software;  Developer;  Ethnographic study;  Key words;  Secure software;  Security activities;  Security-critical;  Software developer, Software design",cited By 1,Security Responses in Software Development,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems. © 2023 Copyright held by the owner/author(s).",autonomous systems;  literature survey;  robotics;  testing;  Verification and validation,"Araujo, H. and Mousavi, M.R. and Varshosaz, M.",,10.1145/3542945,UK Research and Innovation,United Kingdom,"Hugo Araujo and Mohammad Reza Mousavi have been partially supported by the UKRI Trustworthy Autonomous Systems Node in Verifiability, Grant Award Reference EP/V026801/2.","Efficiency;  Industrial research;  Search engines;  Taxonomies;  Uncertainty analysis, Autonomous system;  Domain specific;  Literature survey;  Research gaps;  Search queries;  System behaviors;  Systematic literature review;  Systematic Review;  Validation and verification;  Verification-and-validation, Robotics",cited By 2,"Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality. © 2023 Association for Computing Machinery.",code structure;  Code summary;  multi-head attention;  Transformer,"Gao, S. and Gao, C. and He, Y. and Zeng, J. and Nie, L. and Xia, X. and Lyu, M.",,10.1145/3522674,UK Research and Innovation,United Kingdom,,"Benchmarking;  Codes (symbols);  Data flow analysis;  Learning systems;  Syntactics, Code structure;  Code summary;  Learning techniques;  Multi-head attention;  Performance;  Sequence models;  Source codes;  Structure information;  Task domain;  Transformer, Deep learning",cited By 5,Code Structure-Guided Transformer for Source Code Summarization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes. © 2023 Association for Computing Machinery.",industry;  machine learning;  Regression testing;  taxonomy;  test case prioritisation,"Ramírez, A. and Feldt, R. and Romero, J.R.",,10.1145/3511805,Universidad de Crdoba,Spain,,"Cost benefit analysis;  Software testing;  Transmission control protocol, Adverse effect;  Classifieds;  Industry needs;  Machine-learning;  Re-runs;  Regression testing;  Software company;  Systems under tests;  Test case;  Test case prioritization, Machine learning",cited By 2,"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice - Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",build selection;  Continuous Integration;  Software maintenance;  test selection,"Jin, X. and Servant, F.",,10.1145/3576038,Universidad Rey Juan Carlos,Spain,"This material was based upon work supported by the National Science Foundation under award CCF-2046403, and by Universidad Rey Juan Carlos under an International Distinguished Researcher award C01INVESDIST.","Computer software maintenance;  Cost benefit analysis;  Integration;  Safety engineering, Build selection;  Continuous integrations;  Cost saving;  Google+;  High costs;  High safety;  Integration systems;  Mozilla;  Selection techniques;  Test selection, Software testing",cited By 2,HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the ""High-level FPA""method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression , and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",early size estimation;  Function Point Analysis;  Function Points;  functional size measurement;  High-level FPA;  machine learning estimation;  NESMA Estimated;  Neural Networks;  Random Forests;  SFP;  SiFP;  simple function points;  Support Vector Regression,"Lavazza, L. and Locoro, A. and Liu, G. and Meli, R.",,10.1145/3582575,Universita degli Studi dell'Insubria,Italy,This work was partly supported by the “Fondo di Ricerca d’Ateneo” funded by the Università degli Studi dell’Insubria.,"Forestry;  Large dataset;  Logistic regression;  Machine learning;  Random forests, Early size estimation;  Function point;  Function point analysis;  Functional Size Measurements;  High-level function point analyse;  High-level functions;  Machine learning estimation;  Machine-learning;  NESMA estimated;  Neural-networks;  Random forests;  SFP;  Simple function point;  Simple++;  Size estimation;  Support vector regressions, Statistical tests",cited By 0,Estimating Software Functional Size via Machine Learning,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team's ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR). © 2023 Association for Computing Machinery.",API Recommendation;  Incident Response Plan;  Security Operation Center;  Security Orchestration;  security tool API;  SOAR,"Sworna, Z.T. and Islam, C. and Babar, M.A.",,10.1145/3512768,University of Adelaide,Australia,The work has been supported by the Cyber Security Research Centre Limited whose activities are partially funded by the Australian Government’s Cooperative Research Centres Programme. This work has also been supported with super-computing resources provided by the Phoenix HPC service at the University of Adelaide.,"Semantics, API recommendation;  Augmentation techniques;  Data availability;  Data heterogeneity;  Incident response plans;  Security operation center;  Security orchestration;  Security orchestration, automation, and response;  Security tool API;  Security tools, Automation",cited By 2,APIRO: A Framework for Automated Security Tools API Recommendation,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance. © 2023 Association for Computing Machinery.",category classification;  comment synchronization;  deep learning;  heuristic rules,"Yang, Z. and Keung, J.W. and Yu, X. and Xiao, Y. and Jin, Z. and Zhang, J.",,10.1145/3534117,University of Hong Kong,Hong Kong (China),,"Codes (symbols);  Electric circuit breakers;  Forecasting;  Learning systems;  Long short-term memory;  Semantics, Category Classification;  Code changes;  Comment synchronization;  Deep learning;  Edit distance;  Ensemble learning;  Heuristic rules;  Inconsistent samples;  Semantic modelling;  Synchronization performance, Synchronization",cited By 10,On the Significance of Category Prediction for Code-Comment Synchronization,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConcept location;  automated query reformulation;  bug localization;  Internet-scale code search;  machine learning;  query quality analysis;  systematic literature review;  term weighting,"Rahman, M.M. and Roy, C.K.",,10.1145/3607179,University of Saskatchewan,Canada,,"Program debugging;  Quality control;  Search engines, Additional key word and phrasesconcept location;  Automated query reformulation;  Bug localizations;  Code search;  Internet-scale code search;  Key words;  Machine-learning;  Query quality analyse;  Query reformulation;  Systematic literature review;  Term weighting, Machine learning",cited By 0,A Systematic Review of Automated Query Reformulations in Source Code Search,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. ""1-to-1""mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of ""1-to-n""(one binary function matches multiple source functions or binary functions) or even ""n-to-n""(multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the ""1-to-1""mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy. © 2023 Association for Computing Machinery.",1-to-1;  1-to-n;  Binary similarity analysis;  function inlining,"Jia, A. and Fan, M. and Jin, W. and Xu, X. and Zhou, Z. and Tang, Q. and Nie, S. and Wu, S. and Liu, T.",,10.1145/3561385,Xi'an Science and Technology Association,China,,"Computer software reusability;  Open systems;  Software testing;  Supply chains, 1-to-1;  1-to-n;  Binary files;  Binary functions;  Binary similarity analyse;  Function inlining;  Inlining;  Performance;  Similarity analysis;  Simulation strategies, Open source software",cited By 1,1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPatch detection;  malicious function input;  under constrained symbolic execution;  vulnerability detection,"Yang, S. and Xu, Z. and Xiao, Y. and Lang, Z. and Tang, W. and Liu, Y. and Shi, Z. and Li, H. and Sun, L.",,10.1145/3604608,Young Scientists Fund,China,,"Model checking;  Program compilers;  Semantics, Additional key word and phrasespatch detection;  Code similarities;  False positive rates;  Key words;  Malicious function input;  Similarity detection;  Symbolic execution;  Under constrained symbolic execution;  Under-constrained;  Vulnerability detection, Binary codes",cited By 0,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distributed training;  Empirical study;  software engineering,"Liu, X. and Gu, D. and Chen, Z. and Wen, J. and Zhang, Z. and Ma, Y. and Wang, H. and Jin, X.",,10.1145/3597204,Young Scientists Fund,China,,"Cell proliferation;  Deep learning;  Large dataset;  Learning systems;  Life cycle;  Program processors;  Software testing, Distributed training;  Empirical studies;  Engineering perspective;  Fault symptoms;  Large datasets;  Learning models;  Learning software;  Rich features;  Software life cycles;  Training process, Program debugging",cited By 0,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold start;  optional function elimination;  performance optimization;  Serverless computing,"Liu, X. and Wen, J. and Chen, Z. and Li, D. and Chen, J. and Liu, Y. and Wang, H. and Jin, X.",,10.1145/3585007,Young Scientists Fund,China,,"Loading, Application level;  Code-loading;  Cold-start;  General applications;  Latency optimizations;  Optional function elimination;  Performance optimizations;  Serverless computing;  Services applications;  Total response, High level languages",cited By 8,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",literature view;  Serverless computing,"Wen, J. and Chen, Z. and Jin, X. and Liu, X.",,10.1145/3579643,Young Scientists Fund,China,,"Computation theory;  Program debugging, Application logic;  Cloud-computing;  Computing paradigm;  Error prones;  Infrastructure managements;  Literature view;  Research efforts;  Serverless computing;  Software applications;  Systematic Review, Application programs",cited By 10,Rise of the Planet of Serverless Computing: A Systematic Review,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing - that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality - that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning;  fairness;  robustness;  testing,"Chen, J. and Wang, J. and Ma, X. and Sun, Y. and Sun, J. and Zhang, P. and Cheng, P.",,10.1145/3582573,Zhejiang University,China,,"Deep learning;  Image enhancement;  Quality assurance;  Well testing, Deep learning;  Engineering techniques;  Fairness;  Guided search;  Model properties;  Modeling quality;  Property;  Robustness;  Test case;  Testing framework, Learning systems",cited By 1,QuoTe: Quality-oriented Testing for Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"We provide artifacts to reproduce the evaluation results of our article: ""NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing"". The provided artifacts can be downloaded from https://zenodo.org/record/7134490. It includes 14 docker containers, several scripts for execution and analysis, one additional proof for the crash results, and six related documents for the running of experiments. We claim for all three badges, i.e., Available, Functional, and Reusable. This report gives instructions on how to reproduce the answers which mainly involve basic operations on the Ubuntu operating system. © 2023 Copyright held by the owner/author(s).",fuzzing;  Network service;  vulnerability discovery,"Hu, F. and Qin, S. and Ma, Z. and Zhao, B. and Yin, T. and Zhang, C.",,10.1145/3580599,,,,"Basic operation;  Evaluation results;  Fuzzing;  Networks services;  Vulnerability discovery, Network security",cited By 0,NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing - RCR Report,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database, to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",amount of change;  change impact analysis;  code churn;  defect fixing;  fixing effort;  Issue reports;  topics of issue reports,"Assi, M. and Hassan, S. and Georgiou, S. and Zou, Y.",,10.1145/3593802,,,,"Classification (of information);  Forecasting;  Open source software;  Open systems;  Semantics, Amount of change;  Change impact analysis;  Change impacts;  Code churn;  Defect fixing;  Fixing effort;  Issue report;  Mozilla;  Prediction modelling;  Topic of issue report, Defects",cited By 1,Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Maintaining a deep learning (DL) model by making the model substantially more robust through retraining with plenty of adversarial examples of non-trivial perturbation strength often reduces the model's standard accuracy. Many existing model repair or maintenance techniques sacrifice standard accuracy to produce a large gain in robustness or vice versa. This article proposes DeepPatch, a novel technique to maintain filter-intensive DL models. To the best of our knowledge, DeepPatch is the first work to address the challenge of standard accuracy retention while substantially improving the robustness of DL models with plenty of adversarial examples of non-trivial and diverse perturbation strengths. Rather than following the conventional wisdom to generalize all the components of a DL model over the union set of clean and adversarial samples, DeepPatch formulates a novel division of labor method to adaptively activate a subset of its inserted processing units to process individual samples. Its produced model can generate the original or replacement feature maps in each forward pass of the patched model, making the patched model carry an intrinsic property of behaving like the model under maintenance on demand. The overall experimental results show that DeepPatch successfully retains the standard accuracy of all pretrained models while improving the robustness accuracy substantially. However, the models produced by the peer techniques suffer from either large standard accuracy loss or small robustness improvement compared with the models under maintenance, rendering them unsuitable in general to replace the latter. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accuracy recovery;  maintenance;  Model testing,"Wei, Z. and Wang, H. and Ashraf, I. and Chan, W.-K.",,10.1145/3604609,,,This research is partly supported by CityU MFEXT (project no. 9678180).,"Deep learning;  Learning systems, Accuracy recovery;  Division of labor;  Learning models;  Model maintenance;  Model programs;  Model repair;  Model testing;  Non-trivial;  Novel techniques;  Perturbation strength, Repair",cited By 0,DeepPatch: Maintaining Deep Learning Model Programs to Retain Standard Accuracy with Substantial Robustness Improvement,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., ""Thumbs-up"", ""Laugh"", ""Hooray"", ""Heart"", ""Rocket"", ""Thumbs-down"", ""Confused"", and ""Eyes"". While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests' comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., ""Thumbs-up"", ""Hooray"", ""Heart"", ""Rocket"", and ""Laugh""). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the ""end-users""of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request's source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests' reactions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feedback;  GitHub reactions;  pull requests;  software collaboration,"Batoun, M.A. and Yung, K.L. and Tian, Y. and Sayagh, M.",,10.1145/3597208,,,,"Codes (symbols);  Open systems;  Rockets, Code review;  Empirical studies;  Engineering activities;  Github reaction;  Preliminary analysis;  Pull request;  Review process;  Software collaboration;  Software repositories;  Source code modification, Open source software",cited By 0,An Empirical Study on GitHub Pull Requests' Reactions,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Docker is a containerization technology that allows developers to ship software applications along with their dependencies in Docker images. Developers can extend existing images using them as base images when writing Dockerfiles. However, a lot of alternative functionally equivalent base images are available. Although many studies define and evaluate quality features that can be extracted from Docker artifacts, the criteria on which developers choose a base image over another remain unclear.In this article, we aim to fill this gap. First, we conduct a literature review through which we define a taxonomy of quality features, identifying two main groups: configuration-related features (i.e., mainly related to the Dockerfile and image build process), and externally observable features (i.e., what the Docker image users can observe). Second, we ran an empirical study considering the developers' preference for 2,441 Docker images in 1,911 open source software projects. We want to understand how the externally observable features influence the developers' preferences, and how they are related to the configuration-related features. Our results pave the way to the definition of a reliable quality measure for Docker artifacts, along with tools that support developers for a quality-aware development of them. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmpirical software engineering;  container virtualization;  Docker;  software maintenance,"Rosa, G. and Scalabrino, S. and Bavota, G. and Oliveto, R.",,10.1145/3603111,,,,"Application programs;  Computer software maintenance;  Open source software;  Open systems;  Virtualization, Additional key word and phrasesempirical software engineering;  Base images;  Container virtualization;  Docker;  Key words;  Literature reviews;  Quality aspects;  Quality features;  Software applications;  Virtualizations, Containers",cited By 0,What Quality Aspects Influence the Adoption of Docker Images?,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models' signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models' signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model's performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMachine learning;  curriculum learning;  data augmentation;  explainability;  neural networks;  reliability;  signal awareness,"Suneja, S. and Zhuang, Y. and Zheng, Y. and Laredo, J. and Morari, A. and Khurana, U.",,10.1145/3597202,,,,"Codes (symbols);  Complex networks;  Computer programming languages;  Learning systems;  Network architecture;  Network coding;  Neural networks;  Signal to noise ratio, Additional key word and phrasesmachine learning;  Curriculum learning;  Data augmentation;  Explainability;  Key words;  Model signals;  Neural-networks;  Signal awareness;  Source codes;  Task relevant, Curricula",cited By 0,Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Managing project dependencies is a key maintenance issue in software development. Developers need to choose an update strategy that allows them to receive important updates and fixes while protecting them from breaking changes. Semantic Versioning was proposed to address this dilemma, but many have opted for more restrictive or permissive alternatives. This empirical study explores the association between package characteristics and the dependency update strategy selected by its dependents to understand how developers select and change their update strategies. We study over 112,000 Node Package Manager (npm) packages and use 19 characteristics to build a prediction model that identifies the common dependency update strategy for each package. Our model achieves a minimum improvement of 72% over the baselines and is much better aligned with community decisions than the npm default strategy. We investigate how different package characteristics can influence the predicted update strategy and find that dependent count, age, and release status to be the highest influencing features. We complement the work with qualitative analyses of 160 packages to investigate the evolution of update strategies. While the common update strategy remains consistent for many packages, certain events such as the release of the 1.0.0 version or breaking changes influence the selected update strategy over time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDependency update strategy;  dependency management;  npm;  software ecosystems,"Javan Jafari, A. and Costa, D.E. and Shihab, E. and Abdalkareem, R.",,10.1145/3603110,,,,"Software design, Additional key word and phrasesdependency update strategy;  Breakings;  Dependency management;  Empirical studies;  Key words;  Managing programmes;  Node package manager;  Prediction modelling;  Software ecosystems;  Versioning, Semantics",cited By 0,Dependency Update Strategies and Package Characteristics,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The COVID-19 pandemic in 2020/2021/2022 and the resulting lockdowns forced many companies to switch to working from home, swiftly, on a large scale, and without preparation. This situation created unique challenges for software development, where individual software professionals had to shift instantly from working together at a physical venue to working remotely from home. Our research questions focus on the challenges of software professionals who work from home due to the COVID-19 pandemic, which we studied empirically at a German bank. We conducted a case study employing a mixed methods approach. We aimed to cover both the breadth of challenges via a quantitative survey, as well as a deeper understanding of these challenges via the follow-up qualitative analysis of 15 semi-structured interviews. In this article, we present the key impediments employees faced during the crisis, as well as their similarities and differences to the known challenges in distributed software development (DSD). We also analyze the employees' job satisfaction and how the identified challenges impact job satisfaction. In our study, we focus on challenges in communication, collaboration, tooling, and management. The findings of the study provide insights into this emerging topic of high industry relevance. At the same time, the study contributes to the existing academic research on work from home and on the COVID-19 pandemic aftermath. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",corona crisis;  coronavirus;  COVID-19;  Distributed software development;  DSD;  lockdown;  mixed methods;  open source;  remote work;  work from home,"Müller, K. and Koch, C. and Riehle, D. and Stops, M. and Harutyunyan, N.",,10.1145/3579636,,,,"Job satisfaction;  Locks (fasteners);  Open source software;  Open systems;  Software design, Corona crisis;  Coronaviruses;  Distributed software development;  Large-scales;  Lockdown;  Mixed method;  Open-source;  Remote work;  Work from home, Coronavirus;  COVID-19",cited By 1,Challenges of Working from Home in Software Development During Covid-19 Lockdowns,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.To support this claim, we offer a case study with 240 projects, where we find that the information in those projects ""clumps""towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this ""early bird""data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects.Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions.Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data-lite;  defects;  early;  Quality prediction,"Shrikanth, N.C. and Menzies, T.",,10.1145/3583565,,,This work was partially supported by an NSF-CISE grant #1908762.,"Birds;  Forecasting, Case-studies;  Complex methods;  Data-lite;  Early;  Historical data;  Project quality;  Quality prediction;  Quality prediction models;  Simple++;  Small region, Life cycle",cited By 2,Assessing the Early Bird Heuristic (for Predicting Project Quality),ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",license conflicts;  License management,"Wolter, T. and Barcomb, A. and Riehle, D. and Harutyunyan, N.",,10.1145/3571852,,,,"Codes (symbols);  Copyrights;  Open systems, Copyleft;  License conflict;  License management;  Open source license;  Open source projects;  Open-source;  Open-source code;  Open-source softwares, Open source software",cited By 1,Open Source License Inconsistencies on GitHub,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at https://github.com/WSU-SEAL/ToxiCR. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code review;  Natural Language Processing;  sentiment analysis;  tool development;  Toxicity,"Sarker, J. and Turzo, A.K. and Dong, M. and Bosu, A.",,10.1145/3583562,,,,"Codes (symbols);  Computer software selection and evaluation;  Large dataset;  Learning algorithms;  Open source software;  Open systems;  Software design;  Supervised learning;  Toxicity, Automated identification;  Code review;  Free and open source softwares;  Language processing;  Natural language processing;  Natural languages;  Pre-processing step;  Sentiment analysis;  Tool development;  Vectorization techniques, Sentiment analysis",cited By 2,Automated Identification of Toxic Code Reviews Using ToxiCR,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"This Replicating Computational Report (RCR) describes (a) our datAFLow fuzzer and (b) how to replicate the results in ""datAFLow: Toward a Data-Flow-Guided Fuzzer.""Our primary artifact is the datAFLow fuzzer. Unlike traditional coverage-guided greybox fuzzers - which use control-flow coverage to drive program exploration - datAFLow uses data-flow coverage to drive exploration. This is achieved through a set of LLVM-based analyses and transformations. In addition to datAFLow, we also provide a set of tools, scripts, and patches for (a) statically analyzing data flows in a target program, (b) compiling a target program with the datAFLow instrumentation, (c) evaluating datAFLow on the Magma benchmark suite, and (d) evaluating datAFLow on the DDFuzz dataset. datAFLow is available at https://github.com/HexHive/datAFLow. © 2023 Copyright held by the owner/author(s).",coverage;  data flow;  Fuzzing,"Herrera, A. and Payer, M. and Hosking, A.L.",,10.1145/3587159,,,,"C (programming language);  Data flow analysis;  Data transfer, Benchmark suites;  Control-flow;  Coverage;  Dataflow;  Fuzzing;  Grey-box, Digital storage",cited By 0,DatAFLow: Toward a Data-flow-guided Fuzzer,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Object-sensitive pointer analysis, which separates the calling contexts of a method by its receiver objects, is known to achieve highly useful precision for object-oriented languages such as Java. Despite recent advances, all object-sensitive pointer analysis algorithms still suffer from the scalability problem due to the combinatorial explosion of contexts in large programs. In this article, we introduce a new approach, Conch, that can be applied to debloat contexts for all object-sensitive pointer analysis algorithms, thereby improving significantly their efficiency while incurring a negligible loss of precision. Our key insight is to approximate a recently proposed set of two necessary conditions for an object in a program to be context-sensitive, i.e., context-dependent (whose precise verification is undecidable) with a set of three linearly verifiable conditions in terms of the number of edges in the pointer assignment graph (PAG) representation of the program. These three linearly verifiable conditions, which turn out to be almost always necessary in practice, are synthesized from three key observations regarding context-dependability for the objects created and used in real-world object-oriented programs. To develop a practical implementation for Conch, we introduce an IFDS-based algorithm for reasoning about object reachability in the PAG of a program, which runs linearly in terms of the number of edges in the PAG. By debloating contexts for three representative object-sensitive pointer analysis algorithms, which are applied to a set of representative Java programs, Conch can speed up these three baseline algorithms substantially at only a negligible loss of precision (less than 0.1%) with respect to several commonly used precision metrics. In addition, Conch also improves their scalability by enabling them to analyze substantially more programs to completion than before (under a time budget of 12 hours). Conch has been open-sourced (http://www.cse.unsw.edu.au/∼corg/tools/conch), opening up new opportunities for other researchers and practitioners to further improve this research. To demonstrate this, we introduce one extension of Conch to accelerate further the three baselines without losing any precision, providing further insights on extending Conch to make precision-efficiency tradeoffs in future research. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",context debloating;  IFDS;  object sensitivity;  Pointer analysis,"He, D. and Lu, J. and Xue, J.",,10.1145/3579641,,,,"Budget control;  Computer software;  Java programming language;  Object oriented programming;  Scalability, Analysis algorithms;  Calling contexts;  Combinatorial explosion;  Condition;  Context debloating;  IFDS;  Object sensitivity;  Object-oriented languages;  Pointer analysis;  Scalability problems, Efficiency",cited By 3,IFDS-based Context Debloating for Object-Sensitive Pointer Analysis,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based context for a program repair task. The results show that Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show that Katana fixes between 1.5 and 3.7 times more bugs than existing techniques. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contextual information;  deep learning;  graph neural networks;  program repair;  Program slicing,"Sintaha, M. and Nashid, N. and Mesbah, A.",,10.1145/3579640,,,,"Codes (symbols);  Deep neural networks;  Repair, Bug fixes;  Close proximity;  Contextual information;  Data dependencies;  Deep learning;  Graph neural networks;  Program repair;  Program slicing;  Repair techniques;  Software developer, Graph neural networks",cited By 0,Katana: Dual Slicing Based Context for Learning Bug Fixes,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Binary code similarity is the foundation of many security and software engineering applications. Recent works leverage deep neural networks (DNN) to learn a numeric vector representation (namely, embeddings) of assembly functions, enabling similarity analysis in the numeric space. However, existing DNN-based techniques capture syntactic-, control flow-, or data flow-level information of assembly code, which is too coarse-grained to represent program functionality. These methods can suffer from low robustness to challenging settings such as compiler optimizations and obfuscations.We present sem2vec, a binary code embedding framework that learns from semantics. Given the control-flow graph (CFG), 34 pages. of an assembly function, we divide it into tracelets, denoting continuous and short execution traces that are reachable from the function entry point. We use symbolic execution to extract symbolic constraints and other auxiliary information on each tracelet. We then train masked language models to compute embeddings of symbolic execution outputs. Last, we use graph neural networks, to aggregate tracelet embeddings into the CFG-level embedding for a function. Our evaluation shows that sem2vec extracts high-quality embedding and is robust against different compilers, optimizations, architectures, and popular obfuscation methods including virtualization obfuscation. We further augment a vulnerability search application with embeddings computed by sem2vec and demonstrate a significant improvement in vulnerability search accuracy. © 2023 Association for Computing Machinery.",binary code similarity;  embedding;  graph neural network;  Symbolic execution,"Wang, H. and Ma, P. and Wang, S. and Tang, Q. and Nie, S. and Wu, S.",,10.1145/3569933,,,This work was supported in part by CCF-Tencent Open Research Fund.,"Application programs;  Binary codes;  Data flow analysis;  Deep neural networks;  Flow graphs;  Graph neural networks;  Model checking;  Quality control;  Semantics;  Vector spaces, Assembly functions;  Binary code similarity;  Code similarities;  Compiler optimizations;  Control-flow graphs;  Embeddings;  Graph neural networks;  Learn+;  Semantic-aware;  Symbolic execution, Embeddings",cited By 0,sem2vec: Semantics-aware Assembly Tracelet Embedding,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions & Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teams' dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team members' experience, intuition, and understanding affects the team's overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber security-specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber security-specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, non-cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage. © 2023 Copyright held by the owner/author(s).",Cyber security decision-making;  cyber security risk analysis;  goal modelling,"Shreeve, B. and Gralha, C. and Rashid, A. and Araújo, J. and Goulão, M.",,10.1145/3548682,,,,"Decision making;  Decision theory;  Human resource management;  Risk analysis;  Risk assessment;  Risk management, Cybe security decision-making;  Cybe security risk analyse;  Cyber security;  Deep knowledge;  Goal models;  Security decision makings;  Security goals;  Security risk analysis, Cybersecurity",cited By 4,Making Sense of the Unknown: How Managers Make Cyber Security Decisions,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers. © 2023 Association for Computing Machinery.",bad smells;  empirical study;  Q&A forums;  Video games,"Nardone, V. and Muse, B. and Abidi, M. and Khomh, F. and Di Penta, M.",,10.1145/3563214,,,,"Commerce;  Economic and social effects;  Education computing;  Game design;  Human computer interaction;  Interactive computer graphics;  Software design, Bad smells;  Coding strategy;  Computer Science Education;  Covering problems;  Empirical studies;  Game design;  Q&A forum;  Software markets;  Video game development;  Video-games, Animation",cited By 2,Video Game Bad Smells: What They Are and How Developers Perceive Them,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206 (4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance. © 2023 Association for Computing Machinery.",dependency vulnerabilities;  Open source software;  software ecosystems;  software packages,"Alfadel, M. and Costa, D.E. and Shihab, E. and Adams, B.",,10.1145/3571848,,,,"Ecosystems;  Life cycle;  Open systems, Analysis approach;  Dependency vulnerability;  Empirical studies;  Exposed to;  Large-scales;  Open-source softwares;  Project maintenance;  Software ecosystems;  Software-systems, Open source software",cited By 0,On the Discoverability of npm Vulnerabilities in Node.js Projects,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Android is a highly fragmented platform with a diverse set of devices and users. To support the deployment of apps in such a heterogeneous setting, Android has introduced dynamic delivery - a new model of software deployment in which optional, device- or user-specific functionalities of an app, called Dynamic Feature Modules (DFMs), can be installed, as needed, after the app's initial installation. This model of app deployment, however, has exacerbated the challenges of properly testing Android apps. In this article, we first describe the results of an extensive study in which we formalized a defect model representing the various conditions under which DFM installations may fail. We then present DeltaDroid - a tool aimed at assisting the developers with validating dynamic delivery behavior in their apps by augmenting their existing test suite. Our experimental evaluation using real-world apps corroborates DeltaDroid's ability to detect many crashes and unexpected behaviors that the existing automated testing tools cannot reveal. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Android applications;  dynamic delivery;  Software testing;  test augmentation,"Ghorbani, N. and Jabbarvand, R. and Salehnamadi, N. and Garcia, J. and Malek, S.",,10.1145/3563213,,,,"Ability testing;  Android (operating system);  Application programs, Android applications;  Android apps;  Condition;  Defect model;  Dynamic delivery;  Dynamic features;  Module installation;  Software deployment;  Software testings;  Test augmentation, Software testing",cited By 1,DeltaDroid: Dynamic Delivery Testing in Android,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Producing secure software is challenging. The poor usability of security Application Programming Interfaces (APIs) makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries - rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers, offering 883 recommendations. Through thematic analysis, we identify seven core ways to improve usability of APIs. Most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand. There is less focus, however, on documentation, writing requirements, code quality assessment, and the impact of organizational software development practices. By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time. We find that very few API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAPI;  recommendations;  security;  SLR;  usability,"Patnaik, N. and Dwyer, A. and Hallett, J. and Rashid, A.",,10.1145/3561383,,,,"Application programming interfaces (API);  Codes (symbols);  Usability engineering, Additional key word and phrasesapi;  Applications programming interfaces;  Best practices;  Key words;  Recommendation;  Secure software;  Security;  Security application;  SLR;  Usability, Software design",cited By 0,SLR: From Saltzer and Schroeder to 2021...47 Years of Research on the Development and Validation of Security API Recommendations,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Scrum teams are at the heart of the Scrum framework. Nevertheless, an integrated and systemic theory that can explain what makes some Scrum teams more effective than others is still missing. To address this gap, we performed a 7-year-long mixed-methods investigation composed of two main phases. First, we induced a theoretical model from 13 exploratory field studies. Our model proposes that the effectiveness of Scrum teams depends on five high-level factors (responsiveness, stakeholder concern, continuous improvement, team autonomy, and management support) and 13 lower-level factors. In the second phase of our study, we validated our model with a covariance-based structural equation modeling analysis using data from about 5,000 developers and 2,000 Scrum teams that we gathered with a custom-built survey. Results suggest a very good fit of the empirical data in our theoretical model (CFI = 0.959, RMSEA = 0.038, SRMR = 0.035). Accordingly, this research allowed us to (1) propose and validate a generalizable theory for effective Scrum teams and (2) formulate clear recommendations for how organizations can better support Scrum teams. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAgile;  case studies;  Scrum;  structural equation modeling;  teams,"Verwijs, C. and Russo, D.",,10.1145/3571849,,,,"Additional key word and phrasesagile;  Case-studies;  Key words;  Mixed method;  Scra;  Still missing;  Structural equation models;  Team;  Team effectiveness;  Theoretical modeling, Human resource management",cited By 10,A Theory of Scrum Team Effectiveness,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In migrating and upgrading an Ethereum smart contract, it is necessary to transfer both the code as well as the stored data. Various methods attempt to migrate or upgrade a smart contract, but they are mostly manual, error-prone, and applicable only before deployment. Further, they have challenges in extracting the storage state of complex mapping data structures along with their keys. In this work, we present Smartmuv as an automatic source-code-based static analysis tool to analyze and extract the state from the storage-trie of smart contracts. Based on the abstract syntax tree and the control flow graphs of the Solidity source code, the tool analyzes each state variable including mapping types along the inheritance hierarchy. It also provides the upgrade algorithm that initializes the extracted state in the constructor of new smart contract. Smartmuv safely approximates the origin of the keys used in the mapping to extract values and has been able to extract the mapping state of 23,673 smart contracts with 95.7% overall precision. Moreover, we also validate the Smartmuv's extracted state with the third-party tool Etherscan. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesBlockchain;  compiler;  Smartmuv;  Solidity;  source code;  state extraction;  upgrade,"Ayub, M. and Saleem, T. and Janjua, M. and Ahmad, T.",,10.1145/3548683,,,,"Blockchain;  Codes (symbols);  Digital storage;  Ethereum;  Extraction;  Flow graphs;  Mapping;  Static analysis;  Trees (mathematics), Additional key word and phrasesblockchain;  Compiler;  Key words;  Smartmuv;  Solidity;  Source codes;  State analysis;  State extraction;  Storage state;  Upgrade, Smart contract",cited By 1,Storage State Analysis and Extraction of Ethereum Blockchain Smart Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as ÐApps. When engineering ÐApps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective ÐApps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., ÐApp developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that Etherscan's Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStation's Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, ÐApp developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTransaction processing time;  blockchain;  decentralized applications (DApps);  Ethereum,"Pacheco, M. and Oliva, G. and Rajbahadur, G.K. and Hassan, A.",,10.1145/3549542,,,,"Cost effectiveness;  Ethereum;  Gases;  Regression analysis, Accurate estimation;  Additional key word and phrasestransaction processing time;  Block-chain;  Decentralised;  Decentralized application;  Empirical studies;  Gas price;  Key words;  Processing time;  Transaction processing, Blockchain",cited By 2,Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm users' privacy.In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSmart contract;  backdoor detection;  datalog analysis;  directed fuzzing,"Ma, F. and Ren, M. and Ouyang, L. and Chen, Y. and Zhu, J. and Chen, T. and Zheng, Y. and Dai, X. and Jiang, Y. and Sun, J.",,10.1145/3560264,,,,"Ethereum;  Static analysis, Additional key word and phrasessmart contract;  Backdoor detections;  Backdoors;  Datalog;  Datalog analyse;  Decentralized networks;  Directed fuzzing;  False positive;  Key words;  Property loss, Smart contract",cited By 3,Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Critical decisions among design altern seventh atives with regards to maintainability arise early in the software design cycle. Existing comparison models relayed on the structural evolution of the used design patterns are suitable to support such decisions. However, their effectiveness on predicting maintenance effort is usually verified on a limited number of case studies under heterogeneous metrics. In this article, a multi-variable simulation model for validating the decision-making reliability of the derived formal comparison models for the significant designing problem of recursive hierarchies of part-whole aggregations, proposed in our prior work, is introduced. In the absence of a strict validation, the simulation model has been thoroughly calibrated concerning its decision-making precision based on empirical distributions from time-series analysis, approximating the highly uncertain nature of actual maintenance process. The decision reliability of the formal models has been statistically validated on a sample of 1,000 instances of design attributes representing the entire design space of the problem. Despite the limited accuracy of measurements, the results show that the models demonstrate an increasing reliability in a long-term perspective, even under assumptions of high variability. Thus, the modeling theory discussed in our prior work delivers reliable models that significantly reduce decision-risk and relevant maintenance cost. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesStatistical validation;  design pattern;  maintainability quality attribute requirement;  software evolution,"Karanikolas, C. and Dimitroulakos, G. and Masselos, K.",,10.1145/3569931,,,,"Decision theory;  Maintainability;  Quality control;  Software design;  Software reliability;  Time series analysis, Additional key word and phrasesstatistical validation;  Comparison models;  Decisions makings;  Design Patterns;  Early decision;  Key words;  Maintainability quality attribute requirement;  Quality attributes;  Simulation model;  Software Evolution, Decision making",cited By 0,Simulating Software Evolution to Evaluate the Reliability of Early Decision-making among Design Alternatives toward Maintainability,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Configurable software systems can be tuned for better performance. Leveraging on some Pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., ""the latency is less than 10s""while ""the memory usage is no more than 1GB"". Before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. For this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. However, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why.In this article, we seek to close this gap. Firstly, we do that through a review of over 426 articles in the literature and 14 real-world requirements datasets, from which we summarize four performance requirement patterns that quantify the aspirations in the configuration tuning. Drawing on these, we then conduct a comprehensive empirical study that covers 15 combinations of the state-of-the-art performance requirement patterns, four types of aspiration space, three Pareto optimizers, and eight real-world systems/environments, leading to 1,296 cases of investigation. Our findings reveal that (1) the realism of aspirations is the key factor that determines whether they should be used to guide the tuning; (2) the given patterns and the position of the realistic aspirations in the objective landscape are less important for the choice, but they do matter to the extents of improvement; (3) the available tuning budget can also influence the choice for unrealistic aspirations but it is insignificant under realistic ones. To promote open science practice, we make our code and dataset publicly available at: https://github.com/ideas-labo/aspiration-study. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSearch-Based Software Engineering;  multi-objective optimization;  performance aspiration;  performance requirement;  software configuration tuning,"Chen, T. and Li, M.",,10.1145/3571853,,,,"Budget control;  Decision making;  Pareto principle;  Software engineering, Additional key word and phrasessearch-based software engineering;  Key words;  Multi-objectives optimization;  Optimizers;  Performance;  Performance aspiration;  Performance objective;  Performance requirements;  Software configuration;  Software configuration tuning, Multiobjective optimization",cited By 5,Do Performance Aspirations Matter for Guiding Software Configuration Tuning? An Empirical Investigation under Dual Performance Objectives,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"This report describes the artifacts of the ""Dissecting American Fuzzy Lop - A FuzzBench Evaluation""paper. The artifacts are available online at https://github.com/eurecom-s3/dissecting_afl and archived at https://doi.org/10.6084/m9.figshare.21401280. American Fuzzy Lop (AFL) consists of the produced code, the setup to run the experiments in FuzzBench, and the generated reports. We claim the Functional badge as the patches to AFL are easy to enable and the experiments are easy to run thanks to the FuzzBench service, but the evaluations are self-contained and the modifications to AFL are as is. For the purpose of reproducing the experiments, no particular skills are needed as the process is straightforward and described in https://google.github.io/fuzzbench/getting-started/adding-a-new-fuzzer/#requesting-an-experiment. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",afl;  fuzzbench;  fuzzing,"Fioraldi, A. and Mantovani, A. and Maier, D. and Balzarotti, D.",,10.1145/3580600,,,,"Afl;  Fuzzbench;  Fuzzing, HTTP",cited By 0,Dissecting American Fuzzy Lop - A FuzzBench Evaluation - RCR Report,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"This artifact contains the source code and instructions to reproduce the evaluation results of the article ""Fuzzing Configurations of Program Options.""The source code includes the configuration grammars for six target programs, the scripts to generate configuration stubs, and the scripts to post-process fuzzing results. The README of the artifact includes the steps to prepare the experimental environment on a clean Ubuntu machine and step-by-step commands to reproduce the evaluation experiments. A VirtualBox image with ConfigFuzz properly set up is also included. © 2023 Copyright held by the owner/author(s).",command-line option configurations;  Fuzzing,"Zhang, Z. and Klees, G. and Wang, E. and Hicks, M. and Wei, S.",,10.1145/3580601,,,,Command line;  Command-line option configuration;  Configuration grammars;  Evaluation experiments;  Evaluation results;  Experimental environment;  Fuzzing;  Post process;  Source codes,cited By 1,Fuzzing Configurations of Program Options - RCR Report,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion. © 2023 Association for Computing Machinery.",data entry forms;  Form filling;  machine learning;  software data quality;  user interfaces,"Belgacem, H. and Li, X. and Bianculli, D. and Briand, L.",,10.1145/3533021,,,Financial support for this work was provided by the Alphonse Weicker Foundation.,"Bayesian networks;  Filling;  User profile, Automated approach;  Bayesian network models;  Data entry form;  Error prones;  Field dependencies;  Form filling;  Machine learning approaches;  Machine-learning;  Software data quality;  Software-systems, Machine learning",cited By 1,A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Programming language documentation refers to the set of technical documents that provide application developers with a description of the high-level concepts of a language (e.g., manuals, tutorials, and API references). Such documentation is essential to support application developers in effectively using a programming language. One of the challenges faced by documenters (i.e., personnel that design and produce documentation for a programming language) is to ensure that documentation has relevant information that aligns with the concrete needs of developers, defined as the missing knowledge that developers acquire via voluntary search. In this article, we present an automated approach to support documenters in evaluating the differences and similarities between the concrete information need of developers and the current state of documentation (a problem that we refer to as the topical alignment of a programming language documentation). Our approach leverages semi-supervised topic modelling that uses domain knowledge to guide the derivation of topics. We initially train a baseline topic model from a set of Rust-related Q&A posts. We then use this baseline model to determine the distribution of topic probabilities of each document of the official Rust documentation. Afterwards, we assess the similarities and differences between the topics of the Q&A posts and the official documentation. Our results show a relatively high level of topical alignment in Rust documentation. Still, information about specific topics is scarce in both the Q&A websites and the documentation, particularly related topics with programming niches such as network, game, and database development. For other topics (e.g., related topics with language features such as structs, patterns and matchings, and foreign function interface), information is only available on Q&A websites while lacking in the official documentation. Finally, we discuss implications for programming language documenters, particularly how to leverage our approach to prioritize topics that should be added to the documentation. © 2023 Association for Computing Machinery.",Documentation;  domain knowledge;  programming languages;  Q&A websites;  Rust;  RustForum;  Stack Overflow;  topic models,"Cogo, F.R. and Xia, X. and Hassan, A.E.",,10.1145/3546945,,,,"Alignment;  Application programming interfaces (API);  Concretes;  Domain Knowledge;  High level languages;  Probability distributions, Application developers;  Case-studies;  Documentation;  Domain knowledge;  Q&A website;  Rust;  Rustforum;  Stack overflow;  Technical documents;  Topic Modeling, Websites",cited By 0,Assessing the Alignment between the Information Needs of Developers and the Documentation of Programming Languages: A Case Study on Rust,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Refinement of formal system models towards implementation has been a mainstay of system development since the inception of formal and Correct by Construction approaches to system development. However, pure refinement approaches do not always deal fluently with all desirable system requirements. This prompted the development of alternatives and generalizations, such as retrenchment. The crucial concept of simulation is key to judging the quality of the conformance between abstract and more concrete system models. Reformulations of these theoretical approaches are reprised and are embedded in a graded framework. The added flexibility this offers is intended to deal more effectively with the needs of applications in which the relationship between different levels of abstraction is not straightforward, and in which behavior can oscillate between conforming quite closely to an idealized abstraction and deviating quite far from it. The framework developed is confronted with an intentionally demanding case study: a model active control system for the protection of buildings during earthquakes. This offers many challenges: it is hybrid/cyber-physical; it has to respond to rather unpredictable inputs; and it has to straddle the gap between continuous behavior and discretized/quantized/numerical implementation. © 2023 Association for Computing Machinery.",PhrasesRefinement;  retrenchment;  simulation,"Banach, R.",,10.1145/3534116,,,,"Concrete system;  Construction approaches;  Correct-by-construction;  Formal system models;  Generalisation;  Phrasesrefinement;  Retrenchment;  Simulation;  System development;  System requirements, Abstracting",cited By 1,"Graded Refinement, Retrenchment, and Simulation",ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In collaborative software development, programmers create software branches to add features and fix bugs tentatively, and then merge branches to integrate edits. When edits from different branches textually overlap (i.e., textual conflicts) or lead to compilation and runtime errors (i.e., build and test conflicts), it is challenging for developers to remove such conflicts. Prior work proposed tools to detect and solve conflicts. They investigate how conflicts relate to code smells and the software development process. However, many questions are still not fully investigated, such as what types of conflicts exist in real-world applications and how developers or tools handle them. For this article, we used automated textual merge, compilation, and testing to reveal three types of conflicts in 208 open-source repositories: textual conflicts, build conflicts (i.e., conflicts causing build errors), and test conflicts (i.e., conflicts triggering test failures). We manually inspected 538 conflicts and their resolutions to characterize merge conflicts from different angles. Our analysis revealed three interesting phenomena. First, higher-order conflicts (i.e., build and test conflicts) are harder to detect and resolve, while existing tools mainly focus on textual conflicts. Second, developers manually resolved most higher-order conflicts by applying similar edits to multiple program locations; their conflict resolutions share common editing patterns implying great opportunities for future tool design. Third, developers resolved 64% of true textual conflicts by keeping complete edits from either a left or right branch. Unlike prior studies, our research for the first time thoroughly characterizes three types of conflicts, with a special focus on higher-order conflicts and limitations of existing tool design. Our work will shed light on future research of software merge. © 2023 Association for Computing Machinery.",conflict detection;  conflict resolution;  Empirical;  software merge,"Shen, B. and Gulzar, M.A. and He, F. and Meng, N.",,10.1145/3546944,,,This work was supported by NSF-1845446.,"Java programming language;  Program debugging;  Software design, Characterization studies;  Collaborative software development;  Conflict detection;  Conflict Resolution;  Empirical;  High-order;  Higher-order;  Run-time errors;  Software merge;  Tool designs, Open source software",cited By 2,A Characterization Study of Merge Conflicts in Java Projects,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks. © 2023 Association for Computing Machinery.",code embeddings;  Machine learning;  neural network;  source code representation,"Ding, Z. and Li, H. and Shang, W. and Chen, T.-H.P.",,10.1145/3542944,,,,"Abstracting;  Application programs;  Convolution;  Deep learning;  Graphic methods;  Network coding;  Semantics;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Code embedding;  Convolutional networks;  Down-stream;  Embeddings;  Machine-learning;  Neural-networks;  Software engineering research;  Source code representations;  Source codes, Embeddings",cited By 1,Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge's ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account. © 2023 Association for Computing Machinery.",distributed software development;  merge conflict;  pull request;  Pull-based software development,"Maddila, C. and Upadrasta, S.S. and Bansal, C. and Nagappan, N. and Gousios, G. and Van Deursen, A.",,10.1145/3544791,,,,"Code review;  Collaborative software development;  Distributed software development;  Key parts;  Merge conflict;  MicroSoft;  Pull request;  Pull-based software development;  Resolution time;  Software codes, Software design",cited By 5,Nudge: Accelerating Overdue Pull Requests toward Completion,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution. © 2023 Association for Computing Machinery.",analysis;  consent;  consent revocation;  description logic;  evolution;  formal framework;  GDPR;  logs;  Privacy;  retroactivity;  verification,"Robol, M. and Breaux, T.D. and Paja, E. and Giorgini, P.",,10.1145/3490754,,,,"Consumer protection;  Data acquisition;  Data description;  Data privacy;  Scalability, Analyse;  Consent;  Consent revocation;  Description logic;  Evolution;  Formal framework;  General data protection regulations;  Log;  Privacy;  Retroactivity, Digital storage",cited By 1,Consent Verification Monitoring,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem's Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.This article questions such a ""weighted search first""belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new ""rule-of-thumb""for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at https://github.com/ideas-labo/pareto-vs-weight-for-sbse. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive systems;  configurable systems;  multi-objective optimization;  pareto optimization;  quality evaluation;  quality indicator;  Search-based software engineering;  self-adaptive systems;  user preference,"Chen, T. and Li, M.",,10.1145/3514233,,,,"Adaptive systems;  Budget control;  Pareto principle;  Software engineering, Configurable systems;  Engineering problems;  Multi-objectives optimization;  Pareto-optimization;  Quality evaluation;  Quality indicators;  Search-based;  Search-based software engineering;  Self-adaptive system;  User's preferences, Multiobjective optimization",cited By 7,The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects' maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSocio-technical factors;  mixed-methods research;  modern code review;  open-source software;  pull-based development;  social coding platforms,"Khatoonabadi, S. and Costa, D.E. and Abdalkareem, R. and Shihab, E.",,10.1145/3530785,,,,"Codes (symbols);  Economic and social effects;  Learning systems;  Open systems, Additional key word and phrasessocio-technical factor;  Code review;  Coding platform;  Key words;  Mixed-methods research;  Modern code review;  Open-source softwares;  Pull-based development;  Social coding platform;  Technical factors, Open source software",cited By 0,On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests-A Mixed-Methods Study of 10 Large Open-Source Projects,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesUnit testing;  formal concept analysis,"Wu, J. and Clause, J.",,10.1145/3533313,,,,"Additional key word and phrasesunit testing;  Automated identification;  Comprehension tasks;  Descriptive names;  Formal concepts analysis;  Generation techniques;  Human judgments;  Key words;  Prototype implementations, Formal concept analysis",cited By 0,Automated Identification of Uniqueness in JUnit Tests,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime. © 2023 Copyright held by the owner/author(s).",executable models;  formal techniques;  Internet of Things;  MAPE-K;  Self-adaptation;  statistical model checking,"Weyns, D. and Iftikhar, U.M.",,10.1145/3522585,,,,"Adaptive systems;  Feedback;  Formal methods;  Life cycle;  Model checking, Executable modeling;  Feedback loops;  Formal modeling;  Formal techniques;  Mape;  MAPE-K;  Runtimes;  Self- adaptations;  Self-adaptive system;  Statistical model checking, Internet of things",cited By 6,ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper, a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper, turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client's dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client's build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues. © 2023 Association for Computing Machinery.",Dependency management;  greenkeeper;  mining software repositories;  overhead;  software bots,"Rombaut, B. and Cogo, F.R. and Adams, B. and Hassan, A.E.",,10.1145/3522587,,,,"Botnet;  Errors;  Open systems;  Software design, Dependency management;  Falsealarms;  Greenkeeper;  Human intervention;  Mining software;  Mining software repository;  Open source software projects;  Overhead;  Software development process;  Software repositories, Open source software",cited By 1,There's no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"One can extend the features of a software system by installing a set of additional components called plugins. WordPress, as a typical example of such plugin-based software ecosystems, is used by millions of websites and has a large number (i.e., 54,777) of available plugins. These plugin-based software ecosystems are different from traditional ecosystems (e.g., NPM dependencies) in the sense that there is high coupling between a platform and its plugins compared to traditional ecosystems for which components might not necessarily depend on each other (e.g., NPM libraries do not depend on a specific version of NPM or a specific version of a client software system). The high coupling between a plugin and its platform and other plugins causes incompatibility issues that occur during the co-evolution of a plugin and its platform as well as other plugins. In fact, incompatibility issues represent a major challenge when upgrading WordPress or its plugins. According to our study of the top 500 most-released WordPress plugins, we observe that incompatibility issues represent the third major cause for bad releases, which are rapidly (within the next 24 hours) fixed via urgent releases. Thirty-two percent of these incompatibilities are between a plugin and WordPress while 19% are between peer plugins. In this article, we study how plugins co-evolve with the underlying platform as well as other plugins, in an effort to understand the practices that are related support such co-evolution and reduce incompatibility issues. In particular, we investigate how plugins support the latest available versions of WordPress, as well as how plugins are related to each other, and how they co-evolve. We observe that a plugin's support of new versions of WordPress with a large amount of code change is risky, as the releases that declare such support have a higher chance to be followed by an urgent release compared to ordinary releases. Although plugins support the latest WordPress version, plugin developers omit important changes such as deleting the use of removed WordPress APIs, which are removed a median of 873 days after the APIs have been removed from the source code of WordPress. Plugins introduce new releases that are made according to a median of five other plugins, which we refer to as peer-triggered releases. A median of 20% of the peer-triggered releases are urgent releases that fix problems in their previous releases. The most common goal of peer-triggered releases is the fixing of incompatibility issues that a plugin detects as late as after a median of 36 days since the last release of another plugin. Our work sheds light on the co-evolution of WordPress plugins with their platform as well as peer plugins in an effort to uncover the practices of plugin evolution, so WordPress can accordingly design approaches to avoid incompatibility issues. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPlugin-based ecosystems;  incompatibility issues;  plugins co-evolution,"Lin, J. and Sayagh, M. and Hassan, A.E.",,10.1145/3533700,,,,"Computer software;  Websites, Additional key word and phrasesplugin-based ecosystem;  Co-evolution;  Incompatibility issue;  Key words;  Plug-ins;  Plugin co-evolution;  Software ecosystems;  Software-systems;  Triggered release;  Wordpress, Ecosystems",cited By 0,The Co-evolution of the WordPress Platform and Its Plugins,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs. © 2023 Copyright held by the owner/author(s).",development process semantic network;  information integration;  knowledge graph construction;  Knowledge graphs,"Tamašauskaite, G. and Groth, P.",,10.1145/3522586,,,,"Air navigation;  Graphic methods;  Semantic Web;  Semantics, Development process;  Development process semantic network;  Graph construction;  Information integration;  Knowledge graph construction;  Knowledge graphs;  Process semantics;  Semantics networks;  Systematic Review, Knowledge graph",cited By 16,Defining a Knowledge Graph Development Process Through a Systematic Review,ACM Transactions on Software Engineering and Methodology,2023,TOSEM,journal
