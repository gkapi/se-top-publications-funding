ABSTRACT,AUTHORKEYWORDS,AUTHORS,BOOKTITLE,DOI,FUNDINGDETAILS,FUNDINGDETAILSCOUNTRY,FUNDINGTEXT,KEYWORDS,NOTE,TITLE,VENUE,YEAR,SOURCE,TYPE
"Background: Test smells indicate potential problems in the design and implementation of automated software tests that may negatively impact test code maintainability, coverage, and reliability. When poorly described, manual tests written in natural language may suffer from related problems, which enable their analysis from the point of view of test smells. Despite the possible prejudice to manually tested software products, little is known about test smells in manual tests, which results in many open questions regarding their types, frequency, and harm to tests written in natural language. Aims: Therefore, this study aims to contribute to a catalog of test smells for manual tests. Method: We perform a two-fold empirical strategy. First, an exploratory study in manual tests of three systems: the Ubuntu Operational System, the Brazilian Electronic Voting Machine, and the User Interface of a large smartphone manufacturer. We use our findings to propose a catalog of eight test smells and identification rules based on syntactical and morphological text analysis, validating our catalog with 24 in-company test engineers. Second, using our proposals, we create a tool based on Natural Language Processing (NLP) to analyze the subject systems' tests, validating the results. Results: We observed the occurrence of eight test smells. A survey of 24 in-company test professionals showed that 80.7% agreed with our catalog definitions and examples. Our NLP-based tool achieved a precision of 92%, recall of 95%, and f-measure of 93.5%, and its execution evidenced 13,169 occurrences of our cataloged test smells in the analyzed systems. Conclusion: We contribute with a catalog of natural language test smells and novel detection strategies that better explore the capabilities of current NLP mechanisms with promising results and reduced effort to analyze tests written in different idioms. © 2023 IEEE.",Manual Tests;  Natural Language Processing;  Software/Program Verification;  Test Design;  Test Smells,"Soares, E. and Aranda, M. and Oliveira, N. and Ribeiro, M. and Gheyi, R. and Souza, E. and MacHado, I. and Santos, A. and Fonseca, B. and Bonifacio, R.",,10.1109/ESEM56168.2023.10304800,Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior,Brazil,"We thank the Brazilian Superior Electoral Court (TSE) and our industrial partner for kindly allowing their tests to be analyzed in our study. This research was partially funded by CNPq grants 312195/2021-4, 421306/2018-1, 310313/2022-8; and FAPEAL grants 60030.0000000462/2020 and 60030.0000000161/2022. Also, this work is partially supported by INES (National Institute of Software Engineering): CNPq grant 465614/2014-0, CAPES grant 88887.136410/2017-00, and FACEPE grants APQ-0399-1.03/17 and PRONEX APQ/0388-1.03/14.","Natural language processing systems;  Software reliability;  User interfaces;  Verification, Design and implementations;  Language processing;  Manual tests;  Natural language processing;  Natural languages;  Potential problems;  Software/program verification;  Test code;  Test designs;  Test smell, Software testing",cited By 0,Manual Tests Do Smell! Cataloging and Identifying Natural Language Test Smells,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Test smells indicate potential problems in the design and implementation of automated software tests that may negatively impact test code maintainability, coverage, and reliability. When poorly described, manual tests written in natural language may suffer from related problems, which enable their analysis from the point of view of test smells. Despite the possible prejudice to manually tested software products, little is known about test smells in manual tests, which results in many open questions regarding their types, frequency, and harm to tests written in natural language. Aims: Therefore, this study aims to contribute to a catalog of test smells for manual tests. Method: We perform a two-fold empirical strategy. First, an exploratory study in manual tests of three systems: the Ubuntu Operational System, the Brazilian Electronic Voting Machine, and the User Interface of a large smartphone manufacturer. We use our findings to propose a catalog of eight test smells and identification rules based on syntactical and morphological text analysis, validating our catalog with 24 in-company test engineers. Second, using our proposals, we create a tool based on Natural Language Processing (NLP) to analyze the subject systems' tests, validating the results. Results: We observed the occurrence of eight test smells. A survey of 24 in-company test professionals showed that 80.7% agreed with our catalog definitions and examples. Our NLP-based tool achieved a precision of 92%, recall of 95%, and f-measure of 93.5%, and its execution evidenced 13,169 occurrences of our cataloged test smells in the analyzed systems. Conclusion: We contribute with a catalog of natural language test smells and novel detection strategies that better explore the capabilities of current NLP mechanisms with promising results and reduced effort to analyze tests written in different idioms. © 2023 IEEE.",Manual Tests;  Natural Language Processing;  Software/Program Verification;  Test Design;  Test Smells,"Soares, E. and Aranda, M. and Oliveira, N. and Ribeiro, M. and Gheyi, R. and Souza, E. and MacHado, I. and Santos, A. and Fonseca, B. and Bonifacio, R.",,10.1109/ESEM56168.2023.10304800,Fundacao de Amparo a Pesquisa do Estado de Alagoas,Brazil,,"Natural language processing systems;  Software reliability;  User interfaces;  Verification, Design and implementations;  Language processing;  Manual tests;  Natural language processing;  Natural languages;  Potential problems;  Software/program verification;  Test code;  Test designs;  Test smell, Software testing",cited By 0,Manual Tests Do Smell! Cataloging and Identifying Natural Language Test Smells,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Test smells indicate potential problems in the design and implementation of automated software tests that may negatively impact test code maintainability, coverage, and reliability. When poorly described, manual tests written in natural language may suffer from related problems, which enable their analysis from the point of view of test smells. Despite the possible prejudice to manually tested software products, little is known about test smells in manual tests, which results in many open questions regarding their types, frequency, and harm to tests written in natural language. Aims: Therefore, this study aims to contribute to a catalog of test smells for manual tests. Method: We perform a two-fold empirical strategy. First, an exploratory study in manual tests of three systems: the Ubuntu Operational System, the Brazilian Electronic Voting Machine, and the User Interface of a large smartphone manufacturer. We use our findings to propose a catalog of eight test smells and identification rules based on syntactical and morphological text analysis, validating our catalog with 24 in-company test engineers. Second, using our proposals, we create a tool based on Natural Language Processing (NLP) to analyze the subject systems' tests, validating the results. Results: We observed the occurrence of eight test smells. A survey of 24 in-company test professionals showed that 80.7% agreed with our catalog definitions and examples. Our NLP-based tool achieved a precision of 92%, recall of 95%, and f-measure of 93.5%, and its execution evidenced 13,169 occurrences of our cataloged test smells in the analyzed systems. Conclusion: We contribute with a catalog of natural language test smells and novel detection strategies that better explore the capabilities of current NLP mechanisms with promising results and reduced effort to analyze tests written in different idioms. © 2023 IEEE.",Manual Tests;  Natural Language Processing;  Software/Program Verification;  Test Design;  Test Smells,"Soares, E. and Aranda, M. and Oliveira, N. and Ribeiro, M. and Gheyi, R. and Souza, E. and MacHado, I. and Santos, A. and Fonseca, B. and Bonifacio, R.",,10.1109/ESEM56168.2023.10304800,Instituto Nacional de Ciencia e Tecnologia para Engenharia de Software,Brazil,,"Natural language processing systems;  Software reliability;  User interfaces;  Verification, Design and implementations;  Language processing;  Manual tests;  Natural language processing;  Natural languages;  Potential problems;  Software/program verification;  Test code;  Test designs;  Test smell, Software testing",cited By 0,Manual Tests Do Smell! Cataloging and Identifying Natural Language Test Smells,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Test smells indicate potential problems in the design and implementation of automated software tests that may negatively impact test code maintainability, coverage, and reliability. When poorly described, manual tests written in natural language may suffer from related problems, which enable their analysis from the point of view of test smells. Despite the possible prejudice to manually tested software products, little is known about test smells in manual tests, which results in many open questions regarding their types, frequency, and harm to tests written in natural language. Aims: Therefore, this study aims to contribute to a catalog of test smells for manual tests. Method: We perform a two-fold empirical strategy. First, an exploratory study in manual tests of three systems: the Ubuntu Operational System, the Brazilian Electronic Voting Machine, and the User Interface of a large smartphone manufacturer. We use our findings to propose a catalog of eight test smells and identification rules based on syntactical and morphological text analysis, validating our catalog with 24 in-company test engineers. Second, using our proposals, we create a tool based on Natural Language Processing (NLP) to analyze the subject systems' tests, validating the results. Results: We observed the occurrence of eight test smells. A survey of 24 in-company test professionals showed that 80.7% agreed with our catalog definitions and examples. Our NLP-based tool achieved a precision of 92%, recall of 95%, and f-measure of 93.5%, and its execution evidenced 13,169 occurrences of our cataloged test smells in the analyzed systems. Conclusion: We contribute with a catalog of natural language test smells and novel detection strategies that better explore the capabilities of current NLP mechanisms with promising results and reduced effort to analyze tests written in different idioms. © 2023 IEEE.",Manual Tests;  Natural Language Processing;  Software/Program Verification;  Test Design;  Test Smells,"Soares, E. and Aranda, M. and Oliveira, N. and Ribeiro, M. and Gheyi, R. and Souza, E. and MacHado, I. and Santos, A. and Fonseca, B. and Bonifacio, R.",,10.1109/ESEM56168.2023.10304800,Conselho Nacional de Desenvolvimento Cientifico e Tecnologico,Brazil,,"Natural language processing systems;  Software reliability;  User interfaces;  Verification, Design and implementations;  Language processing;  Manual tests;  Natural language processing;  Natural languages;  Potential problems;  Software/program verification;  Test code;  Test designs;  Test smell, Software testing",cited By 0,Manual Tests Do Smell! Cataloging and Identifying Natural Language Test Smells,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Test smells indicate potential problems in the design and implementation of automated software tests that may negatively impact test code maintainability, coverage, and reliability. When poorly described, manual tests written in natural language may suffer from related problems, which enable their analysis from the point of view of test smells. Despite the possible prejudice to manually tested software products, little is known about test smells in manual tests, which results in many open questions regarding their types, frequency, and harm to tests written in natural language. Aims: Therefore, this study aims to contribute to a catalog of test smells for manual tests. Method: We perform a two-fold empirical strategy. First, an exploratory study in manual tests of three systems: the Ubuntu Operational System, the Brazilian Electronic Voting Machine, and the User Interface of a large smartphone manufacturer. We use our findings to propose a catalog of eight test smells and identification rules based on syntactical and morphological text analysis, validating our catalog with 24 in-company test engineers. Second, using our proposals, we create a tool based on Natural Language Processing (NLP) to analyze the subject systems' tests, validating the results. Results: We observed the occurrence of eight test smells. A survey of 24 in-company test professionals showed that 80.7% agreed with our catalog definitions and examples. Our NLP-based tool achieved a precision of 92%, recall of 95%, and f-measure of 93.5%, and its execution evidenced 13,169 occurrences of our cataloged test smells in the analyzed systems. Conclusion: We contribute with a catalog of natural language test smells and novel detection strategies that better explore the capabilities of current NLP mechanisms with promising results and reduced effort to analyze tests written in different idioms. © 2023 IEEE.",Manual Tests;  Natural Language Processing;  Software/Program Verification;  Test Design;  Test Smells,"Soares, E. and Aranda, M. and Oliveira, N. and Ribeiro, M. and Gheyi, R. and Souza, E. and MacHado, I. and Santos, A. and Fonseca, B. and Bonifacio, R.",,10.1109/ESEM56168.2023.10304800,Fundacao de Amparo a Ciencia e Tecnologia do Estado de Pernambuco,Brazil,,"Natural language processing systems;  Software reliability;  User interfaces;  Verification, Design and implementations;  Language processing;  Manual tests;  Natural language processing;  Natural languages;  Potential problems;  Software/program verification;  Test code;  Test designs;  Test smell, Software testing",cited By 0,Manual Tests Do Smell! Cataloging and Identifying Natural Language Test Smells,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Bug dependencies refer to the link relationships between bugs and related issues, which are commonly observed in software evolution. It has been found that bugs with bug dependencies often take longer time to be resolved than other bugs without any dependencies. Despite the potential impact of bug dependencies on bug-fixing time, previous studies use traditional metrics without considering bug dependencies to build bug-fixing time prediction models. As a result, there is currently little empirical evidence to support the use of bug dependencies in improving prediction accuracy. Aims: We aim to conduct a comprehensive empirical study to investigate the value of considering bug dependencies for bug-fixing time prediction. Method: We define a set of bug dependency metrics based on bug dependencies. We first investigate the correlation between bug dependency metrics and bug-fixing time to investigate whether bugs with more complex dependencies are more time-consuming to be fixed. Next, we employ principal component analysis to study whether bug dependency metrics capture additional dimensions of a bug compared to traditional metrics. Finally, we build multivariate prediction models to explore whether considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Results: The experimental results suggest that: (1) bugs with more complex dependencies require more time to be fixed; (2) bug dependency metrics are complementary to traditional metrics; (3) considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Conclusions: These findings highlight the importance of considering bug dependencies in bug-fixing time prediction, and provide valuable insights into the potential impact of bug dependencies on software development processes. © 2023 IEEE.",bug dependency;  bug fixing;  bug-fixing time prediction;  network analysis,"Li, C. and Zhao, Y. and Yang, Y. and Zhou, Y. and Nie, L. and Ding, Z.",,10.1109/ESEM56168.2023.10304804,Natural Science Foundation of Zhejiang Province,China,"ACKNOWLEDGEMENT This work was supported by the National Nature Science Foundation of China (Grant No. 62132014, 62072194 and 62172205), and Zhejiang Provincial Key Research and Development Program of China (No.2022C01045) and Zhejiang Provincial Natural Science Foundation of China(No.LQ23F020020).","Complex networks;  Forecasting;  Program debugging;  Software design, Bug dependency;  Bug-fixing;  Bug-fixing time prediction;  Link relationships;  Potential impacts;  Prediction accuracy;  Prediction modelling;  Software Evolution;  Time predictions, Principal component analysis",cited By 0,Investigating the Impact of Bug Dependencies on Bug-Fixing Time Prediction,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Bug dependencies refer to the link relationships between bugs and related issues, which are commonly observed in software evolution. It has been found that bugs with bug dependencies often take longer time to be resolved than other bugs without any dependencies. Despite the potential impact of bug dependencies on bug-fixing time, previous studies use traditional metrics without considering bug dependencies to build bug-fixing time prediction models. As a result, there is currently little empirical evidence to support the use of bug dependencies in improving prediction accuracy. Aims: We aim to conduct a comprehensive empirical study to investigate the value of considering bug dependencies for bug-fixing time prediction. Method: We define a set of bug dependency metrics based on bug dependencies. We first investigate the correlation between bug dependency metrics and bug-fixing time to investigate whether bugs with more complex dependencies are more time-consuming to be fixed. Next, we employ principal component analysis to study whether bug dependency metrics capture additional dimensions of a bug compared to traditional metrics. Finally, we build multivariate prediction models to explore whether considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Results: The experimental results suggest that: (1) bugs with more complex dependencies require more time to be fixed; (2) bug dependency metrics are complementary to traditional metrics; (3) considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Conclusions: These findings highlight the importance of considering bug dependencies in bug-fixing time prediction, and provide valuable insights into the potential impact of bug dependencies on software development processes. © 2023 IEEE.",bug dependency;  bug fixing;  bug-fixing time prediction;  network analysis,"Li, C. and Zhao, Y. and Yang, Y. and Zhou, Y. and Nie, L. and Ding, Z.",,10.1109/ESEM56168.2023.10304804,Key Research and Development Program of Zhejiang Province,China,,"Complex networks;  Forecasting;  Program debugging;  Software design, Bug dependency;  Bug-fixing;  Bug-fixing time prediction;  Link relationships;  Potential impacts;  Prediction accuracy;  Prediction modelling;  Software Evolution;  Time predictions, Principal component analysis",cited By 0,Investigating the Impact of Bug Dependencies on Bug-Fixing Time Prediction,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Bug dependencies refer to the link relationships between bugs and related issues, which are commonly observed in software evolution. It has been found that bugs with bug dependencies often take longer time to be resolved than other bugs without any dependencies. Despite the potential impact of bug dependencies on bug-fixing time, previous studies use traditional metrics without considering bug dependencies to build bug-fixing time prediction models. As a result, there is currently little empirical evidence to support the use of bug dependencies in improving prediction accuracy. Aims: We aim to conduct a comprehensive empirical study to investigate the value of considering bug dependencies for bug-fixing time prediction. Method: We define a set of bug dependency metrics based on bug dependencies. We first investigate the correlation between bug dependency metrics and bug-fixing time to investigate whether bugs with more complex dependencies are more time-consuming to be fixed. Next, we employ principal component analysis to study whether bug dependency metrics capture additional dimensions of a bug compared to traditional metrics. Finally, we build multivariate prediction models to explore whether considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Results: The experimental results suggest that: (1) bugs with more complex dependencies require more time to be fixed; (2) bug dependency metrics are complementary to traditional metrics; (3) considering bug dependencies can improve the effectiveness of bug-fixing time prediction. Conclusions: These findings highlight the importance of considering bug dependencies in bug-fixing time prediction, and provide valuable insights into the potential impact of bug dependencies on software development processes. © 2023 IEEE.",bug dependency;  bug fixing;  bug-fixing time prediction;  network analysis,"Li, C. and Zhao, Y. and Yang, Y. and Zhou, Y. and Nie, L. and Ding, Z.",,10.1109/ESEM56168.2023.10304804,National Natural Science Foundation of China,China,,"Complex networks;  Forecasting;  Program debugging;  Software design, Bug dependency;  Bug-fixing;  Bug-fixing time prediction;  Link relationships;  Potential impacts;  Prediction accuracy;  Prediction modelling;  Software Evolution;  Time predictions, Principal component analysis",cited By 0,Investigating the Impact of Bug Dependencies on Bug-Fixing Time Prediction,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: According to GitGuardian's monitoring of public GitHub repositories, secrets sprawl continued accelerating in 2022 by 67% compared to 2021, exposing over 10 million secrets (API keys and other credentials). Though many open-source and proprietary secret detection tools are available, these tools output many false positives, making it difficult for developers to take action and teams to choose one tool out of many. To our knowledge, the secret detection tools are not yet compared and evaluated. Aims: The goal of our study is to aid developers in choosing a secret detection tool to reduce the exposure of secrets through an empirical investigation of existing secret detection tools. Method: We present an evaluation of five open-source and four proprietary tools against a benchmark dataset. Results: The top three tools based on precision are: GitHub Secret Scanner (75%), Gitleaks (46%), and Commercial X (25%), and based on recall are: Gitleaks (88%), SpectralOps (67%) and TruffleHog (52%). Our manual analysis of reported secrets reveals that false positives are due to employing generic regular expressions and ineffective entropy calculation. In contrast, false negatives are due to faulty regular expressions, skipping specific file types, and insufficient rulesets. Conclusions: We recommend developers choose tools based on secret types present in their projects to prevent missing secrets. In addition, we recommend tool vendors update detection rules periodically and correctly employ secret verification mechanisms by collaborating with API vendors to improve accuracy. © 2023 IEEE.",,"Basak, S.K. and Cox, J. and Reaves, B. and Williams, L.",,10.1109/ESEM56168.2023.10304853,National Science Foundation,United States,This work was supported by the National Science Foundation (NSF) 2055554 grant. The authors would also like to thank the Realsearch research group for their valuable input on this paper.,"Inspection equipment;  Pattern matching, Benchmark datasets;  Comparatives studies;  Detection tools;  Empirical investigation;  False negatives;  False positive;  File types;  Manual analysis;  Open-source;  Regular expressions, Open source software",cited By 0,A Comparative Study of Software Secrets Reporting by Secret Detection Tools,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers. © 2023 IEEE.",machine learning;  regression testing;  software testing;  test flakiness,"Berndt, A. and Nochta, Z. and Bach, T.",,10.1109/ESEM56168.2023.10304860,,,,"Computer programming languages;  Machine learning, Data set;  F1 scores;  Large-scales;  Machine-learning;  Regression testing;  Root cause;  Software testings;  Source codes;  Test code;  Test flakiness, Software testing",cited By 0,The Vocabulary of Flaky Tests in the Context of SAP HANA,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Along with the development of large language models (LLMs), e.g., ChatGPT, many existing approaches and tools for software security are changing. It is, therefore, essential to understand how security-aware these models are and how these models impact software security practices and education. In exercises of a software security course at our university, we ask students to identify and fix vulnerabilities we insert in a web application using state-of-the-art tools. After ChatGPT, especially the GPT-4 version of the model, we want to know how the students can possibly use ChatGPT to complete the exercise tasks. We input the vulnerable code to ChatGPT and measure its accuracy in vulnerability identification and fixing. In addition, we investigated whether ChatGPT can provide a proper source of information to support its outputs. Results show that ChatGPT can identify 20 of the 28 vulnerabilities we inserted in the web application in a white-box setting, reported three false positives, and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makes nine satisfactory penetration testing and fixing recommendations for the ten vulnerabilities we want students to fix and can often point to related sources of information. © 2023 IEEE.",artificial intelligence;  ChatGPT;  IT education;  large language models;  Software security,"Li, J. and Meland, P.H. and Notland, J.S. and Storhaug, A. and Tysse, J.H.",,10.1109/ESEM56168.2023.10304857,European Commission,EU,ACKNOWLEDGMENT The research conducted in this paper has partly been related to the CyberSecPro project under the European Union’s Digital Europe Programme (DEP) (grant agreement No 101083594).,"Application programs;  Computational linguistics;  Curricula;  Education computing;  Engineering education;  Technology transfer, ChatGPT;  IT-education;  Language model;  Large language model;  Security Practice;  Security-aware;  Software security;  Sources of informations;  WEB application;  Web applications, Students",cited By 0,Evaluating the Impact of ChatGPT on Exercises of a Software Security Course,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context. Algorithmic racism is the term used to describe the behavior of technological solutions that constrains users based on their ethnicity. Lately, various data-driven software systems have been reported to discriminate against Black people, either for the use of biased data sets or due to the prejudice propagated by software professionals in their code. As a result, Black people are experiencing disadvantages in accessing technology-based services, such as housing, banking, and law enforcement. Goal. This study aims to explore algorithmic racism from the perspective of software professionals. Method. A survey questionnaire was applied to explore the understanding of software practitioners on algorithmic racism, and data analysis was conducted using descriptive statistics and coding techniques. Results. We obtained answers from a sample of 73 software professionals discussing their understanding and perspectives on algorithmic racism in software development. Our results demonstrate that the effects of algorithmic racism are well-known among practitioners. However, there is no consensus on how the problem can be effectively addressed in software engineering. In this paper, some solutions to the problem are proposed based on the professionals' narratives. Conclusion. Combining technical and social strategies, including training on structural racism for software professionals, is the most promising way to address the algorithmic racism problem and its effects on the software solutions delivered to our society. © 2023 IEEE.",EDI;  racism;  software development,"De Souza Santos, R. and De Lima, L.F. and Magalhaes, C.",,10.1109/ESEM56168.2023.10304856,,,,"Computer software, Algorithmics;  Coding techniques;  Data driven;  Data set;  Descriptive statistics;  Racism;  Software practitioners;  Software-systems;  Technological solution;  Technology-based, Software design",cited By 0,The Perspective of Software Professionals on Algorithmic Racism,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background. Infrastructure-as-Code (IaC) is an emerging practice to manage cloud infrastructure resources for software systems. Modern software development has evolved to embrace IaC as a best practice for consistently provisioning and managing infrastructure using various tools such as Terraform and Ansible. However, recent studies highlighted that developers still encounter various challenges with IaC tools. Aims. We aim in this paper to understand the different challenges that developers encounter with IaC and analyze the trend of seeking assistance on Q&A platforms in the context of IaC. To this end, we conduct a large-scale empirical study investigating developers' discussions in Stack Overflow. Method. We first collect IaC-relevant tags on Stack Overflow, constituting a dataset that comprises 52,692 questions and 64,078 answers. Then, we group questions into specific topics using the Latent Dirichlet Allocation (LDA) method, which we optimize using a Genetic Algorithm (GA) for parameter's fine-tuning. Finally, to gain better insights, we analyze the identified topics based on different criteria such as popularity and difficulty. Results. Our findings reveal an average yearly increase of 150% in terms of IaC-related questions and 135% in terms of users between 2011 and 2022. Furthermore, we observe that IaC questions revolve around seven main topics: server configuration, policy configuration, networking, deployment pipelines, variable management, templating, and file management. Notably, we found that server configuration and file management are the most popular topics, i.e., the most discussed among IaC developers, while the deployment pipelines and templating topics are the most difficult. Conclusions. Our results shed light on IaC challenges that are often encountered by developers on popular Q&A platforms. These findings reveal important implications for practitioners seeking better support for IaC tools in real-world settings and for researchers to better understand the IaC community needs and further investigate IaC in different aspects. © 2023 IEEE.",Developers Discussions;  Infrastructure-as-Code (IaC);  Stack Overflow;  Topic Modeling,"Begoug, M. and Bessghaier, N. and Ouni, A. and Alomar, E.A. and Mkaouer, M.W.",,10.1109/ESEM56168.2023.10304847,Natural Sciences and Engineering Research Council of Canada,Canada,This work is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) RGPIN-2018-05960.,"Codes (symbols);  Pipelines;  Software design;  Statistics, Cloud infrastructures;  Developer discussion;  Empirical studies;  File management;  Infrastructure resources;  Infrastructure-as-code;  Software-systems;  Stack overflow;  Templating;  Topic Modeling, Genetic algorithms",cited By 0,What Do Infrastructure-as-Code Practitioners Discuss: An Empirical Study on Stack Overflow,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"In recent years, software engineers have explored ways to assist quantum software programmers. Our goal in this paper is to continue this exploration and see if quantum software programmers deal with some problems plaguing classical programs. Specifically, we examine whether intermittently failing tests, i.e., flaky tests, affect quantum software development. To explore flakiness, we conduct a preliminary analysis of 14 quantum software repositories. Then, we identify flaky tests and categorize their causes and methods of fixing them. We find flaky tests in 12 out of 14 quantum software repositories. In these 12 repositories, the lower boundary of the percentage of issues related to flaky tests ranges between 0.26% and 1.85% per repository. We identify 46 distinct flaky test reports with 8 groups of causes and 7 common solutions. Further, we notice that quantum programmers are not using some of the recent flaky test countermeasures developed by software engineers. This work may interest practitioners, as it provides useful insight into the resolution of flaky tests in quantum programs. Researchers may also find the paper helpful as it offers quantitative data on flaky tests in quantum software and points to new research opportunities. © 2023 IEEE.",flaky tests;  quantum software engineering;  software testing,"Zhang, L. and Radnejad, M. and Miranskyy, A.",,10.1109/ESEM56168.2023.10304850,,,,"Software design, Flaky test;  Lower boundary;  Preliminary analysis;  Quantitative data;  Quantum software engineering;  Research opportunities;  Software repositories;  Software testings;  Test range;  Test reports, Software testing",cited By 0,Identifying Flakiness in Quantum Programs,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Pair programming (PP) has been a widespread practice for decades and is known for facilitating knowledge exchange and improving the quality of software. Many agilists advocated the importance of collocation, face-to-face interaction, and physical artifacts incorporated in the shared workspace when pairing. After a long period of forced work-from-home, many knowledge workers prefer to work remotely two or three days per week, which is affecting practices such as PP. In this revelatory single-case study, we aimed to understand how PP is practiced during hybrid work when team members alternate between on-site days and working from home. We collected qualitative and quantitative data through 11 semi-structured interviews, observations, feedback sessions, and self-reported surveys. The interviewees were members of an agile software development team in a Norwegian fintech company. The results presented in this paper indicate that PP can be practiced through on-site, remote, and mixed sessions, where the mixed mode seems to be the least advantageous. The findings highlight the importance of adapting the work environment to suit individual work mode preferences when it comes to PP. In the future, we will build on these findings to explore PP in other teams and organizations practicing hybrid work. © 2023 IEEE.",agile software development;  collaboration practices;  hybrid work;  remote work;  teamwork,"Tkalich, A. and Moe, N.B. and Andersen, N.H. and Stray, V. and Barbala, A.M.",,10.1109/ESEM56168.2023.10304797,,,The work was partially supported by the Research Council of Norway through the 10xTeams project (Grant 309344).,"Knowledge management, Agile software development;  Collaboration practices;  Face-to-face interaction;  Hybrid work;  Knowledge exchange;  Pair-programming;  Physical artifacts;  Quality of softwares;  Remote work;  Teamwork, Software design",cited By 1,Pair Programming Practiced in Hybrid Work,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews. © 2023 IEEE.",empirical research;  infrastructure;  Knowledge graph;  literature review;  requirements engineering;  sustainability,"Karras, O. and Wernlein, F. and Klunder, J. and Auer, S.",,10.1109/ESEM56168.2023.10304795,Deutsche Forschungsgemeinschaft,Germany,"ACKNOWLEDGMENT The authors thank the Federal Government, the Heads of Government of the Länder, as well as the Joint Science Conference (GWK), for their funding and support within the NFDI4Ing and NFDI4DataScience consortia. This work was funded by the German Research Foundation (DFG) - project numbers 442146713 and 460234259, by the European Research Council for the project ScienceGRAPH (Grant agreement ID: 819536), and by the TIB - Leibniz Information Centre for Science and Technology.","Knowledge graph, 'current;  Divide-and-conquer;  Empirical research;  Infrastructure;  Knowledge graphs;  Literature reviews;  Long-term goals;  Requirement engineering;  Research communities;  Technical infrastructure, Requirements engineering",cited By 0,Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews. © 2023 IEEE.",empirical research;  infrastructure;  Knowledge graph;  literature review;  requirements engineering;  sustainability,"Karras, O. and Wernlein, F. and Klunder, J. and Auer, S.",,10.1109/ESEM56168.2023.10304795,Leibniz Information Centre for Science and Technology,Germany,,"Knowledge graph, 'current;  Divide-and-conquer;  Empirical research;  Infrastructure;  Knowledge graphs;  Literature reviews;  Long-term goals;  Requirement engineering;  Research communities;  Technical infrastructure, Requirements engineering",cited By 0,Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews. © 2023 IEEE.",empirical research;  infrastructure;  Knowledge graph;  literature review;  requirements engineering;  sustainability,"Karras, O. and Wernlein, F. and Klunder, J. and Auer, S.",,10.1109/ESEM56168.2023.10304795,European Commission,EU,,"Knowledge graph, 'current;  Divide-and-conquer;  Empirical research;  Infrastructure;  Knowledge graphs;  Literature reviews;  Long-term goals;  Requirement engineering;  Research communities;  Technical infrastructure, Requirements engineering",cited By 0,Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Time-bounded collaborative events are events that bring together different participants to address a problem through the creation of a computational artifact over a short period of time. Examples of these events include hackathons, game jams, and ideathons. Despite previous research on these events, little is understood about how people seek and get help during these events. Aims: Our goal is to understand how event participants seek and get help during time-bounded collaborative events to design better strategies supporting search behaviour. This is necessary because participants learn and improve different skills important for working in the industry during these events. Method: We conducted a mixed-methods study where we collected data through a large-scale survey with participants of a global-scale game jam. Our primary method was quantitative, but we had a qualitative dataset that we used to augment aspects of our quantitative results. Results: Our findings suggest that professional and independent developers are the ones who provide help most often, while students interact more with mentors. In addition, the frequency that participants getting help from mentors decreases with increased experience in game development and the number of participations in game jams. Participants also point to mentors as an essential part of the jams: they are expert facilitators, so much so that the perception of the mentors' absence or distance was reported as a disappointment. Conclusions: Understanding how participants in time-bounded collaborative events seek and get help is important to design more effective events and improving the participants' overall experience. We translate our findings into suggestions for event organizers. © 2023 IEEE.",Game development;  Game Jams;  Help-seeking;  Information-seeking;  Time-bounded collaborative events,"De Almeida Melo, L. and De Souza, C.R.B. and Perin, M.G. and Batista De Almeida, A.C.D. and Filho, F.F.",,10.1109/ESEM56168.2023.10304790,Conselho Nacional de Desenvolvimento Cientifico e Tecnologico,Brazil,"This research has been partially funded by the Brazilian National Council for Research and Development (CNPq), under research grant number 400920/2019-0. The authors also thank survey respondents for their time.","Game development;  Game jam;  Help seeking;  Information seeking;  Large scale surveys;  Learn+;  Mixed method;  Search behavior;  Short periods;  Time-bounded collaborative event, Software design",cited By 0,All For One and One For All: Investigating How Global Game Jam Participants Get and Offer Help,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in C++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results. © 2023 IEEE.",Deep learning;  Design patterns recognition;  Machine learning;  NLP;  Programming language models,"Pandey, S.K. and Staron, M. and Horkoff, J. and Ochodek, M. and Mucci, N. and Durisic, D.",,10.1109/ESEM56168.2023.10304862,Vinnova,Sweden,"Acknowledgements. This study was financed by CHAIR (Chalmers AI Research Center) project “T4AI”, Vinnova, Software Center, Volvo Cars, AB Volvo, Chalmers, University of Gothenburg, and National Science Centre, Poland project “Source-code-representations for machine-learning-based identification of defective code fragments” (OPUS 21), registered under no. 2021/41/B/ST6/02510, also provided funding.","C++ (programming language);  Computational linguistics;  Deep learning;  Learning systems;  Open source software;  Semantics, 'current;  Deep learning;  Design pattern recognition;  Design Patterns;  Facebook;  Machine-learning;  Pattern recognition method;  Pattern recognitions techniques;  Programming language models;  Semantics Information, Pattern recognition",cited By 0,TransDPR: Design Pattern Recognition Using Programming Language Models,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in C++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results. © 2023 IEEE.",Deep learning;  Design patterns recognition;  Machine learning;  NLP;  Programming language models,"Pandey, S.K. and Staron, M. and Horkoff, J. and Ochodek, M. and Mucci, N. and Durisic, D.",,10.1109/ESEM56168.2023.10304862,University of Gothenburg,Sweden,,"C++ (programming language);  Computational linguistics;  Deep learning;  Learning systems;  Open source software;  Semantics, 'current;  Deep learning;  Design pattern recognition;  Design Patterns;  Facebook;  Machine-learning;  Pattern recognition method;  Pattern recognitions techniques;  Programming language models;  Semantics Information, Pattern recognition",cited By 0,TransDPR: Design Pattern Recognition Using Programming Language Models,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in C++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results. © 2023 IEEE.",Deep learning;  Design patterns recognition;  Machine learning;  NLP;  Programming language models,"Pandey, S.K. and Staron, M. and Horkoff, J. and Ochodek, M. and Mucci, N. and Durisic, D.",,10.1109/ESEM56168.2023.10304862,"National Science Centre, Poland",Poland,,"C++ (programming language);  Computational linguistics;  Deep learning;  Learning systems;  Open source software;  Semantics, 'current;  Deep learning;  Design pattern recognition;  Design Patterns;  Facebook;  Machine-learning;  Pattern recognition method;  Pattern recognitions techniques;  Programming language models;  Semantics Information, Pattern recognition",cited By 0,TransDPR: Design Pattern Recognition Using Programming Language Models,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in C++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results. © 2023 IEEE.",Deep learning;  Design patterns recognition;  Machine learning;  NLP;  Programming language models,"Pandey, S.K. and Staron, M. and Horkoff, J. and Ochodek, M. and Mucci, N. and Durisic, D.",,10.1109/ESEM56168.2023.10304862,Vinnova,Sweden,,"C++ (programming language);  Computational linguistics;  Deep learning;  Learning systems;  Open source software;  Semantics, 'current;  Deep learning;  Design pattern recognition;  Design Patterns;  Facebook;  Machine-learning;  Pattern recognition method;  Pattern recognitions techniques;  Programming language models;  Semantics Information, Pattern recognition",cited By 0,TransDPR: Design Pattern Recognition Using Programming Language Models,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in C++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results. © 2023 IEEE.",Deep learning;  Design patterns recognition;  Machine learning;  NLP;  Programming language models,"Pandey, S.K. and Staron, M. and Horkoff, J. and Ochodek, M. and Mucci, N. and Durisic, D.",,10.1109/ESEM56168.2023.10304862,Volvo Cars,Sweden,,"C++ (programming language);  Computational linguistics;  Deep learning;  Learning systems;  Open source software;  Semantics, 'current;  Deep learning;  Design pattern recognition;  Design Patterns;  Facebook;  Machine-learning;  Pattern recognition method;  Pattern recognitions techniques;  Programming language models;  Semantics Information, Pattern recognition",cited By 0,TransDPR: Design Pattern Recognition Using Programming Language Models,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Cyber-Physical Systems (CPSs) combine digital cyber technologies with physical processes. As in any other software system, in the case of CPSs, the use of Artificial Intelligence (AI) techniques in general, and Deep Neural Networks (DNNs) in particular, is contantly increasing. While recent studies have considerably advanced the field of testing AI-enabled systems, it has not yet been investigated how different Deep Learning (DL) bugs affect AI-enabled CPSs in operation. This work-in-progress paper presents a preliminary evaluation on how such bugs can affect CPSs in operation by using a mobile robot as a case study system. For that, we generated DL mutants by using operators proposed by Humbatova et al., which are operators based on real-world DL faults. Our preliminary investigation suggests that such bugs are more difficult to detect when they are deployed in operation rather than when testing their DNN in an off-line setup, which contrast with related studies. © 2023 IEEE.",Deep Learning;  Mutation Testing;  Physical Testing,"Arrieta, A. and Valle, P. and Iriarte, A. and Illarramendi, M.",,10.1109/ESEM56168.2023.10304794,,,"This work was partially founded by the Basque Government through their Elkartek program (EGIA project, ref. KK-2022/00119 and SIIRSE project, ref. KK-2022/00007). The authors are part of the Software and Systems Engineering research group of Mondragon Unibertsitatea (IT1519-22), supported by the Department of Education, Universities and Research of the Basque Country.","Deep neural networks;  Embedded systems;  Learning systems;  Program debugging;  Software testing, Artificial intelligence techniques;  Case-studies;  Cybe-physical systems;  Cyber-physical systems;  Deep learning;  Mutation operators;  Mutation testing;  Physical process;  Physical testing;  Software-systems, Cyber Physical System",cited By 0,How Do Deep Learning Faults Affect AI-Enabled Cyber-Physical Systems in Operation? A Preliminary Study Based on DeepCrime Mutation Operators,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Code development is done collaboratively in platforms such as GitHub and GitLab, following a pull-based development model. In this model, developers actively communicate and share their knowledge through conversations. Pull request conversations are affected by social aspects such as communication dynamics among developers, discussion content, and organizational dynamics. Despite prior studies indicating that social aspects indeed impact software quality, it is still unknown to what extent social aspects influence design decay during software development. Thus, since social aspects are intertwined with design and implementation decisions, there is a need for investigating how social aspects contribute to avoiding, reducing, or accelerating design decay. Aims: To fill this gap, we performed a study aimed at investigating the effects of pull request conversation on design decay. Method: We investigated 10,746 pull request conversations from 11 open-source systems, characterizing in terms of three different social aspects: discussion content, organizational and communication dynamics. We considered 18 social metrics to these three social aspects, and analyzed how they associate with design decay. We used a statistical approach to assess which social metrics are able to discriminate between impactful and unimpactful pull requests. Then, we employed a multiple logistic regression model to evaluate the influence of each social metric per social aspect in the presence of each other on design decay. Finally, we also observed how the combination of all social metrics influences the design decay. Results: Our findings reveal that social metrics related to the size and duration of a discussion, the presence of design-related keywords, the team size, and gender diversity can be used to discriminate between design impactful and unimpactful pull requests. Organizational growth and gender diversity prevent decay. Each software community has its unique aspects that can be used to detect and prevent design decay. Also, design improvements can be accomplished by timely feedback, engaged communication, and design-oriented discussions with the contribution of multiple participants who provide significant comments. Conclusion: The social aspects related to pull request conversations are useful indicators of design decay. © 2023 IEEE.",design decay;  pull request;  social aspects,"Barbosa, C. and Uchoa, A. and Coutinho, D. and Assuncao, W.K.G. and Oliveira, A. and Garcia, A. and Fonseca, B. and Rabelo, M. and Coelho, J.E. and Carvalho, E. and Santos, H.",,10.1109/ESEM56168.2023.10304805,Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior,Brazil,"This work is partially funded by CNPq (140770/2021-6, 140185/2020-8, 434969/2018-4, 312149/2016-6, 409536/2017-2, and 427787/2018-1), CAPES/Procad (175956), CAPES/Proex (88887.373933/2019-00), FAPERJ (22520-7/2016), FAPERJ/PDR-10 (202073/2020), FUNCAP (BP5-00197-00042.01.00/22), and by the Instituto Estadual de Engenharia e Arquitetura (IEEA), Secretaria Estadual de Infraestrutura do Estado do Rio de Janeiro (001/2021).","Computer software selection and evaluation;  Dynamics;  Economic and social effects;  Open source software;  Open systems;  Regression analysis;  Software design, Code development;  Design and implementations;  Design decay;  Development model;  Gender diversity;  Organisational;  Organizational dynamics;  Pull request;  Social Metrics;  Software Quality, Social aspects",cited By 1,Beyond the Code: Investigating the Effects of Pull Request Conversations on Design Decay,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Code development is done collaboratively in platforms such as GitHub and GitLab, following a pull-based development model. In this model, developers actively communicate and share their knowledge through conversations. Pull request conversations are affected by social aspects such as communication dynamics among developers, discussion content, and organizational dynamics. Despite prior studies indicating that social aspects indeed impact software quality, it is still unknown to what extent social aspects influence design decay during software development. Thus, since social aspects are intertwined with design and implementation decisions, there is a need for investigating how social aspects contribute to avoiding, reducing, or accelerating design decay. Aims: To fill this gap, we performed a study aimed at investigating the effects of pull request conversation on design decay. Method: We investigated 10,746 pull request conversations from 11 open-source systems, characterizing in terms of three different social aspects: discussion content, organizational and communication dynamics. We considered 18 social metrics to these three social aspects, and analyzed how they associate with design decay. We used a statistical approach to assess which social metrics are able to discriminate between impactful and unimpactful pull requests. Then, we employed a multiple logistic regression model to evaluate the influence of each social metric per social aspect in the presence of each other on design decay. Finally, we also observed how the combination of all social metrics influences the design decay. Results: Our findings reveal that social metrics related to the size and duration of a discussion, the presence of design-related keywords, the team size, and gender diversity can be used to discriminate between design impactful and unimpactful pull requests. Organizational growth and gender diversity prevent decay. Each software community has its unique aspects that can be used to detect and prevent design decay. Also, design improvements can be accomplished by timely feedback, engaged communication, and design-oriented discussions with the contribution of multiple participants who provide significant comments. Conclusion: The social aspects related to pull request conversations are useful indicators of design decay. © 2023 IEEE.",design decay;  pull request;  social aspects,"Barbosa, C. and Uchoa, A. and Coutinho, D. and Assuncao, W.K.G. and Oliveira, A. and Garcia, A. and Fonseca, B. and Rabelo, M. and Coelho, J.E. and Carvalho, E. and Santos, H.",,10.1109/ESEM56168.2023.10304805,Conselho Nacional de Desenvolvimento Cientifico e Tecnologico,Brazil,,"Computer software selection and evaluation;  Dynamics;  Economic and social effects;  Open source software;  Open systems;  Regression analysis;  Software design, Code development;  Design and implementations;  Design decay;  Development model;  Gender diversity;  Organisational;  Organizational dynamics;  Pull request;  Social Metrics;  Software Quality, Social aspects",cited By 1,Beyond the Code: Investigating the Effects of Pull Request Conversations on Design Decay,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Effective peer code review in collaborative software development necessitates useful reviewer comments and supportive automated tools. Code review comments are a central component of the Modern Code Review process in the industry and open-source development. Therefore, it is important to ensure these comments serve their purposes. This paper reflects the evolution of research on the usefulness of code review comments. It examines papers that define the usefulness of code review comments, mine and annotate datasets, study developers' perceptions, analyze factors from different aspects, and use machine learning classifiers to automatically predict the usefulness of code review comments. Finally, it discusses the open problems and challenges in recognizing useful code review comments for future research. © 2023 IEEE.",Modern Code Review;  Software Engineering;  Software Quality;  Useful Comments,"Ahmed, S. and Eisty, N.U.",,10.1109/ESEM56168.2023.10304792,,,,"Computer software selection and evaluation;  Learning systems;  Open source software;  Open systems;  Software design, Automated tools;  Central component;  Code review;  Collaborative software development;  Modern code review;  Open source development;  Peer code review;  Review process;  Software Quality;  Useful comment, Classification (of information)",cited By 0,Exploring the Advances in Identifying Useful Code Review Comments,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Fault localization in software maintenance and debugging can be a costly process. Spectrum-Based Fault Localization (SBFL) is a widely-used method for fault localization. It assigns suspicion scores to code elements based on tests, indicating the likelihood of defects in specific code lines. However, the effectiveness of SBFL approaches varies depending on the subject code. Aims: In this paper, our aim is to present an approach that combines multiple SBFL formulae using evidence theory. Method: We first introduce a taxonomy of SBFL techniques. Then, we describe how we fuse suspiciousness scores obtained from a set of SBFL formulae. We also introduce a concept of fuzzy windows, and describe how they can enhance localization accuracy and how they can be tuned to further refine results. Results: We present an empirical evaluation of our approach using the Defects4J dataset. Our results demonstrate improvements in fault localization accuracy over existing statement-level SBFL techniques. Specifically, by fusing three SBFL methods, our approach reduces code inspection effort by up to 34.5 % with a size-4 window and increases the hit rate for the top 10% most suspicious lines by 27.9 % using a size-7 window. Moreover, in multi-line bug scenarios, our approach reduces code inspection effort by up to 35.6% and achieves a maximum increase of 43.2% in the hit rate of the top 10% most suspicious lines. Additionally, our approach outperforms state-of-the-art machine learning-based method-level fusion approaches in terms of top rank fault localization accuracy. Conclusions: Our study highlights the applicability of evidence theory in addressing fault localization as an uncertain and ambiguous information fusion problem involving multiple SBFL techniques. The combination of SBFL formulae using evidence theory, along with the use of fuzzy windows, shows promise in enhancing fault localization accuracy. © 2023 IEEE.",evidence theory;  fault localization;  information fusion;  uncertainty,"Zhang, Y. and Leach, K. and Huang, Y.",,10.1109/ESEM56168.2023.10304791,,,,"Codes (symbols);  Defects;  Program debugging, Code inspections;  Evidence theories;  Exploratory studies;  Fault localization;  Hit rate;  Localization accuracy;  Localization technique;  Multiple spectra;  Spectra's;  Uncertainty, Information fusion",cited By 0,Leveraging Evidence Theory to Improve Fault Localization: An Exploratory Study,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context: Society 5.0 depends on intelligent technologies that capture and monitor data in real-time. Software development needs to guarantee the privacy and protection of personal data, according to regulations such as the GDPR in Europe and the LGPD in Brazil. Objective: The main goal of this study is to check the possibility of Personal Data Inventory (PDI) adoption for User Stories and BDD Scenarios creation, to verify its relevance regarding software understanding and documentation. Method: An experiment was performed with 34 undergraduate students from higher education institutions in Brazil. They had to create User Stories and BDD Scenarios from two documents: (1) PDI and (2) a detailed system description. Results: After assessing 184 software engineering artifacts, findings indicated there were no statistically significant differences in the quantity and quality of Stories and Scenarios generated by the two groups. Conclusion: The PDI document can be effectively employed in the Requirements Engineering field as a valuable tool for comprehending and specifying software, thereby ensuring compliance with data protection and privacy regulations. The fusion of Information Security and Software Engineering has the potential to enhance the development of products and services that align better with privacy requirements. © 2023 IEEE.",Agile Requirements Specification;  LGPD;  Personal Data Inventory;  Privacy Laws,"Saraiva, J. and Soares, S.",,10.1109/ESEM56168.2023.10304806,Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior,Brazil,"ACKNOWLEDGMENT This work is partially supported by INES 2.0 (www.ines.org.br), CNPq grant 465614/2014-0, FACEPE grants APQ-0399-1.03/17 and APQ/0388-1.03/14, and CAPES grant 88887.136410/2017-00. Sérgio Soares is partially supported by CNPq grant 306000/2022-9.","Boolean functions;  Laws and legislation;  Software design;  Students, Agile requirement specification;  Agile requirements;  Agile software engineering;  LGPD;  Personal data inventory;  Privacy and security;  Privacy law;  Requirements specifications;  Security documents;  User stories, Data privacy",cited By 0,Privacy and Security Documents for Agile Software Engineering: An Experiment of LGPD Inventory Adoption,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context: Society 5.0 depends on intelligent technologies that capture and monitor data in real-time. Software development needs to guarantee the privacy and protection of personal data, according to regulations such as the GDPR in Europe and the LGPD in Brazil. Objective: The main goal of this study is to check the possibility of Personal Data Inventory (PDI) adoption for User Stories and BDD Scenarios creation, to verify its relevance regarding software understanding and documentation. Method: An experiment was performed with 34 undergraduate students from higher education institutions in Brazil. They had to create User Stories and BDD Scenarios from two documents: (1) PDI and (2) a detailed system description. Results: After assessing 184 software engineering artifacts, findings indicated there were no statistically significant differences in the quantity and quality of Stories and Scenarios generated by the two groups. Conclusion: The PDI document can be effectively employed in the Requirements Engineering field as a valuable tool for comprehending and specifying software, thereby ensuring compliance with data protection and privacy regulations. The fusion of Information Security and Software Engineering has the potential to enhance the development of products and services that align better with privacy requirements. © 2023 IEEE.",Agile Requirements Specification;  LGPD;  Personal Data Inventory;  Privacy Laws,"Saraiva, J. and Soares, S.",,10.1109/ESEM56168.2023.10304806,Instituto Nacional de Ciencia e Tecnologia para Engenharia de Software,Brazil,,"Boolean functions;  Laws and legislation;  Software design;  Students, Agile requirement specification;  Agile requirements;  Agile software engineering;  LGPD;  Personal data inventory;  Privacy and security;  Privacy law;  Requirements specifications;  Security documents;  User stories, Data privacy",cited By 0,Privacy and Security Documents for Agile Software Engineering: An Experiment of LGPD Inventory Adoption,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context: Society 5.0 depends on intelligent technologies that capture and monitor data in real-time. Software development needs to guarantee the privacy and protection of personal data, according to regulations such as the GDPR in Europe and the LGPD in Brazil. Objective: The main goal of this study is to check the possibility of Personal Data Inventory (PDI) adoption for User Stories and BDD Scenarios creation, to verify its relevance regarding software understanding and documentation. Method: An experiment was performed with 34 undergraduate students from higher education institutions in Brazil. They had to create User Stories and BDD Scenarios from two documents: (1) PDI and (2) a detailed system description. Results: After assessing 184 software engineering artifacts, findings indicated there were no statistically significant differences in the quantity and quality of Stories and Scenarios generated by the two groups. Conclusion: The PDI document can be effectively employed in the Requirements Engineering field as a valuable tool for comprehending and specifying software, thereby ensuring compliance with data protection and privacy regulations. The fusion of Information Security and Software Engineering has the potential to enhance the development of products and services that align better with privacy requirements. © 2023 IEEE.",Agile Requirements Specification;  LGPD;  Personal Data Inventory;  Privacy Laws,"Saraiva, J. and Soares, S.",,10.1109/ESEM56168.2023.10304806,Conselho Nacional de Desenvolvimento Cientifico e Tecnologico,Brazil,,"Boolean functions;  Laws and legislation;  Software design;  Students, Agile requirement specification;  Agile requirements;  Agile software engineering;  LGPD;  Personal data inventory;  Privacy and security;  Privacy law;  Requirements specifications;  Security documents;  User stories, Data privacy",cited By 0,Privacy and Security Documents for Agile Software Engineering: An Experiment of LGPD Inventory Adoption,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context: Society 5.0 depends on intelligent technologies that capture and monitor data in real-time. Software development needs to guarantee the privacy and protection of personal data, according to regulations such as the GDPR in Europe and the LGPD in Brazil. Objective: The main goal of this study is to check the possibility of Personal Data Inventory (PDI) adoption for User Stories and BDD Scenarios creation, to verify its relevance regarding software understanding and documentation. Method: An experiment was performed with 34 undergraduate students from higher education institutions in Brazil. They had to create User Stories and BDD Scenarios from two documents: (1) PDI and (2) a detailed system description. Results: After assessing 184 software engineering artifacts, findings indicated there were no statistically significant differences in the quantity and quality of Stories and Scenarios generated by the two groups. Conclusion: The PDI document can be effectively employed in the Requirements Engineering field as a valuable tool for comprehending and specifying software, thereby ensuring compliance with data protection and privacy regulations. The fusion of Information Security and Software Engineering has the potential to enhance the development of products and services that align better with privacy requirements. © 2023 IEEE.",Agile Requirements Specification;  LGPD;  Personal Data Inventory;  Privacy Laws,"Saraiva, J. and Soares, S.",,10.1109/ESEM56168.2023.10304806,Fundacao de Amparo a Ciencia e Tecnologia do Estado de Pernambuco,Brazil,,"Boolean functions;  Laws and legislation;  Software design;  Students, Agile requirement specification;  Agile requirements;  Agile software engineering;  LGPD;  Personal data inventory;  Privacy and security;  Privacy law;  Requirements specifications;  Security documents;  User stories, Data privacy",cited By 0,Privacy and Security Documents for Agile Software Engineering: An Experiment of LGPD Inventory Adoption,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Wrongdoings occurring within or in relation to software can have big implications on individuals, groups of people, or society as a whole. Whistleblowing is considered an effective tool to reveal and stop wrongdoing but is still a controversial topic that has been researched sparsely in the software industry. Aim: In this study we address this gap and research the current environment for whistleblowing (reporting wrongdoing) in the software industry. Method: We surveyed 147 software practitioners about their views on whistleblowing, the current means they have to report software-related wrongdoing, and the enabling and obstructing factors to whistleblow. Results: Our study shows that software practitioners have a positive view towards whistleblowing. However, in practice whistleblowing is obstructed by the difficulty of proving the actual harm and fear of retaliation. Practitioners with more years of experience report more comfort using readily established mechanisms and procedures in their organization, are more willing to speak up and have more confidence that their report will lead to action than their less experienced peers. These differences are statistically significant. Conclusion: Through our results we conclude that the software industry needs to improve the environment for whistleblowers by providing more external reporting mechanisms, anonymity, and confidentiality, as well as support practitioners with less years of experience. © 2023 IEEE.",ethics in software engineering;  software industry;  whistleblowing,"Reijenga, S. and Aslam, K. and Guzman, E.",,10.1109/ESEM56168.2023.10304802,,,,"'current;  Controversial topics;  Effective tool;  Ethic in software engineering;  Experience report;  Industry needs;  Reporting mechanisms;  Software industry;  Software practitioners;  Whistleblowing, Software engineering",cited By 0,Whistleblowing in the Software Industry: a Survey,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Since there are not many empirical studies on the effect of sprint duration on efficiency and productivity in large-scale agile software developments, we collected and analyzed data from large-scale embedded software development. Aim: Our objective is to analyze the effect of sprint duration on the velocity in scrum-based software development projects. Method: We selected five scrums with the least changes from a large-scale embedded software development project of a big company and measured the change in velocity when operating with 3-week and 2-week sprint durations under the same condition. Results: Our analysis revealed that the 2-week sprint duration had about 30 percent higher development velocity. The average velocity was analyzed in four milestones over about two and half years of period, and the one-tailed T-test with an assumption of unequal variances proved that the development cycle of a 2-week sprint duration was higher than that of a 3-week sprint duration. Conclusions: In this respect, this paper contributes to improving the productivity in large-scale embedded software projects. © 2023 IEEE.",Agile Development;  Sprint Duration;  Story Point;  Velocity,"Park, D.S. and Noh, J.Y.",,10.1109/ESEM56168.2023.10304859,,,,"Embedded software, Agile development;  Agile software development;  Embedded software development;  Embedded software development projects;  Empirical studies;  Large-scales;  Software development projects;  Software project;  Sprint duration;  Story point, Software design",cited By 0,The Effect of Sprint Duration to the Velocity in a Large-Scale Embedded Software Project,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: The existence of toxic conversations in open-source platforms can degrade relationships among software developers and may negatively impact software product quality. To help mitigate this, some initial work has been done to detect toxic comments in the Software Engineering (SE) domain. Aims: Since automatically classifying an entire text as toxic or non-toxic does not help human moderators to understand the specific reason(s) for toxicity, we worked to develop an explainable toxicity detector for the SE domain. Method: Our explainable toxicity detector can detect specific spans of toxic content from SE texts, which can help human moderators by automatically highlighting those spans. This toxic span detection model, ToxiSpanSE, is trained with the 19,651 code review (CR) comments with labeled toxic spans. Our annotators labeled the toxic spans within 3,757 toxic CR samples. We explored several types of models, including one lexicon-based approach and five different transformer-based encoders. Results: After an extensive evaluation of all models, we found that our fine-tuned RoBERTa model achieved the best score with 0.88 F1, 0.87 precision, and 0.93 recall for toxic class tokens, providing an explainable toxicity classifier for the SE domain. Conclusion: Since ToxiSpanSE is the first tool to detect toxic spans in the SE domain, this tool will pave a path to combat toxicity in the SE community. © 2023 IEEE.",explainability;  natural language processing;  software engineering;  span detection;  toxicity,"Sarker, J. and Sultana, S. and Wilson, S.R. and Bosu, A.",,10.1109/ESEM56168.2023.10304855,,,,"Codes (symbols);  Natural language processing systems;  Open source software;  Open systems, Code review;  Explainability;  Language processing;  Natural language processing;  Natural languages;  Open source platforms;  Software developer;  Software engineering domain;  Span detection;  Toxicity detection, Toxicity",cited By 0,ToxiSpanSE: An Explainable Toxicity Detection in Code Review Comments,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: As improving code review (CR) effectiveness is a priority for many software development organizations, projects have deployed CR analytics platforms to identify potential improvement areas. The number of issues identified, which is a crucial metric to measure CR effectiveness, can be misleading if all issues are placed in the same bin. Therefore, a finer-grained classification of issues identified during CRs can provide actionable insights to improve CR effectiveness. Although a recent work by Fregnan et al. proposed automated models to classify CR-induced changes, we have noticed two potential improvement areas - i) classifying comments that do not induce changes and ii) using deep neural networks (DNN) in conjunction with code context to improve performances. Aims: This study aims to develop an automated CR comment classifier that leverages DNN models to achieve a more reliable performance than Fregnan et al. Method: Using a manually labeled dataset of 1,828 CR comments, we trained and evaluated supervised learning-based DNN models leveraging code context, comment text, and a set of code metrics to classify CR comments into one of the five high-level categories proposed by Turzo and Bosu. Results: Based on our 10-fold cross-validation-based evaluations of multiple combinations of tokenization approaches, we found a model using CodeBERT achieving the best accuracy of 59.3%. Our approach outperforms Fregnan et al.'s approach by achieving 18.7% higher accuracy. Conclusion: In addition to facilitating improved CR analytics, our proposed model can be useful for developers in prioritizing code review feedback and selecting reviewers. © 2023 IEEE.",analytics;  classification;  code review;  open source software;  OSS;  review comment,"Turzo, A.K. and Faysal, F. and Poddar, O. and Sarker, J. and Iqbal, A. and Bosu, A.",,10.1109/ESEM56168.2023.10304851,National Science Foundation,United States,"Work conducted for this research is partially supported by the US National Science Foundation under Grant No. 1850475. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We thank Md. Toufikuzzaman for his assistance during the data mining process.","Automation;  Classification (of information);  Codes (symbols);  Neural network models;  Open source software;  Open systems;  Software design, Analytic;  Automated classification;  Code review;  Fine grained;  Neural network model;  Open-source softwares;  Organization projects;  OSS;  Review comment;  Software development organizations, Deep neural networks",cited By 0,Towards Automated Classification of Code Review Feedback to Support Analytics,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"BACKGROUND: In an online issue tracking system, e.g., GitHub Issue Tracker, feature requests and the associated comment stream provide valuable crowd-generated knowledge for requirements elicitation. To decide whether a feature request should be accepted or not, stakeholders need to identify the comments for/against the feature and understand the two-sided opinions, which is time- and effort-consuming considering the abundant information embedded in lengthy comment stream per feature request. AIMS: This paper proposes VoteBot for automatically detecting stance (for/against) and summarizing the related opinions on a feature request, which can facilitate the decision making (i.e., voting) of feature requests. To our best knowledge, such an approach is previously unexplored for crowd-based requirements elicitation. METHOD: VoteBot is a relation-aware approach, which incorporates three types of relations among the comments or among the comment sentences to better understand the discussions about feature requests. Specifically, it extracts the reply-to relation among the comments, and incorporates it into a BERT-based classifier for stance detection. It also designs a graph-based ranking algorithm, and incorporates semantic relevance and argumentative relations for stance summarization. RESULTS: The automatic evaluation on 250 feature requests with 6,598 comments from five GitHub projects, and the evaluation with practitioners on five new projects, show the promising results. CONCLUSIONS: VoteBot is effective in stance detection and stance summarization, and potentially useful for understanding feature requests and associated discussions in real-world practice. © 2023 IEEE.",Feature Request;  Stance Detection;  Stance Summarization,"Wang, Y. and Wang, J. and Zhang, H. and Wang, K. and Wang, Q.",,10.1109/ESEM56168.2023.10304865,Chinese Academy of Sciences,China,"This work was supported by the National Natural Science Foundation of China Grant No.62232016, No.62072442 and No.62002348, and Youth Innovation Promotion Association Chinese Academy of Sciences.","Feature extraction;  Graphic methods;  Requirements engineering;  Semantics, Decisions makings;  Feature requests;  Graph based rankings;  Issue Tracking;  Ranking algorithm;  Requirements elicitation;  Stance detection;  Stance summarization;  Tracking system;  Types of relations, Decision making",cited By 0,What are Pros and Cons? Stance Detection and Summarization on Feature Request,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"BACKGROUND: In an online issue tracking system, e.g., GitHub Issue Tracker, feature requests and the associated comment stream provide valuable crowd-generated knowledge for requirements elicitation. To decide whether a feature request should be accepted or not, stakeholders need to identify the comments for/against the feature and understand the two-sided opinions, which is time- and effort-consuming considering the abundant information embedded in lengthy comment stream per feature request. AIMS: This paper proposes VoteBot for automatically detecting stance (for/against) and summarizing the related opinions on a feature request, which can facilitate the decision making (i.e., voting) of feature requests. To our best knowledge, such an approach is previously unexplored for crowd-based requirements elicitation. METHOD: VoteBot is a relation-aware approach, which incorporates three types of relations among the comments or among the comment sentences to better understand the discussions about feature requests. Specifically, it extracts the reply-to relation among the comments, and incorporates it into a BERT-based classifier for stance detection. It also designs a graph-based ranking algorithm, and incorporates semantic relevance and argumentative relations for stance summarization. RESULTS: The automatic evaluation on 250 feature requests with 6,598 comments from five GitHub projects, and the evaluation with practitioners on five new projects, show the promising results. CONCLUSIONS: VoteBot is effective in stance detection and stance summarization, and potentially useful for understanding feature requests and associated discussions in real-world practice. © 2023 IEEE.",Feature Request;  Stance Detection;  Stance Summarization,"Wang, Y. and Wang, J. and Zhang, H. and Wang, K. and Wang, Q.",,10.1109/ESEM56168.2023.10304865,National Natural Science Foundation of China,China,,"Feature extraction;  Graphic methods;  Requirements engineering;  Semantics, Decisions makings;  Feature requests;  Graph based rankings;  Issue Tracking;  Ranking algorithm;  Requirements elicitation;  Stance detection;  Stance summarization;  Tracking system;  Types of relations, Decision making",cited By 0,What are Pros and Cons? Stance Detection and Summarization on Feature Request,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Despite the widespread use of automated security defect detection tools, software projects still contain many security defects that could result in serious damage. Such tools are largely context-insensitive and may not cover all possible scenarios in testing potential issues, which makes them susceptible to missing complex security defects. Hence, thorough detection entails a synergistic cooperation between these tools and human-intensive detection techniques, including code review. Code review is widely recognized as a crucial and effective practice for identifying security defects. Aim: This work aims to empirically investigate security defect detection through code review. Method: To this end, we conducted an empirical study by analyzing code review comments derived from four projects in the OpenStack and Qt communities. Through manually checking 20,995 review comments obtained by keyword-based search, we identified 614 comments as security-related. Results: Our results show that (1) security defects are not prevalently discussed in code review, (2) more than half of the reviewers provided explicit fixing strategies/solutions to help developers fix security defects, (3) developers tend to follow reviewers' suggestions and action the changes, (4) Not worth fixing the defect now and Disagreement between the developer and the reviewer are the main causes for not resolving security defects. Conclusions: Our research results demonstrate that (1) software security practices should combine manual code review with automated detection tools, achieving a more comprehensive coverage to identifying and addressing security defects, and (2) promoting appropriate standardization of practitioners' behaviors during code review remains necessary for enhancing software security. © 2023 IEEE.",Code Review;  Empirical Study;  OpenStack;  Qt;  Security Defect,"Yu, J. and Fu, L. and Liang, P. and Tahir, A. and Shahin, M.",,10.1109/ESEM56168.2023.10304852,National Natural Science Foundation of China,China,This work is funded by the NSFC with Grant No. 62172311 and the Special Fund of Hubei Luojia Laboratory. Amjed Tahir is supported by a MU SREF grant.,"Damage detection;  Inspection equipment;  Platform as a Service (PaaS), Code review;  Defect detection;  Detection tools;  Effective practices;  Empirical studies;  Openstack;  Qt;  Security defect;  Software project;  Software security, Defects",cited By 0,Security Defect Detection via Code Review: A Study of the OpenStack and Qt Communities,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Contemporary software development relies heavily on reusing already implemented functionality, usually in the form of packages. Aims: We aim to shed light on developers' preferences when selecting packages in R language. Method: To do that, we create and administer a survey to over 1000 developers who have added one of two common dataframe enhancement libraries in R to their projects: data.table or tidyr. We design a questionnaire using the Social Contagion Theory (SCT) following prior work on technology adoption and ensure that key dimensions affecting developer choice are considered. Results: Of the 1085 developers we contacted, 803 completed the survey asking them to prioritize various factors known to affect developer perceptions of package quality and to provide their background. Most developers self-identified as data scientists with two to five years of work experience. We found significant differences between the preferences of developers who chose data.table and tidyr. Surprisingly, package reputation based on easy-to-see measures, such as the number of stars on GitHub, was not an important factor for either group. Conclusions: Our findings demonstrate the inherently social nature of package adoption. They can help design future studies on how different populations of developers make decisions on which software packages to use in their projects. Finally, package developers and maintainers can benefit by better understanding the prime concerns of the users of their packages. © 2023 IEEE.",Code reuse;  Empirical Software engineering;  R System;  Social aspects;  Social Contagion Theory;  Software engineering research;  Software measurement;  Software Supply chains;  User behavior,"Malviya-Thakur, A. and Mockus, A. and Zaretzki, R. and Bichescu, B. and Bradley, R.",,10.1109/ESEM56168.2023.10304869,U.S. Department of Energy,United States,"This manuscript has been authored by UT-Battelle, LLC, USA under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The publisher, by accepting the article for publication, acknowledges that the U.S. Government retains a non-exclusive, paid up, irrevocable, worldwide license to publish or reproduce the published form of the manuscript, or allow others to do so, for U.S. Government purposes. The DOE will provide public access to these results in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).","Behavioral research;  Computer software reusability;  Software design;  Supply chains, Code reuse;  Empirical Software Engineering;  Project data;  R languages;  R system;  Social contagion theory;  Software engineering research;  Software Measurement;  Software supply chains;  User behaviors, Social aspects",cited By 0,How R Developers explain their Package Choice: A Survey,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Contemporary software development relies heavily on reusing already implemented functionality, usually in the form of packages. Aims: We aim to shed light on developers' preferences when selecting packages in R language. Method: To do that, we create and administer a survey to over 1000 developers who have added one of two common dataframe enhancement libraries in R to their projects: data.table or tidyr. We design a questionnaire using the Social Contagion Theory (SCT) following prior work on technology adoption and ensure that key dimensions affecting developer choice are considered. Results: Of the 1085 developers we contacted, 803 completed the survey asking them to prioritize various factors known to affect developer perceptions of package quality and to provide their background. Most developers self-identified as data scientists with two to five years of work experience. We found significant differences between the preferences of developers who chose data.table and tidyr. Surprisingly, package reputation based on easy-to-see measures, such as the number of stars on GitHub, was not an important factor for either group. Conclusions: Our findings demonstrate the inherently social nature of package adoption. They can help design future studies on how different populations of developers make decisions on which software packages to use in their projects. Finally, package developers and maintainers can benefit by better understanding the prime concerns of the users of their packages. © 2023 IEEE.",Code reuse;  Empirical Software engineering;  R System;  Social aspects;  Social Contagion Theory;  Software engineering research;  Software measurement;  Software Supply chains;  User behavior,"Malviya-Thakur, A. and Mockus, A. and Zaretzki, R. and Bichescu, B. and Bradley, R.",,10.1109/ESEM56168.2023.10304869,National Science Foundation,United States,"This work was supported by NSF awards 1633437, 1901102, and 2120429.","Behavioral research;  Computer software reusability;  Software design;  Supply chains, Code reuse;  Empirical Software Engineering;  Project data;  R languages;  R system;  Social contagion theory;  Software engineering research;  Software Measurement;  Software supply chains;  User behaviors, Social aspects",cited By 0,How R Developers explain their Package Choice: A Survey,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Software security is crucial to ensure that the users are protected from undesirable consequences such as malware attacks which can result in loss of data and, subsequently, financial loss. Technical Debt (TD) is a metaphor incurred by suboptimal decisions resulting in long-term consequences such as increased defects and vulnerabilities if not managed. Although previous studies have studied the relationship between security and TD, examining their intersection in developers' discussion on Stack Overflow (SO) is still unexplored. Aims: This study investigates the characteristics of security-related TD questions on SO. More specifically, we explore the prevalence of TD in security-related queries, identify the security tags most prone to TD, and investigate which user groups are more aware of TD. Method: We mined 117,233 security-related questions on SO and used a deep-learning approach to identify 45,078 security-related TD questions. Subsequently, we conducted quantitative and qualitative analyses of the collected security-related TD questions, including sentiment analysis. Results: Our analysis revealed that 38% of the security questions on SO are security-related TD questions. The most recurrent tags among the security-related TD questions emerged as 'security' and 'encryption.' The latter typically have a neutral sentiment, are lengthier, and are posed by users with higher reputation scores. Conclusions: Our findings reveal that developers implicitly discuss TD, suggesting developers have a potential knowledge gap regarding the TD metaphor in the security domain. Moreover, we identified the most common security topics mentioned in TD-related posts, providing valuable insights for developers and researchers to assist developers in prioritizing security concerns in order to minimize TD and enhance software security. © 2023 IEEE.",Crowdsourcing;  Security Vulnerability;  Stack Overflow;  Technical Debt,"Edbert, J.A. and Oishwee, S.J. and Karmakar, S. and Codabux, Z. and Verdecchia, R.",,10.1109/ESEM56168.2023.10304868,Natural Sciences and Engineering Research Council of Canada,Canada,"This research is partly supported by an NSERC Collaborative Research and Training Experience (CREATE) grant on Software Analytics at the University of Saskatchewan and the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on Telecommunications of the Future” (PE0000001 - program “RESTART”).","Cryptography;  Deep learning;  Losses;  Malware;  Sentiment analysis, Financial loss;  Learning approach;  Malware attacks;  Quantitative and qualitative analysis;  Security vulnerabilities;  Sentiment analysis;  Software security;  Stack overflow;  Technical debts;  User groups, Crowdsourcing",cited By 0,Exploring Technical Debt in Security Questions on Stack Overflow,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Software security is crucial to ensure that the users are protected from undesirable consequences such as malware attacks which can result in loss of data and, subsequently, financial loss. Technical Debt (TD) is a metaphor incurred by suboptimal decisions resulting in long-term consequences such as increased defects and vulnerabilities if not managed. Although previous studies have studied the relationship between security and TD, examining their intersection in developers' discussion on Stack Overflow (SO) is still unexplored. Aims: This study investigates the characteristics of security-related TD questions on SO. More specifically, we explore the prevalence of TD in security-related queries, identify the security tags most prone to TD, and investigate which user groups are more aware of TD. Method: We mined 117,233 security-related questions on SO and used a deep-learning approach to identify 45,078 security-related TD questions. Subsequently, we conducted quantitative and qualitative analyses of the collected security-related TD questions, including sentiment analysis. Results: Our analysis revealed that 38% of the security questions on SO are security-related TD questions. The most recurrent tags among the security-related TD questions emerged as 'security' and 'encryption.' The latter typically have a neutral sentiment, are lengthier, and are posed by users with higher reputation scores. Conclusions: Our findings reveal that developers implicitly discuss TD, suggesting developers have a potential knowledge gap regarding the TD metaphor in the security domain. Moreover, we identified the most common security topics mentioned in TD-related posts, providing valuable insights for developers and researchers to assist developers in prioritizing security concerns in order to minimize TD and enhance software security. © 2023 IEEE.",Crowdsourcing;  Security Vulnerability;  Stack Overflow;  Technical Debt,"Edbert, J.A. and Oishwee, S.J. and Karmakar, S. and Codabux, Z. and Verdecchia, R.",,10.1109/ESEM56168.2023.10304868,University of Saskatchewan,Canada,,"Cryptography;  Deep learning;  Losses;  Malware;  Sentiment analysis, Financial loss;  Learning approach;  Malware attacks;  Quantitative and qualitative analysis;  Security vulnerabilities;  Sentiment analysis;  Software security;  Stack overflow;  Technical debts;  User groups, Crowdsourcing",cited By 0,Exploring Technical Debt in Security Questions on Stack Overflow,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Most quality assessment approaches for Deep Learning (DL) focus on finding misbehaviour-inducing inputs. However, it is difficult to clearly understand the causes of misbehaviours, due to the DL software opaqueness. Recent research proposed different techniques to explain DL misbehaviours, producing input explanations either at a 'low level' (raw input elements) or at a 'high level' (input features). Aims: We aim to compare the similarity between different explanations and assess to what extent they are understandable. Method: We have conducted an empirical study involving 3 state-of-the-art techniques for DL explanation in 13 configurations, applied to 2 different DL tasks. We have also collected answers from 48 questionnaires submitted to SE experts. Results: Low- and high-level techniques provide dissimilar explanations for the same inputs. However, experts deemed none of the explanations as useful in 28% of the cases. Conclusion: Despite the complementarity of existing explanations, further research is needed to produce better explanations. © 2023 IEEE.",deep learning;  explainable artificial intelligence;  software testing,"Zohdinasab, T. and Riccio, V. and Tonella, P.",,10.1109/ESEM56168.2023.10304866,Cancer Research Institute,United States,"This work was partially supported by the H2020 project PRECRIME, funded under the ERC Advanced Grant 2017 Program (Agreement n. 787703).","Deep learning, Assessment approaches;  Deep learning;  Empirical studies;  Explainable artificial intelligence;  Input features;  Learning software;  Misbehaviour;  Quality assessment;  Recent researches;  Software testings, Software testing",cited By 0,An Empirical Study on Low- and High-Level Explanations of Deep Learning Misbehaviours,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: The adoption of agile scaling frameworks has become increasingly prevalent in the software industry as organizations seek to extend the use of agile to large and complex projects. The frameworks claim to provide the necessary structure and guidance for scaling agile practices across multiple teams and domains. Despite their growing popularity, the challenges of adopting the frameworks are poorly understood. Aims: In this study, we analyze the experienced challenges related to adopting agile scaling frameworks to understand whether they are different for the different frameworks or between industries or roles within the adopting organization. Method: We conducted a survey targeting software practitioners with experience adopting agile scaling frameworks. We received 204 valid responses, representing ten frameworks adopted in 26 countries and six continents. Results: The most salient challenge, regardless of scaling framework, industry, or role, was organizational politics. Across frameworks, the challenge of forming agile teams emerged as the second most significant and change resistance as the third. We found significant differences in the challenges experienced by organizations based on their chosen framework and significant differences in the challenges between different industries and organizational roles, highlighting the importance of context when adopting agile scaling frameworks. Conclusions: This study provides the first quantitative assessment of the challenges of adopting specific agile scaling frameworks in various contexts. By comparing the challenges associated with different frameworks, industries, and organizational roles, other organizations can better understand the most significant obstacles they may face and improve their framework fit to mitigate them. Future studies could build on these findings to provide additional insights and recommendations for organizations seeking to adopt agile scaling frameworks, including identifying strategies for overcoming common challenges and improving the overall effectiveness of these frameworks in a diverse range of contexts. © 2023 IEEE.",adoption challenges;  agile scaling frameworks;  Agile software development,"Safonova, I. and Paasivaara, M. and Lassenius, C. and Uludag, O. and Putta, A.",,10.1109/ESEM56168.2023.10304858,,,,"Adoption challenge;  Agile practices;  Agile scaling framework;  Agile software development;  Complex programs;  Large programs;  Multiple domains;  Organisational;  Scalings;  Software industry, Software design",cited By 1,Experienced Challenges of Adopting Agile Scaling Frameworks,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Current anti-phishing interventions, which typically involve one-size-fits-all solutions, suffer from limitations such as inadequate usability and poor implementation. Human-centric challenges in anti-phishing technologies remain little understood. Research shows a deficiency in the comprehension of end-user preferences, mental states, and cognitive requirements by developers and practitioners involved in the design, implementation, and evaluation of anti-phishing interventions. Aims: This study addresses the current lack of resources and guidelines for the design, implementation and evaluation of anti-phishing interventions, by presenting personalized guidelines to the developers and practitioners. Method: Through an analysis of 53 academic studies and 16 items of grey literature studies, we systematically identified the challenges and recommendations within the anti-phishing interventions, across different practitioner groups and intervention types. Results: We identified 22 dominant factors at the individual, technical, and organizational levels, that affected the effectiveness of anti-phishing interventions and, accordingly, reported 41 guidelines based on the suggestions and recommendations provided in the studies to improve the outcome of anti-phishing interventions. Conclusions: Our dominant factors can help developers and practitioners enhance their understanding of human-centric, technical and organizational issues in anti-phishing interventions. Our customized guidelines can empower developers and practitioners to counteract phishing attacks. © 2023 IEEE.",human factor;  personalized guidelines;  phishing awareness;  phishing education;  phishing intervention;  phishing training,"Sarker, O. and Haggag, S. and Jayatilaka, A. and Liu, C.",,10.1109/ESEM56168.2023.10304861,Australian Government,Australia,The work has been supported by the Cyber Security Research Centre Limited whose activities are partially funded by the Australian Government’s Cooperative Research Centres Programme.,"Anti-phishing;  Personalized guideline;  Phishing;  Phishing awareness;  Phishing education;  Phishing intervention;  Phishing training, Computer crime",cited By 0,"Personalized Guidelines for Design, Implementation and Evaluation of Anti-Phishing Interventions",International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venues for about two decades. However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional literature review. Very often, SE researchers embark on an SLR with such doubts. We aspire to provide more understanding of when an SLR in SE should be conducted. [Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which indicates the development of a research topic or area. The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE. [Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals. We extracted and analysed the quantitative attributes of the datasets of these SLRs. [Results] The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period covered is 14 years. The number of reviewed papers and review period have a very weak and non-significant positive correlation. [Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether an SLR is conducted at a good time. © 2023 IEEE.",Methodological Study;  Research Synthesis;  SLR;  Software Engineering;  Systematic Literature Review,"Wang, X. and Edison, H. and Khanna, D. and Rafiq, U.",,10.1109/ESEM56168.2023.10304863,Excellence Center at Linkoeping,Sweden,This work has been supported by ELLIIT; the Swedish Strategic Research Area in IT and Mobile Communications.,"Paper, Literature reviews;  Methodological studies;  Positive correlations;  Quantitative attributes;  Research areas;  Research synthesis;  Research topics;  Software engineering journals;  Systematic literature review, Software engineering",cited By 0,How Many Papers Should You Review? A Research Synthesis of Systematic Literature Reviews in Software Engineering,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Empirical studies on widely used model-based development tools such as MATLAB/Simulink are limited despite the tools' importance in various industries. Aims: The aim of this paper is to investigate the reproducibility of previous empirical studies that used Simulink model corpora and to evaluate the generalizability of their results to a newer and larger corpus, including a comparison with proprietary models. Method: The study reviews methodologies and data sources employed in prior Simulink model studies and replicates the previous analysis using SLNET. In addition, we propose a heuristic for determining code-generating Simulink models and assess the open-source models' similarity to proprietary models. Results: Our analysis of SLNET confirms and contradicts earlier findings and highlights its potential as a valuable resource for model-based development research. We found that open-source Simulink models follow good modeling practices and contain models comparable in size and properties to proprietary models. We also collected and distribute 208 git repositories with over 9k commits, facilitating studies on model evolution. Conclusions: The replication study offers actionable insights and lessons learned from the reproduction process, including valuable information on the generalizability of research findings based on earlier open-source corpora to the newer and larger SLNET corpus. The study sheds light on noteworthy attributes of SLNET, which is self-contained and redistributable. © 2023 IEEE.",code generation;  open science;  replication;  reproducibility;  Simulink;  Simulink model,"Shrestha, S.L. and Chowdhury, S.A. and Csallner, C.",,10.1109/ESEM56168.2023.10304867,Microsoft,United States,ACKNOWLEDGEMENTS Christoph Csallner has a potential research conflict of interest due to a financial interest with Microsoft and The Trade Desk. A management plan has been created to preserve objectivity in research in accordance with UTA policy. This material is based upon work supported by the National Science Foundation (NSF) under Grant No. 1911017 and a gift from MathWorks.,"Cell proliferation;  Codes (symbols);  Open source software;  Open systems, Codegeneration;  Empirical studies;  Model based development;  Open science;  Open-source;  Replicability;  Replication;  Reproducibilities;  Simulink;  Simulink models, Simulink",cited By 0,Replicability Study: Corpora For Understanding Simulink Models & Projects,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: Empirical studies on widely used model-based development tools such as MATLAB/Simulink are limited despite the tools' importance in various industries. Aims: The aim of this paper is to investigate the reproducibility of previous empirical studies that used Simulink model corpora and to evaluate the generalizability of their results to a newer and larger corpus, including a comparison with proprietary models. Method: The study reviews methodologies and data sources employed in prior Simulink model studies and replicates the previous analysis using SLNET. In addition, we propose a heuristic for determining code-generating Simulink models and assess the open-source models' similarity to proprietary models. Results: Our analysis of SLNET confirms and contradicts earlier findings and highlights its potential as a valuable resource for model-based development research. We found that open-source Simulink models follow good modeling practices and contain models comparable in size and properties to proprietary models. We also collected and distribute 208 git repositories with over 9k commits, facilitating studies on model evolution. Conclusions: The replication study offers actionable insights and lessons learned from the reproduction process, including valuable information on the generalizability of research findings based on earlier open-source corpora to the newer and larger SLNET corpus. The study sheds light on noteworthy attributes of SLNET, which is self-contained and redistributable. © 2023 IEEE.",code generation;  open science;  replication;  reproducibility;  Simulink;  Simulink model,"Shrestha, S.L. and Chowdhury, S.A. and Csallner, C.",,10.1109/ESEM56168.2023.10304867,National Science Foundation,United States,,"Cell proliferation;  Codes (symbols);  Open source software;  Open systems, Codegeneration;  Empirical studies;  Model based development;  Open science;  Open-source;  Replicability;  Replication;  Reproducibilities;  Simulink;  Simulink models, Simulink",cited By 0,Replicability Study: Corpora For Understanding Simulink Models & Projects,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background. Mindfulness is a meditation technique whose main goal is educating attention by focusing only on one thing at a time, usually breathing. Mindfulness practices improve concentration and attention, being particularly valuable in demanding and high-stress work settings, such as those found in software companies. A family of five controlled experiments on the impact of mindfulness on future and current software engineers' performance has been carried out in six years, whose participants practiced mindfulness daily for several weeks. Aims. This work has a twofold purpose, to present the fifth experiment in the series and to summarize the lessons learned across the family of experiments. The fifth experiment was carried out at INPRO, a public software company in Seville (Spain), in order to evaluate whether software workers improve their performance and some psychological factors, i.e. attention awareness, techno-stress and well-being, compared to a control group. Method. Employees of two departments (Development and Operation) were recruited to participate in the study. Mindfulness (the treatment) was applied to 24 subjects who attended mindfulness sessions daily for six weeks, while the other 27 subjects were the control group. For all subjects, such psychological factors were measured using questionnaires, whereas performance was measured using INPRO's task management systems. Results. Findings have shown significant differences in terms of attention awareness and techno-stress levels between the practitioners who practised mindfulness and those who did not. Benefits on the perceived well-being have also been reported by participants after the continued practise of mindfulness. Regarding the performance, the analysis depicts inconclusive results, probably due to the small size of the sample, a problem which was accentuated by significant variability in the kinds of tasks performed in both departments. Conclusions. Mindfulness practice has yielded significant benefits in the series of experiments, such as performance in the academy and psychological factors in the industry. Nevertheless, its impact on performance in software companies requires further research, since the limited data availability on subjects' performance has led to a small sample size, ultimately posing challenges in drawing dependable conclusions. © 2023 IEEE.",Family of experiments;  Field Experiment;  Human factor;  Mindfulness;  Software Psychology,"Bernardez, B. and Parejo, J.A. and Cruz, M. and Munoz, S. and Ruiz-Cortes, A.",,10.1109/ESEM56168.2023.10304864,Junta de Andalucia,Spain,"This work has been partially supported by grants PID2021-126227NB-C21, PID2021-126227NB-C22, TED2021-131023B-C21, TED2021-131023B-C22 ( MCIN/AEI/10.13039/501100011033) and “ERDF a way of making Europe” (“European Union NextGenerationEU/PRTR”);US–1381595 (Junta de Andalucıa/ERDF,UE). 978-1-6654-5223-6/23/$31.00 ©2023 European Union","Control groups;  Family of experiment;  Field experiment;  Mindfulness;  Performance;  Psychological factors;  Real-world;  Software company;  Software psychology;  Well being, Personnel",cited By 0,On the Impact and Lessons Learned from Mindfulness Practice in a Real-World Software Company,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: In modern software systems, more and more systems are written in multiple programming languages (PLs). There is no comprehensive investigation on the phenomenon of multi-programming-language (MPL) bugs, which resolution involves source files written in multiple PLs. Aim: This work investigated the characteristics of bug resolution in MPL software systems and explored the reasons why bug resolution involves multiple PLs. Method: We conducted an empirical study on 54 MPL projects selected from 655 Apache OSS projects, of which 66,932 bugs were analyzed. Results: (1) the percentage of MPL bugs (MPLBs) in the selected projects ranges from 0.17% to 42.26%, and the percentage of MPLBs for all projects as a whole is 10.01%; (2) 95.0% and 4.5% of all the MPLBs involve source files written in 2 and 3 PLs, respectively; (3) the change complexity resolution characteristics of MPLBs tend to be higher than those of single-programming-language bugs (SPLBs); (4) the open time for MPLBs is 19.52% to 529.57% significantly longer than SPLBs regarding 9 PL combinations; (5) the reopen rate of bugs involving the PL combination of JavaScript and Python reaches 20.66%; (6) we found 6 causes why the bug resolution involves multiple PLs and identified 5 cross-language calling mechanisms. Conclusion: MPLBs are related to increased development difficulty. © 2023 IEEE.",Bug Resolution Characteristic;  Multi-Programming-Language Software System;  Open Source Software,"Li, Z. and Wang, W. and Wang, S. and Liang, P. and Mo, R.",,10.1109/ESEM56168.2023.10304793,Natural Science Foundation of Hubei Province,China,"This work was funded by the Natural Science Foundation of Hubei Province of China under Grant No. 2021CFB577, the National Natural Science Foundation of China under Grant Nos. 62176099 and 62172311, and the Knowledge Innovation Program of Wuhan-Shuguang Project under Grant No. 2022010801020280.","High level languages;  Open systems;  Program debugging, Bug resolution characteristic;  Cross languages;  Empirical studies;  Javascript;  Multi languages;  Multi-programming;  Multi-programming-language software system;  Open-source softwares;  Software-systems;  Source files, Open source software",cited By 0,Understanding Resolution of Multi-Language Bugs: An Empirical Study on Apache Projects,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: In modern software systems, more and more systems are written in multiple programming languages (PLs). There is no comprehensive investigation on the phenomenon of multi-programming-language (MPL) bugs, which resolution involves source files written in multiple PLs. Aim: This work investigated the characteristics of bug resolution in MPL software systems and explored the reasons why bug resolution involves multiple PLs. Method: We conducted an empirical study on 54 MPL projects selected from 655 Apache OSS projects, of which 66,932 bugs were analyzed. Results: (1) the percentage of MPL bugs (MPLBs) in the selected projects ranges from 0.17% to 42.26%, and the percentage of MPLBs for all projects as a whole is 10.01%; (2) 95.0% and 4.5% of all the MPLBs involve source files written in 2 and 3 PLs, respectively; (3) the change complexity resolution characteristics of MPLBs tend to be higher than those of single-programming-language bugs (SPLBs); (4) the open time for MPLBs is 19.52% to 529.57% significantly longer than SPLBs regarding 9 PL combinations; (5) the reopen rate of bugs involving the PL combination of JavaScript and Python reaches 20.66%; (6) we found 6 causes why the bug resolution involves multiple PLs and identified 5 cross-language calling mechanisms. Conclusion: MPLBs are related to increased development difficulty. © 2023 IEEE.",Bug Resolution Characteristic;  Multi-Programming-Language Software System;  Open Source Software,"Li, Z. and Wang, W. and Wang, S. and Liang, P. and Mo, R.",,10.1109/ESEM56168.2023.10304793,National Natural Science Foundation of China,China,,"High level languages;  Open systems;  Program debugging, Bug resolution characteristic;  Cross languages;  Empirical studies;  Javascript;  Multi languages;  Multi-programming;  Multi-programming-language software system;  Open-source softwares;  Software-systems;  Source files, Open source software",cited By 0,Understanding Resolution of Multi-Language Bugs: An Empirical Study on Apache Projects,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: In modern software systems, more and more systems are written in multiple programming languages (PLs). There is no comprehensive investigation on the phenomenon of multi-programming-language (MPL) bugs, which resolution involves source files written in multiple PLs. Aim: This work investigated the characteristics of bug resolution in MPL software systems and explored the reasons why bug resolution involves multiple PLs. Method: We conducted an empirical study on 54 MPL projects selected from 655 Apache OSS projects, of which 66,932 bugs were analyzed. Results: (1) the percentage of MPL bugs (MPLBs) in the selected projects ranges from 0.17% to 42.26%, and the percentage of MPLBs for all projects as a whole is 10.01%; (2) 95.0% and 4.5% of all the MPLBs involve source files written in 2 and 3 PLs, respectively; (3) the change complexity resolution characteristics of MPLBs tend to be higher than those of single-programming-language bugs (SPLBs); (4) the open time for MPLBs is 19.52% to 529.57% significantly longer than SPLBs regarding 9 PL combinations; (5) the reopen rate of bugs involving the PL combination of JavaScript and Python reaches 20.66%; (6) we found 6 causes why the bug resolution involves multiple PLs and identified 5 cross-language calling mechanisms. Conclusion: MPLBs are related to increased development difficulty. © 2023 IEEE.",Bug Resolution Characteristic;  Multi-Programming-Language Software System;  Open Source Software,"Li, Z. and Wang, W. and Wang, S. and Liang, P. and Mo, R.",,10.1109/ESEM56168.2023.10304793, Knowledge Innovation Program of Wuhan-Shuguang,China,,"High level languages;  Open systems;  Program debugging, Bug resolution characteristic;  Cross languages;  Empirical studies;  Javascript;  Multi languages;  Multi-programming;  Multi-programming-language software system;  Open-source softwares;  Software-systems;  Source files, Open source software",cited By 0,Understanding Resolution of Multi-Language Bugs: An Empirical Study on Apache Projects,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies. © 2023 IEEE.",AutoAI;  benchmark;  mixed-method study,"Calefato, F. and Quaranta, L. and Lanubile, F. and Kalinowski, M.",,10.1109/ESEM56168.2023.10304796,,,,"Artificial intelligence, Artificial intelligence learning;  Autoai;  Benchmark;  Building softwares;  Data driven;  End to end;  Machine-learning;  Mixed method;  Mixed-method study;  Team members, Application programs",cited By 0,Assessing the Use of AutoML for Data-Driven Software Engineering,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Due to the high demand for mobile applications, given the exponential growth of users of this type of technology, testing professionals are frequently required to invest time in studying testing tools, in particular, because nowadays, several different tools are available. A variety of tools makes it difficult for testing professionals to choose the one that best fits their goals and supports them in their work. In this sense, we conducted a comparative analysis among five open-source tools for mobile testing: Appium, Robotium, Espresso, Frank, and EarGrey. We used the documentary analysis method to explore the official documentation of each above-cited tool and developed various comparisons based on technical criteria reported in the literature about characteristics that mobile testing tools should have. Our findings are expected to help practitioners understand several aspects of mobile testing tools. © 2023 IEEE.",mobile testing;  software testing;  testing tools,"Da Silva, G. and De Souza Santos, R.",,10.1109/ESEM56168.2023.10304798,,,,"Open source software, Best fit;  Comparative analyzes;  Exponential growth;  High demand;  Mobile applications;  Mobile testing;  Software testings;  Technology testing;  Testing tools;  Type of technology, Software testing",cited By 0,Comparing Mobile Testing Tools Using Documentary Analysis,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Continuous integration (CI) has become a popular method for automating code changes, testing, and software project delivery. However, sufficient testing prior to code submission is crucial to prevent build breaks. Additionally, testing must provide developers with quick feedback on code changes, which requires fast testing times. While regression test selection (RTS) has been studied to improve the cost-effectiveness of regression testing for lower-level tests (i.e., unit tests), it has not been applied to the testing of user interfaces (UI) in application domains such as mobile apps. UI testing at the UI level requires different techniques such as impact analysis and automated test execution. In this paper, we examine the use of RTS in CI settings for UI testing across various open-source mobile apps. Our analysis focuses on using Frequency Analysis to understand the need for RTS, Cost Analysis to evaluate the cost of impact analysis and test case selection algorithms, and Test Reuse Analysis to determine the reusability of UI test sequences for automation. The insights from this study will guide practitioners and researchers in developing advanced RTS techniques that can be adapted to CI environments for mobile apps. © 2023 IEEE.",Android apps;  Empirical study;  Regression testing,"Wang, D. and Zhao, Y. and Xiao, L. and Yu, T.",,10.1109/ESEM56168.2023.10304799,National Science Foundation,United States,"ACKNOWLEDGMENTS This work was supported in part by U.S. National Science Foundation (NSF) under grants CCF-2246186, CCF-2140524, CCF-2152340, CCF-1909085, and CCF-1909763.","Android (operating system);  Codes (symbols);  Computer software reusability;  Cost benefit analysis;  Cost effectiveness;  Integration testing;  Open source software;  Regression analysis;  Reusability, Android apps;  Code changes;  Continuous integrations;  Empirical studies;  Impact analysis;  Integration environments;  Interface testings;  Mobile app;  Regression test selection;  Regression testing, User interfaces",cited By 0,An Empirical Study of Regression Testing for Android Apps in Continuous Integration Environment,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. Aims: This paper analyzes the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face. Hugging Face is the most popular repository for pretrained ML models. We aim to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. Method: We conduct the first repository mining study on the Hugging Face Hub API on carbon emissions and answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? Results: Key findings from the study include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain reporting emissions. The study also uncovers correlations between carbon emissions and various attributes, such as model size, dataset size, ML application domains and performance metrics. Conclusions: The results emphasize the need for software measurements to improve energy reporting practices and the promotion of carbon-efficient model development within the Hugging Face community. To address this issue, we propose two classifications: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. With these classification proposals, we aim to encourage transparency and sustainable model development within the ML community. © 2023 IEEE.",carbon-aware ML;  carbon-efficient ML;  green AI;  repository mining;  software measurement;  sustainable software,"Castano, J. and Martinez-Fernandez, S. and Franch, X. and Bogner, J.",,10.1109/ESEM56168.2023.10304801,European Commission,EU,"VIII. ACKNOWLEDGMENT This work is partially supported by the project TED2021-130923B-I00, funded by MCIN/AEI/10.13039/501100011033 and the European Union Next Generation EU/PRTR.","Artificial intelligence;  Emission control, Carbon emissions;  Carbon-aware machine learning;  Carbon-efficient machine learning;  Green AI;  Machine learning models;  Machine-learning;  Repository mining;  Software Measurement;  Sustainable softwares, Carbon footprint",cited By 0,Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Background: The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. Aims: This paper analyzes the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face. Hugging Face is the most popular repository for pretrained ML models. We aim to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. Method: We conduct the first repository mining study on the Hugging Face Hub API on carbon emissions and answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? Results: Key findings from the study include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain reporting emissions. The study also uncovers correlations between carbon emissions and various attributes, such as model size, dataset size, ML application domains and performance metrics. Conclusions: The results emphasize the need for software measurements to improve energy reporting practices and the promotion of carbon-efficient model development within the Hugging Face community. To address this issue, we propose two classifications: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. With these classification proposals, we aim to encourage transparency and sustainable model development within the ML community. © 2023 IEEE.",carbon-aware ML;  carbon-efficient ML;  green AI;  repository mining;  software measurement;  sustainable software,"Castano, J. and Martinez-Fernandez, S. and Franch, X. and Bogner, J.",,10.1109/ESEM56168.2023.10304801,"Ministerio de Ciencia, Innovacion y Universidades",Spain,"VIII. ACKNOWLEDGMENT This work is partially supported by the project TED2021-130923B-I00, funded by MCIN/AEI/10.13039/501100011033 and the European Union Next Generation EU/PRTR.","Artificial intelligence;  Emission control, Carbon emissions;  Carbon-aware machine learning;  Carbon-efficient machine learning;  Green AI;  Machine learning models;  Machine-learning;  Repository mining;  Software Measurement;  Sustainable softwares, Carbon footprint",cited By 0,Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
"Context: Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair. Large amounts of data for training such models benefit the models' performance. However, the size of the data and models results in long training times and high energy consumption. While publishing source code allows for replicability, users need to repeat the expensive training process if models are not shared. Goals: The main goal of the study is to investigate if publications that trained language models for software engineering (SE) tasks share source code and trained artifacts. The second goal is to analyze the transparency on training energy usage. Methods: We perform a snowballing-based literature search to find publications on language models for source code, and analyze their reusability from a sustainability standpoint. Results: From a total of 494 unique publications, we identified 293 relevant publications that use language models to address code-related tasks. Among them, 27% (79 out of 293) make artifacts available for reuse. This can be in the form of tools or IDE plugins designed for specific tasks or task-agnostic models that can be fine-tuned for a variety of downstream tasks. Moreover, we collect insights on the hardware used for model training, as well as training time, which together determine the energy consumption of the development process. Conclusion: We find that there are deficiencies in the sharing of information and artifacts for current studies on source code models for software engineering tasks, with 40% of the surveyed papers not sharing source code or trained artifacts. We recommend the sharing of source code as well as trained artifacts, to enable sustainable reproducibility. Moreover, comprehensive information on training times and hardware configurations should be shared for transparency on a model's carbon footprint. © 2023 IEEE.",DL4SE;  energy;  replication;  reuse;  sustainability,"Hort, M. and Grishina, A. and Moonen, L.",,10.1109/ESEM56168.2023.10304803,European Research Consortium for Informatics and Mathematics,EU,ACKNOWLEDGEMENTS The research presented in this paper was financially supported by the Research Council of Norway through the secureIT project (grant #288787). Max Hort is supported through the ERCIM ‘Alain Bensoussan’ Fellowship Programme.,"Carbon footprint;  Computational linguistics;  Computer programming languages;  Energy utilization;  Publishing;  Reusability;  Software design;  Sustainable development, DL4SE;  Energy;  Energy use;  Engineering tasks;  Language model;  Literature studies;  Replication;  Reuse;  Source codes;  Training time, Transparency",cited By 0,An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code,International Symposium on Empirical Software Engineering and Measurement,2023,ESEM,conf
